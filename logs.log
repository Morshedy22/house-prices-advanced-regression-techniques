2023-05-19 23:31:39,711:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-19 23:31:39,711:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-19 23:31:39,711:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-19 23:31:39,711:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-19 23:31:40,626:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-20 08:23:58,262:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 08:23:58,329:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 14:07:18,881:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 14:07:18,971:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 14:28:19,806:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 14:28:19,876:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 14:45:19,913:INFO:PyCaret RegressionExperiment
2023-05-20 14:45:19,913:INFO:Logging name: reg-default-name
2023-05-20 14:45:19,913:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 14:45:19,913:INFO:version 3.0.2
2023-05-20 14:45:19,913:INFO:Initializing setup()
2023-05-20 14:45:19,913:INFO:self.USI: f130
2023-05-20 14:45:19,913:INFO:self._variable_keys: {'USI', '_ml_usecase', 'pipeline', 'html_param', 'n_jobs_param', 'fold_generator', 'gpu_param', 'target_param', 'gpu_n_jobs_param', 'fold_groups_param', 'X_test', 'logging_param', 'exp_id', 'log_plots_param', 'data', '_available_plots', 'y_test', 'memory', 'idx', 'X_train', 'y', 'seed', 'y_train', 'fold_shuffle_param', 'transform_target_param', 'X', 'exp_name_log'}
2023-05-20 14:45:19,913:INFO:Checking environment
2023-05-20 14:45:19,913:INFO:python_version: 3.10.3
2023-05-20 14:45:19,913:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 14:45:19,913:INFO:machine: AMD64
2023-05-20 14:45:19,913:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 14:45:19,914:INFO:Memory: svmem(total=17083187200, available=4267069440, percent=75.0, used=12816117760, free=4267069440)
2023-05-20 14:45:19,914:INFO:Physical Core: 6
2023-05-20 14:45:19,914:INFO:Logical Core: 12
2023-05-20 14:45:19,914:INFO:Checking libraries
2023-05-20 14:45:19,914:INFO:System:
2023-05-20 14:45:19,914:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 14:45:19,914:INFO:executable: c:\Python310\python.exe
2023-05-20 14:45:19,914:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 14:45:19,914:INFO:PyCaret required dependencies:
2023-05-20 14:45:19,914:INFO:                 pip: 23.1.2
2023-05-20 14:45:19,914:INFO:          setuptools: 58.1.0
2023-05-20 14:45:19,914:INFO:             pycaret: 3.0.2
2023-05-20 14:45:19,914:INFO:             IPython: 8.5.0
2023-05-20 14:45:19,914:INFO:          ipywidgets: 8.0.6
2023-05-20 14:45:19,914:INFO:                tqdm: 4.65.0
2023-05-20 14:45:19,914:INFO:               numpy: 1.23.2
2023-05-20 14:45:19,914:INFO:              pandas: 1.5.2
2023-05-20 14:45:19,915:INFO:              jinja2: 3.1.2
2023-05-20 14:45:19,915:INFO:               scipy: 1.9.3
2023-05-20 14:45:19,915:INFO:              joblib: 1.2.0
2023-05-20 14:45:19,915:INFO:             sklearn: 1.1.3
2023-05-20 14:45:19,915:INFO:                pyod: 1.0.9
2023-05-20 14:45:19,915:INFO:            imblearn: 0.10.1
2023-05-20 14:45:19,915:INFO:   category_encoders: 2.6.1
2023-05-20 14:45:19,915:INFO:            lightgbm: 3.3.5
2023-05-20 14:45:19,915:INFO:               numba: 0.57.0
2023-05-20 14:45:19,915:INFO:            requests: 2.28.2
2023-05-20 14:45:19,915:INFO:          matplotlib: 3.5.3
2023-05-20 14:45:19,915:INFO:          scikitplot: 0.3.7
2023-05-20 14:45:19,915:INFO:         yellowbrick: 1.5
2023-05-20 14:45:19,915:INFO:              plotly: 5.13.1
2023-05-20 14:45:19,915:INFO:             kaleido: 0.2.1
2023-05-20 14:45:19,915:INFO:         statsmodels: 0.13.5
2023-05-20 14:45:19,915:INFO:              sktime: 0.17.0
2023-05-20 14:45:19,915:INFO:               tbats: 1.1.3
2023-05-20 14:45:19,915:INFO:            pmdarima: 2.0.3
2023-05-20 14:45:19,915:INFO:              psutil: 5.9.2
2023-05-20 14:45:19,915:INFO:PyCaret optional dependencies:
2023-05-20 14:45:19,930:INFO:                shap: Not installed
2023-05-20 14:45:19,930:INFO:           interpret: Not installed
2023-05-20 14:45:19,930:INFO:                umap: Not installed
2023-05-20 14:45:19,930:INFO:    pandas_profiling: Not installed
2023-05-20 14:45:19,930:INFO:  explainerdashboard: Not installed
2023-05-20 14:45:19,930:INFO:             autoviz: Not installed
2023-05-20 14:45:19,930:INFO:           fairlearn: Not installed
2023-05-20 14:45:19,930:INFO:             xgboost: 1.7.4
2023-05-20 14:45:19,930:INFO:            catboost: 1.1.1
2023-05-20 14:45:19,930:INFO:              kmodes: Not installed
2023-05-20 14:45:19,930:INFO:             mlxtend: Not installed
2023-05-20 14:45:19,931:INFO:       statsforecast: Not installed
2023-05-20 14:45:19,931:INFO:        tune_sklearn: Not installed
2023-05-20 14:45:19,931:INFO:                 ray: Not installed
2023-05-20 14:45:19,931:INFO:            hyperopt: Not installed
2023-05-20 14:45:19,931:INFO:              optuna: Not installed
2023-05-20 14:45:19,931:INFO:               skopt: Not installed
2023-05-20 14:45:19,931:INFO:              mlflow: Not installed
2023-05-20 14:45:19,931:INFO:              gradio: Not installed
2023-05-20 14:45:19,931:INFO:             fastapi: Not installed
2023-05-20 14:45:19,931:INFO:             uvicorn: Not installed
2023-05-20 14:45:19,931:INFO:              m2cgen: Not installed
2023-05-20 14:45:19,931:INFO:           evidently: Not installed
2023-05-20 14:45:19,931:INFO:               fugue: Not installed
2023-05-20 14:45:19,931:INFO:           streamlit: Not installed
2023-05-20 14:45:19,931:INFO:             prophet: Not installed
2023-05-20 14:45:19,931:INFO:None
2023-05-20 14:45:19,931:INFO:Set up data.
2023-05-20 14:45:20,009:INFO:Set up train/test split.
2023-05-20 14:45:20,021:INFO:Set up index.
2023-05-20 14:45:20,022:INFO:Set up folding strategy.
2023-05-20 14:45:20,022:INFO:Assigning column types.
2023-05-20 14:45:20,031:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 14:45:20,031:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,035:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,040:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,096:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,136:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,136:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:20,354:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:20,485:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,488:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,491:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,546:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,584:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,585:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:20,587:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:20,587:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 14:45:20,591:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,595:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,649:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,689:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,690:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:20,692:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:20,696:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,699:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,756:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,794:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,795:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:20,797:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:20,800:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 14:45:20,808:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,863:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,901:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,902:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:20,905:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:20,914:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:45:20,967:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,007:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,007:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,009:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,009:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 14:45:21,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,113:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,114:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,116:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,181:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,220:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,221:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,223:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,223:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 14:45:21,286:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,325:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,327:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,390:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:45:21,430:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,432:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,433:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 14:45:21,537:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,539:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,643:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:21,645:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:21,649:INFO:Preparing preprocessing pipeline...
2023-05-20 14:45:21,649:INFO:Set up simple imputation.
2023-05-20 14:45:21,651:INFO:Set up column name cleaning.
2023-05-20 14:45:21,701:INFO:Finished creating preprocessing pipeline.
2023-05-20 14:45:21,707:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 14:45:21,708:INFO:Creating final display dataframe.
2023-05-20 14:45:21,874:INFO:Setup _display_container:                     Description             Value
0                    Session id              1804
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              f130
2023-05-20 14:45:22,031:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:22,033:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:22,136:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:45:22,139:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:45:22,139:INFO:setup() successfully completed in 2.23s...............
2023-05-20 14:49:05,055:INFO:PyCaret RegressionExperiment
2023-05-20 14:49:05,056:INFO:Logging name: reg-default-name
2023-05-20 14:49:05,056:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 14:49:05,056:INFO:version 3.0.2
2023-05-20 14:49:05,056:INFO:Initializing setup()
2023-05-20 14:49:05,056:INFO:self.USI: a670
2023-05-20 14:49:05,056:INFO:self._variable_keys: {'USI', '_ml_usecase', 'pipeline', 'html_param', 'n_jobs_param', 'fold_generator', 'gpu_param', 'target_param', 'gpu_n_jobs_param', 'fold_groups_param', 'X_test', 'logging_param', 'exp_id', 'log_plots_param', 'data', '_available_plots', 'y_test', 'memory', 'idx', 'X_train', 'y', 'seed', 'y_train', 'fold_shuffle_param', 'transform_target_param', 'X', 'exp_name_log'}
2023-05-20 14:49:05,056:INFO:Checking environment
2023-05-20 14:49:05,056:INFO:python_version: 3.10.3
2023-05-20 14:49:05,056:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 14:49:05,056:INFO:machine: AMD64
2023-05-20 14:49:05,056:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 14:49:05,056:INFO:Memory: svmem(total=17083187200, available=3887865856, percent=77.2, used=13195321344, free=3887865856)
2023-05-20 14:49:05,056:INFO:Physical Core: 6
2023-05-20 14:49:05,056:INFO:Logical Core: 12
2023-05-20 14:49:05,057:INFO:Checking libraries
2023-05-20 14:49:05,057:INFO:System:
2023-05-20 14:49:05,057:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 14:49:05,057:INFO:executable: c:\Python310\python.exe
2023-05-20 14:49:05,057:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 14:49:05,057:INFO:PyCaret required dependencies:
2023-05-20 14:49:05,057:INFO:                 pip: 23.1.2
2023-05-20 14:49:05,057:INFO:          setuptools: 58.1.0
2023-05-20 14:49:05,057:INFO:             pycaret: 3.0.2
2023-05-20 14:49:05,057:INFO:             IPython: 8.5.0
2023-05-20 14:49:05,057:INFO:          ipywidgets: 8.0.6
2023-05-20 14:49:05,057:INFO:                tqdm: 4.65.0
2023-05-20 14:49:05,057:INFO:               numpy: 1.23.2
2023-05-20 14:49:05,057:INFO:              pandas: 1.5.2
2023-05-20 14:49:05,057:INFO:              jinja2: 3.1.2
2023-05-20 14:49:05,057:INFO:               scipy: 1.9.3
2023-05-20 14:49:05,057:INFO:              joblib: 1.2.0
2023-05-20 14:49:05,057:INFO:             sklearn: 1.1.3
2023-05-20 14:49:05,057:INFO:                pyod: 1.0.9
2023-05-20 14:49:05,057:INFO:            imblearn: 0.10.1
2023-05-20 14:49:05,057:INFO:   category_encoders: 2.6.1
2023-05-20 14:49:05,057:INFO:            lightgbm: 3.3.5
2023-05-20 14:49:05,057:INFO:               numba: 0.57.0
2023-05-20 14:49:05,057:INFO:            requests: 2.28.2
2023-05-20 14:49:05,058:INFO:          matplotlib: 3.5.3
2023-05-20 14:49:05,058:INFO:          scikitplot: 0.3.7
2023-05-20 14:49:05,058:INFO:         yellowbrick: 1.5
2023-05-20 14:49:05,058:INFO:              plotly: 5.13.1
2023-05-20 14:49:05,058:INFO:             kaleido: 0.2.1
2023-05-20 14:49:05,058:INFO:         statsmodels: 0.13.5
2023-05-20 14:49:05,058:INFO:              sktime: 0.17.0
2023-05-20 14:49:05,058:INFO:               tbats: 1.1.3
2023-05-20 14:49:05,058:INFO:            pmdarima: 2.0.3
2023-05-20 14:49:05,058:INFO:              psutil: 5.9.2
2023-05-20 14:49:05,058:INFO:PyCaret optional dependencies:
2023-05-20 14:49:05,058:INFO:                shap: Not installed
2023-05-20 14:49:05,058:INFO:           interpret: Not installed
2023-05-20 14:49:05,058:INFO:                umap: Not installed
2023-05-20 14:49:05,058:INFO:    pandas_profiling: Not installed
2023-05-20 14:49:05,058:INFO:  explainerdashboard: Not installed
2023-05-20 14:49:05,058:INFO:             autoviz: Not installed
2023-05-20 14:49:05,058:INFO:           fairlearn: Not installed
2023-05-20 14:49:05,058:INFO:             xgboost: 1.7.4
2023-05-20 14:49:05,058:INFO:            catboost: 1.1.1
2023-05-20 14:49:05,058:INFO:              kmodes: Not installed
2023-05-20 14:49:05,058:INFO:             mlxtend: Not installed
2023-05-20 14:49:05,058:INFO:       statsforecast: Not installed
2023-05-20 14:49:05,058:INFO:        tune_sklearn: Not installed
2023-05-20 14:49:05,058:INFO:                 ray: Not installed
2023-05-20 14:49:05,059:INFO:            hyperopt: Not installed
2023-05-20 14:49:05,059:INFO:              optuna: Not installed
2023-05-20 14:49:05,059:INFO:               skopt: Not installed
2023-05-20 14:49:05,059:INFO:              mlflow: Not installed
2023-05-20 14:49:05,059:INFO:              gradio: Not installed
2023-05-20 14:49:05,059:INFO:             fastapi: Not installed
2023-05-20 14:49:05,059:INFO:             uvicorn: Not installed
2023-05-20 14:49:05,059:INFO:              m2cgen: Not installed
2023-05-20 14:49:05,059:INFO:           evidently: Not installed
2023-05-20 14:49:05,059:INFO:               fugue: Not installed
2023-05-20 14:49:05,059:INFO:           streamlit: Not installed
2023-05-20 14:49:05,059:INFO:             prophet: Not installed
2023-05-20 14:49:05,059:INFO:None
2023-05-20 14:49:05,059:INFO:Set up data.
2023-05-20 14:49:05,140:INFO:Set up train/test split.
2023-05-20 14:49:05,420:INFO:Set up index.
2023-05-20 14:49:05,420:INFO:Set up folding strategy.
2023-05-20 14:49:05,420:INFO:Assigning column types.
2023-05-20 14:49:05,427:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 14:49:05,427:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,432:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,435:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,489:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,528:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,529:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:05,531:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:05,531:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,536:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,540:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,616:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,664:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,664:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:05,667:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:05,667:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 14:49:05,672:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,676:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,739:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,777:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,778:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:05,780:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:05,784:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,788:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,839:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,876:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,877:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:05,880:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:05,880:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 14:49:05,887:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,943:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,981:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:05,982:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:05,984:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:05,992:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,049:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,087:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,088:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,090:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,090:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 14:49:06,152:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,189:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,190:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,192:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,251:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,290:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,291:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,293:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,294:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 14:49:06,365:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,407:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,409:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,486:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:49:06,603:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,605:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,606:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 14:49:06,705:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,707:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,806:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:06,807:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:06,809:INFO:Preparing preprocessing pipeline...
2023-05-20 14:49:06,809:INFO:Set up simple imputation.
2023-05-20 14:49:06,811:INFO:Set up column name cleaning.
2023-05-20 14:49:06,846:INFO:Finished creating preprocessing pipeline.
2023-05-20 14:49:06,852:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 14:49:06,852:INFO:Creating final display dataframe.
2023-05-20 14:49:07,003:INFO:Setup _display_container:                     Description             Value
0                    Session id              6369
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              a670
2023-05-20 14:49:07,116:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:07,119:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:07,233:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:49:07,236:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:49:07,237:INFO:setup() successfully completed in 2.18s...............
2023-05-20 14:51:06,691:INFO:PyCaret RegressionExperiment
2023-05-20 14:51:06,691:INFO:Logging name: reg-default-name
2023-05-20 14:51:06,692:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 14:51:06,692:INFO:version 3.0.2
2023-05-20 14:51:06,692:INFO:Initializing setup()
2023-05-20 14:51:06,692:INFO:self.USI: 99b2
2023-05-20 14:51:06,692:INFO:self._variable_keys: {'USI', '_ml_usecase', 'pipeline', 'html_param', 'n_jobs_param', 'fold_generator', 'gpu_param', 'target_param', 'gpu_n_jobs_param', 'fold_groups_param', 'X_test', 'logging_param', 'exp_id', 'log_plots_param', 'data', '_available_plots', 'y_test', 'memory', 'idx', 'X_train', 'y', 'seed', 'y_train', 'fold_shuffle_param', 'transform_target_param', 'X', 'exp_name_log'}
2023-05-20 14:51:06,692:INFO:Checking environment
2023-05-20 14:51:06,692:INFO:python_version: 3.10.3
2023-05-20 14:51:06,692:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 14:51:06,692:INFO:machine: AMD64
2023-05-20 14:51:06,692:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 14:51:06,693:INFO:Memory: svmem(total=17083187200, available=4037066752, percent=76.4, used=13046120448, free=4037066752)
2023-05-20 14:51:06,693:INFO:Physical Core: 6
2023-05-20 14:51:06,693:INFO:Logical Core: 12
2023-05-20 14:51:06,693:INFO:Checking libraries
2023-05-20 14:51:06,693:INFO:System:
2023-05-20 14:51:06,693:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 14:51:06,693:INFO:executable: c:\Python310\python.exe
2023-05-20 14:51:06,693:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 14:51:06,693:INFO:PyCaret required dependencies:
2023-05-20 14:51:06,693:INFO:                 pip: 23.1.2
2023-05-20 14:51:06,694:INFO:          setuptools: 58.1.0
2023-05-20 14:51:06,694:INFO:             pycaret: 3.0.2
2023-05-20 14:51:06,694:INFO:             IPython: 8.5.0
2023-05-20 14:51:06,694:INFO:          ipywidgets: 8.0.6
2023-05-20 14:51:06,694:INFO:                tqdm: 4.65.0
2023-05-20 14:51:06,694:INFO:               numpy: 1.23.2
2023-05-20 14:51:06,694:INFO:              pandas: 1.5.2
2023-05-20 14:51:06,694:INFO:              jinja2: 3.1.2
2023-05-20 14:51:06,694:INFO:               scipy: 1.9.3
2023-05-20 14:51:06,694:INFO:              joblib: 1.2.0
2023-05-20 14:51:06,694:INFO:             sklearn: 1.1.3
2023-05-20 14:51:06,694:INFO:                pyod: 1.0.9
2023-05-20 14:51:06,694:INFO:            imblearn: 0.10.1
2023-05-20 14:51:06,694:INFO:   category_encoders: 2.6.1
2023-05-20 14:51:06,694:INFO:            lightgbm: 3.3.5
2023-05-20 14:51:06,694:INFO:               numba: 0.57.0
2023-05-20 14:51:06,694:INFO:            requests: 2.28.2
2023-05-20 14:51:06,695:INFO:          matplotlib: 3.5.3
2023-05-20 14:51:06,695:INFO:          scikitplot: 0.3.7
2023-05-20 14:51:06,695:INFO:         yellowbrick: 1.5
2023-05-20 14:51:06,695:INFO:              plotly: 5.13.1
2023-05-20 14:51:06,695:INFO:             kaleido: 0.2.1
2023-05-20 14:51:06,695:INFO:         statsmodels: 0.13.5
2023-05-20 14:51:06,695:INFO:              sktime: 0.17.0
2023-05-20 14:51:06,695:INFO:               tbats: 1.1.3
2023-05-20 14:51:06,695:INFO:            pmdarima: 2.0.3
2023-05-20 14:51:06,695:INFO:              psutil: 5.9.2
2023-05-20 14:51:06,695:INFO:PyCaret optional dependencies:
2023-05-20 14:51:06,696:INFO:                shap: Not installed
2023-05-20 14:51:06,696:INFO:           interpret: Not installed
2023-05-20 14:51:06,696:INFO:                umap: Not installed
2023-05-20 14:51:06,696:INFO:    pandas_profiling: Not installed
2023-05-20 14:51:06,696:INFO:  explainerdashboard: Not installed
2023-05-20 14:51:06,696:INFO:             autoviz: Not installed
2023-05-20 14:51:06,696:INFO:           fairlearn: Not installed
2023-05-20 14:51:06,696:INFO:             xgboost: 1.7.4
2023-05-20 14:51:06,696:INFO:            catboost: 1.1.1
2023-05-20 14:51:06,696:INFO:              kmodes: Not installed
2023-05-20 14:51:06,696:INFO:             mlxtend: Not installed
2023-05-20 14:51:06,696:INFO:       statsforecast: Not installed
2023-05-20 14:51:06,696:INFO:        tune_sklearn: Not installed
2023-05-20 14:51:06,696:INFO:                 ray: Not installed
2023-05-20 14:51:06,696:INFO:            hyperopt: Not installed
2023-05-20 14:51:06,696:INFO:              optuna: Not installed
2023-05-20 14:51:06,697:INFO:               skopt: Not installed
2023-05-20 14:51:06,697:INFO:              mlflow: Not installed
2023-05-20 14:51:06,697:INFO:              gradio: Not installed
2023-05-20 14:51:06,697:INFO:             fastapi: Not installed
2023-05-20 14:51:06,697:INFO:             uvicorn: Not installed
2023-05-20 14:51:06,697:INFO:              m2cgen: Not installed
2023-05-20 14:51:06,697:INFO:           evidently: Not installed
2023-05-20 14:51:06,697:INFO:               fugue: Not installed
2023-05-20 14:51:06,697:INFO:           streamlit: Not installed
2023-05-20 14:51:06,697:INFO:             prophet: Not installed
2023-05-20 14:51:06,697:INFO:None
2023-05-20 14:51:06,697:INFO:Set up data.
2023-05-20 14:51:06,782:INFO:Set up train/test split.
2023-05-20 14:51:06,790:INFO:Set up index.
2023-05-20 14:51:06,790:INFO:Set up folding strategy.
2023-05-20 14:51:06,791:INFO:Assigning column types.
2023-05-20 14:51:06,798:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 14:51:06,799:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,803:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,807:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,860:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,896:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,897:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:06,900:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:06,900:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,904:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,908:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,959:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:06,997:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,000:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,001:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 14:51:07,004:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,008:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,061:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,098:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,098:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,100:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,104:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,108:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,160:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,196:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,196:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,198:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,198:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 14:51:07,206:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,257:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,295:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,296:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,298:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,307:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,357:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,396:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,396:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,399:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,399:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 14:51:07,459:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,496:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,496:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,498:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,563:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,601:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,601:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,603:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,604:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 14:51:07,663:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,702:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,704:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,764:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 14:51:07,801:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,803:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,803:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 14:51:07,901:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:07,903:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:07,999:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:08,002:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:08,003:INFO:Preparing preprocessing pipeline...
2023-05-20 14:51:08,003:INFO:Set up simple imputation.
2023-05-20 14:51:08,004:INFO:Set up column name cleaning.
2023-05-20 14:51:08,093:INFO:Finished creating preprocessing pipeline.
2023-05-20 14:51:08,103:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 14:51:08,103:INFO:Creating final display dataframe.
2023-05-20 14:51:08,282:INFO:Setup _display_container:                     Description             Value
0                    Session id              5326
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              99b2
2023-05-20 14:51:08,399:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:08,402:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:08,514:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 14:51:08,517:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 14:51:08,517:INFO:setup() successfully completed in 1.83s...............
2023-05-20 15:10:08,125:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 15:10:08,188:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29600\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 15:10:08,716:INFO:PyCaret RegressionExperiment
2023-05-20 15:10:08,716:INFO:Logging name: reg-default-name
2023-05-20 15:10:08,716:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:10:08,716:INFO:version 3.0.2
2023-05-20 15:10:08,716:INFO:Initializing setup()
2023-05-20 15:10:08,716:INFO:self.USI: 3a08
2023-05-20 15:10:08,716:INFO:self._variable_keys: {'USI', '_ml_usecase', 'pipeline', 'html_param', 'n_jobs_param', 'fold_generator', 'gpu_param', 'target_param', 'gpu_n_jobs_param', 'fold_groups_param', 'X_test', 'logging_param', 'exp_id', 'log_plots_param', 'data', '_available_plots', 'y_test', 'memory', 'idx', 'X_train', 'y', 'seed', 'y_train', 'fold_shuffle_param', 'transform_target_param', 'X', 'exp_name_log'}
2023-05-20 15:10:08,716:INFO:Checking environment
2023-05-20 15:10:08,716:INFO:python_version: 3.10.3
2023-05-20 15:10:08,716:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:10:08,716:INFO:machine: AMD64
2023-05-20 15:10:08,716:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:10:08,716:INFO:Memory: svmem(total=17083187200, available=3502575616, percent=79.5, used=13580611584, free=3502575616)
2023-05-20 15:10:08,716:INFO:Physical Core: 6
2023-05-20 15:10:08,716:INFO:Logical Core: 12
2023-05-20 15:10:08,717:INFO:Checking libraries
2023-05-20 15:10:08,717:INFO:System:
2023-05-20 15:10:08,717:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:10:08,717:INFO:executable: c:\Python310\python.exe
2023-05-20 15:10:08,717:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:10:08,717:INFO:PyCaret required dependencies:
2023-05-20 15:10:08,717:INFO:                 pip: 23.1.2
2023-05-20 15:10:08,717:INFO:          setuptools: 58.1.0
2023-05-20 15:10:08,717:INFO:             pycaret: 3.0.2
2023-05-20 15:10:08,717:INFO:             IPython: 8.5.0
2023-05-20 15:10:08,717:INFO:          ipywidgets: 8.0.6
2023-05-20 15:10:08,717:INFO:                tqdm: 4.65.0
2023-05-20 15:10:08,718:INFO:               numpy: 1.23.2
2023-05-20 15:10:08,718:INFO:              pandas: 1.5.2
2023-05-20 15:10:08,718:INFO:              jinja2: 3.1.2
2023-05-20 15:10:08,718:INFO:               scipy: 1.9.3
2023-05-20 15:10:08,718:INFO:              joblib: 1.2.0
2023-05-20 15:10:08,718:INFO:             sklearn: 1.1.3
2023-05-20 15:10:08,718:INFO:                pyod: 1.0.9
2023-05-20 15:10:08,718:INFO:            imblearn: 0.10.1
2023-05-20 15:10:08,718:INFO:   category_encoders: 2.6.1
2023-05-20 15:10:08,718:INFO:            lightgbm: 3.3.5
2023-05-20 15:10:08,718:INFO:               numba: 0.57.0
2023-05-20 15:10:08,718:INFO:            requests: 2.28.2
2023-05-20 15:10:08,718:INFO:          matplotlib: 3.5.3
2023-05-20 15:10:08,718:INFO:          scikitplot: 0.3.7
2023-05-20 15:10:08,718:INFO:         yellowbrick: 1.5
2023-05-20 15:10:08,718:INFO:              plotly: 5.13.1
2023-05-20 15:10:08,718:INFO:             kaleido: 0.2.1
2023-05-20 15:10:08,719:INFO:         statsmodels: 0.13.5
2023-05-20 15:10:08,719:INFO:              sktime: 0.17.0
2023-05-20 15:10:08,719:INFO:               tbats: 1.1.3
2023-05-20 15:10:08,719:INFO:            pmdarima: 2.0.3
2023-05-20 15:10:08,719:INFO:              psutil: 5.9.2
2023-05-20 15:10:08,719:INFO:PyCaret optional dependencies:
2023-05-20 15:10:08,719:INFO:                shap: Not installed
2023-05-20 15:10:08,719:INFO:           interpret: Not installed
2023-05-20 15:10:08,719:INFO:                umap: Not installed
2023-05-20 15:10:08,719:INFO:    pandas_profiling: Not installed
2023-05-20 15:10:08,719:INFO:  explainerdashboard: Not installed
2023-05-20 15:10:08,719:INFO:             autoviz: Not installed
2023-05-20 15:10:08,719:INFO:           fairlearn: Not installed
2023-05-20 15:10:08,719:INFO:             xgboost: 1.7.4
2023-05-20 15:10:08,719:INFO:            catboost: 1.1.1
2023-05-20 15:10:08,720:INFO:              kmodes: Not installed
2023-05-20 15:10:08,720:INFO:             mlxtend: Not installed
2023-05-20 15:10:08,720:INFO:       statsforecast: Not installed
2023-05-20 15:10:08,720:INFO:        tune_sklearn: Not installed
2023-05-20 15:10:08,720:INFO:                 ray: Not installed
2023-05-20 15:10:08,720:INFO:            hyperopt: Not installed
2023-05-20 15:10:08,720:INFO:              optuna: Not installed
2023-05-20 15:10:08,720:INFO:               skopt: Not installed
2023-05-20 15:10:08,720:INFO:              mlflow: Not installed
2023-05-20 15:10:08,720:INFO:              gradio: Not installed
2023-05-20 15:10:08,720:INFO:             fastapi: Not installed
2023-05-20 15:10:08,720:INFO:             uvicorn: Not installed
2023-05-20 15:10:08,720:INFO:              m2cgen: Not installed
2023-05-20 15:10:08,720:INFO:           evidently: Not installed
2023-05-20 15:10:08,720:INFO:               fugue: Not installed
2023-05-20 15:10:08,720:INFO:           streamlit: Not installed
2023-05-20 15:10:08,720:INFO:             prophet: Not installed
2023-05-20 15:10:08,720:INFO:None
2023-05-20 15:10:08,720:INFO:Set up data.
2023-05-20 15:10:08,802:INFO:Set up train/test split.
2023-05-20 15:10:08,810:INFO:Set up index.
2023-05-20 15:10:08,811:INFO:Set up folding strategy.
2023-05-20 15:10:08,811:INFO:Assigning column types.
2023-05-20 15:10:08,818:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:10:08,819:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,823:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,826:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,878:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,916:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,917:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:08,918:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:08,919:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,924:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,928:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:08,981:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,017:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,017:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,019:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,020:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:10:09,024:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,027:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,080:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,118:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,118:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,120:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,124:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,128:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,179:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,216:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,216:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,219:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,220:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:10:09,227:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,279:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,316:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,318:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,320:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,328:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,380:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,418:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,419:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,421:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,421:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:10:09,480:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,517:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,518:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,520:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,584:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,622:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,622:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,624:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,624:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:10:09,689:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,726:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,728:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,789:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:09,828:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,831:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:09,831:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:10:09,952:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:09,954:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:10,142:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:10,146:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:10,147:INFO:Preparing preprocessing pipeline...
2023-05-20 15:10:10,147:INFO:Set up simple imputation.
2023-05-20 15:10:10,150:INFO:Set up column name cleaning.
2023-05-20 15:10:10,205:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:10:10,212:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:10:10,212:INFO:Creating final display dataframe.
2023-05-20 15:10:10,445:INFO:Setup _display_container:                     Description             Value
0                    Session id              2942
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              3a08
2023-05-20 15:10:10,569:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:10,571:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:10,687:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:10,689:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:10,692:INFO:setup() successfully completed in 1.98s...............
2023-05-20 15:10:54,184:INFO:PyCaret RegressionExperiment
2023-05-20 15:10:54,184:INFO:Logging name: reg-default-name
2023-05-20 15:10:54,184:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:10:54,184:INFO:version 3.0.2
2023-05-20 15:10:54,184:INFO:Initializing setup()
2023-05-20 15:10:54,184:INFO:self.USI: 7364
2023-05-20 15:10:54,184:INFO:self._variable_keys: {'USI', '_ml_usecase', 'pipeline', 'html_param', 'n_jobs_param', 'fold_generator', 'gpu_param', 'target_param', 'gpu_n_jobs_param', 'fold_groups_param', 'X_test', 'logging_param', 'exp_id', 'log_plots_param', 'data', '_available_plots', 'y_test', 'memory', 'idx', 'X_train', 'y', 'seed', 'y_train', 'fold_shuffle_param', 'transform_target_param', 'X', 'exp_name_log'}
2023-05-20 15:10:54,184:INFO:Checking environment
2023-05-20 15:10:54,184:INFO:python_version: 3.10.3
2023-05-20 15:10:54,184:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:10:54,184:INFO:machine: AMD64
2023-05-20 15:10:54,184:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:10:54,184:INFO:Memory: svmem(total=17083187200, available=3313324032, percent=80.6, used=13769863168, free=3313324032)
2023-05-20 15:10:54,184:INFO:Physical Core: 6
2023-05-20 15:10:54,184:INFO:Logical Core: 12
2023-05-20 15:10:54,184:INFO:Checking libraries
2023-05-20 15:10:54,186:INFO:System:
2023-05-20 15:10:54,186:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:10:54,186:INFO:executable: c:\Python310\python.exe
2023-05-20 15:10:54,186:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:10:54,186:INFO:PyCaret required dependencies:
2023-05-20 15:10:54,186:INFO:                 pip: 23.1.2
2023-05-20 15:10:54,186:INFO:          setuptools: 58.1.0
2023-05-20 15:10:54,186:INFO:             pycaret: 3.0.2
2023-05-20 15:10:54,186:INFO:             IPython: 8.5.0
2023-05-20 15:10:54,186:INFO:          ipywidgets: 8.0.6
2023-05-20 15:10:54,186:INFO:                tqdm: 4.65.0
2023-05-20 15:10:54,186:INFO:               numpy: 1.23.2
2023-05-20 15:10:54,186:INFO:              pandas: 1.5.2
2023-05-20 15:10:54,187:INFO:              jinja2: 3.1.2
2023-05-20 15:10:54,187:INFO:               scipy: 1.9.3
2023-05-20 15:10:54,187:INFO:              joblib: 1.2.0
2023-05-20 15:10:54,187:INFO:             sklearn: 1.1.3
2023-05-20 15:10:54,187:INFO:                pyod: 1.0.9
2023-05-20 15:10:54,187:INFO:            imblearn: 0.10.1
2023-05-20 15:10:54,187:INFO:   category_encoders: 2.6.1
2023-05-20 15:10:54,187:INFO:            lightgbm: 3.3.5
2023-05-20 15:10:54,187:INFO:               numba: 0.57.0
2023-05-20 15:10:54,187:INFO:            requests: 2.28.2
2023-05-20 15:10:54,187:INFO:          matplotlib: 3.5.3
2023-05-20 15:10:54,187:INFO:          scikitplot: 0.3.7
2023-05-20 15:10:54,187:INFO:         yellowbrick: 1.5
2023-05-20 15:10:54,187:INFO:              plotly: 5.13.1
2023-05-20 15:10:54,187:INFO:             kaleido: 0.2.1
2023-05-20 15:10:54,187:INFO:         statsmodels: 0.13.5
2023-05-20 15:10:54,187:INFO:              sktime: 0.17.0
2023-05-20 15:10:54,187:INFO:               tbats: 1.1.3
2023-05-20 15:10:54,187:INFO:            pmdarima: 2.0.3
2023-05-20 15:10:54,187:INFO:              psutil: 5.9.2
2023-05-20 15:10:54,187:INFO:PyCaret optional dependencies:
2023-05-20 15:10:54,187:INFO:                shap: Not installed
2023-05-20 15:10:54,187:INFO:           interpret: Not installed
2023-05-20 15:10:54,187:INFO:                umap: Not installed
2023-05-20 15:10:54,187:INFO:    pandas_profiling: Not installed
2023-05-20 15:10:54,187:INFO:  explainerdashboard: Not installed
2023-05-20 15:10:54,188:INFO:             autoviz: Not installed
2023-05-20 15:10:54,188:INFO:           fairlearn: Not installed
2023-05-20 15:10:54,188:INFO:             xgboost: 1.7.4
2023-05-20 15:10:54,188:INFO:            catboost: 1.1.1
2023-05-20 15:10:54,188:INFO:              kmodes: Not installed
2023-05-20 15:10:54,188:INFO:             mlxtend: Not installed
2023-05-20 15:10:54,188:INFO:       statsforecast: Not installed
2023-05-20 15:10:54,188:INFO:        tune_sklearn: Not installed
2023-05-20 15:10:54,188:INFO:                 ray: Not installed
2023-05-20 15:10:54,188:INFO:            hyperopt: Not installed
2023-05-20 15:10:54,188:INFO:              optuna: Not installed
2023-05-20 15:10:54,188:INFO:               skopt: Not installed
2023-05-20 15:10:54,188:INFO:              mlflow: Not installed
2023-05-20 15:10:54,188:INFO:              gradio: Not installed
2023-05-20 15:10:54,188:INFO:             fastapi: Not installed
2023-05-20 15:10:54,188:INFO:             uvicorn: Not installed
2023-05-20 15:10:54,188:INFO:              m2cgen: Not installed
2023-05-20 15:10:54,189:INFO:           evidently: Not installed
2023-05-20 15:10:54,189:INFO:               fugue: Not installed
2023-05-20 15:10:54,189:INFO:           streamlit: Not installed
2023-05-20 15:10:54,189:INFO:             prophet: Not installed
2023-05-20 15:10:54,189:INFO:None
2023-05-20 15:10:54,189:INFO:Set up data.
2023-05-20 15:10:54,315:INFO:Set up train/test split.
2023-05-20 15:10:54,336:INFO:Set up index.
2023-05-20 15:10:54,337:INFO:Set up folding strategy.
2023-05-20 15:10:54,337:INFO:Assigning column types.
2023-05-20 15:10:54,352:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:10:54,353:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,359:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,368:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,431:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,475:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,476:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:54,478:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:54,478:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,482:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,486:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,539:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,578:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,578:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:54,581:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:54,581:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:10:54,585:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,589:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,641:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,680:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,681:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:54,683:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:54,687:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,690:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,754:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,799:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:54,801:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:54,802:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:10:54,809:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,866:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,905:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,906:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:54,909:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:54,917:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:10:54,974:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,013:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,014:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,016:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,017:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:10:55,079:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,117:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,118:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,121:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,187:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,247:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,247:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,250:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,250:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:10:55,349:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,390:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,392:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,462:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:10:55,500:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,502:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,503:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:10:55,602:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,604:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,706:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:55,707:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:55,708:INFO:Preparing preprocessing pipeline...
2023-05-20 15:10:55,708:INFO:Set up simple imputation.
2023-05-20 15:10:55,710:INFO:Set up column name cleaning.
2023-05-20 15:10:55,753:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:10:55,759:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:10:55,759:INFO:Creating final display dataframe.
2023-05-20 15:10:55,930:INFO:Setup _display_container:                     Description             Value
0                    Session id              1842
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              7364
2023-05-20 15:10:56,065:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:56,067:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:56,166:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:10:56,168:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:10:56,170:INFO:setup() successfully completed in 1.99s...............
2023-05-20 15:16:41,328:INFO:Initializing compare_models()
2023-05-20 15:16:41,328:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 15:16:41,328:INFO:Checking exceptions
2023-05-20 15:16:41,334:INFO:Preparing display monitor
2023-05-20 15:16:41,393:INFO:Initializing Linear Regression
2023-05-20 15:16:41,394:INFO:Total runtime is 1.6542275746663413e-05 minutes
2023-05-20 15:16:41,397:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:41,398:INFO:Initializing create_model()
2023-05-20 15:16:41,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:41,398:INFO:Checking exceptions
2023-05-20 15:16:41,398:INFO:Importing libraries
2023-05-20 15:16:41,398:INFO:Copying training dataset
2023-05-20 15:16:41,415:INFO:Defining folds
2023-05-20 15:16:41,415:INFO:Declaring metric variables
2023-05-20 15:16:41,419:INFO:Importing untrained model
2023-05-20 15:16:41,422:INFO:Linear Regression Imported successfully
2023-05-20 15:16:41,429:INFO:Starting cross validation
2023-05-20 15:16:41,439:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:52,151:INFO:Calculating mean and std
2023-05-20 15:16:52,171:INFO:Creating metrics dataframe
2023-05-20 15:16:52,193:INFO:Uploading results into container
2023-05-20 15:16:52,194:INFO:Uploading model into container now
2023-05-20 15:16:52,197:INFO:_master_model_container: 1
2023-05-20 15:16:52,198:INFO:_display_container: 2
2023-05-20 15:16:52,200:INFO:LinearRegression(n_jobs=-1)
2023-05-20 15:16:52,201:INFO:create_model() successfully completed......................................
2023-05-20 15:16:53,017:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:53,017:INFO:Creating metrics dataframe
2023-05-20 15:16:53,025:INFO:Initializing Lasso Regression
2023-05-20 15:16:53,026:INFO:Total runtime is 0.19387069940567017 minutes
2023-05-20 15:16:53,029:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:53,029:INFO:Initializing create_model()
2023-05-20 15:16:53,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:53,030:INFO:Checking exceptions
2023-05-20 15:16:53,030:INFO:Importing libraries
2023-05-20 15:16:53,030:INFO:Copying training dataset
2023-05-20 15:16:53,043:INFO:Defining folds
2023-05-20 15:16:53,043:INFO:Declaring metric variables
2023-05-20 15:16:53,046:INFO:Importing untrained model
2023-05-20 15:16:53,049:INFO:Lasso Regression Imported successfully
2023-05-20 15:16:53,056:INFO:Starting cross validation
2023-05-20 15:16:53,059:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:55,402:INFO:Calculating mean and std
2023-05-20 15:16:55,404:INFO:Creating metrics dataframe
2023-05-20 15:16:55,407:INFO:Uploading results into container
2023-05-20 15:16:55,408:INFO:Uploading model into container now
2023-05-20 15:16:55,409:INFO:_master_model_container: 2
2023-05-20 15:16:55,409:INFO:_display_container: 2
2023-05-20 15:16:55,409:INFO:Lasso(random_state=1842)
2023-05-20 15:16:55,409:INFO:create_model() successfully completed......................................
2023-05-20 15:16:55,643:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:55,644:INFO:Creating metrics dataframe
2023-05-20 15:16:55,651:INFO:Initializing Ridge Regression
2023-05-20 15:16:55,651:INFO:Total runtime is 0.237634015083313 minutes
2023-05-20 15:16:55,654:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:55,654:INFO:Initializing create_model()
2023-05-20 15:16:55,654:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:55,654:INFO:Checking exceptions
2023-05-20 15:16:55,654:INFO:Importing libraries
2023-05-20 15:16:55,654:INFO:Copying training dataset
2023-05-20 15:16:55,667:INFO:Defining folds
2023-05-20 15:16:55,667:INFO:Declaring metric variables
2023-05-20 15:16:55,670:INFO:Importing untrained model
2023-05-20 15:16:55,672:INFO:Ridge Regression Imported successfully
2023-05-20 15:16:55,677:INFO:Starting cross validation
2023-05-20 15:16:55,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:56,062:INFO:Calculating mean and std
2023-05-20 15:16:56,064:INFO:Creating metrics dataframe
2023-05-20 15:16:56,068:INFO:Uploading results into container
2023-05-20 15:16:56,069:INFO:Uploading model into container now
2023-05-20 15:16:56,069:INFO:_master_model_container: 3
2023-05-20 15:16:56,069:INFO:_display_container: 2
2023-05-20 15:16:56,069:INFO:Ridge(random_state=1842)
2023-05-20 15:16:56,069:INFO:create_model() successfully completed......................................
2023-05-20 15:16:56,279:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:56,279:INFO:Creating metrics dataframe
2023-05-20 15:16:56,288:INFO:Initializing Elastic Net
2023-05-20 15:16:56,288:INFO:Total runtime is 0.24823654492696126 minutes
2023-05-20 15:16:56,290:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:56,290:INFO:Initializing create_model()
2023-05-20 15:16:56,290:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:56,290:INFO:Checking exceptions
2023-05-20 15:16:56,291:INFO:Importing libraries
2023-05-20 15:16:56,291:INFO:Copying training dataset
2023-05-20 15:16:56,301:INFO:Defining folds
2023-05-20 15:16:56,301:INFO:Declaring metric variables
2023-05-20 15:16:56,304:INFO:Importing untrained model
2023-05-20 15:16:56,307:INFO:Elastic Net Imported successfully
2023-05-20 15:16:56,313:INFO:Starting cross validation
2023-05-20 15:16:56,314:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:56,678:INFO:Calculating mean and std
2023-05-20 15:16:56,680:INFO:Creating metrics dataframe
2023-05-20 15:16:56,684:INFO:Uploading results into container
2023-05-20 15:16:56,685:INFO:Uploading model into container now
2023-05-20 15:16:56,685:INFO:_master_model_container: 4
2023-05-20 15:16:56,685:INFO:_display_container: 2
2023-05-20 15:16:56,685:INFO:ElasticNet(random_state=1842)
2023-05-20 15:16:56,685:INFO:create_model() successfully completed......................................
2023-05-20 15:16:56,898:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:56,898:INFO:Creating metrics dataframe
2023-05-20 15:16:56,906:INFO:Initializing Least Angle Regression
2023-05-20 15:16:56,906:INFO:Total runtime is 0.25854066212972004 minutes
2023-05-20 15:16:56,909:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:56,909:INFO:Initializing create_model()
2023-05-20 15:16:56,909:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:56,909:INFO:Checking exceptions
2023-05-20 15:16:56,909:INFO:Importing libraries
2023-05-20 15:16:56,909:INFO:Copying training dataset
2023-05-20 15:16:56,922:INFO:Defining folds
2023-05-20 15:16:56,922:INFO:Declaring metric variables
2023-05-20 15:16:56,926:INFO:Importing untrained model
2023-05-20 15:16:56,928:INFO:Least Angle Regression Imported successfully
2023-05-20 15:16:56,934:INFO:Starting cross validation
2023-05-20 15:16:56,936:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:57,072:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,077:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,087:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,092:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,120:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,129:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.831e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.493e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,137:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.393e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,138:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=5.652e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,139:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.017e-04, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,139:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.213e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,137:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=5.829e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,140:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.706e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,144:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.629e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,145:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=4.530e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,147:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.489e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,148:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=1.328e-04, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,148:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.157e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,149:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.092e-04, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,151:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.095e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,152:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.177e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 119 iterations, i.e. alpha=1.120e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.921e-04, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,154:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,154:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.865e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,154:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:57,157:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.460e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,166:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.817e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,167:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.018e-04, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,169:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=5.097e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,171:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=7.506e-05, with an active set of 144 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,173:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=7.352e-05, with an active set of 146 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,177:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.103e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,179:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=2.100e-04, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,179:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=6.775e-05, with an active set of 157 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,179:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=2.099e-04, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,181:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.418e-04, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,189:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.914e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-05-20 15:16:57,193:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.381e-04, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,198:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 193 iterations, i.e. alpha=3.826e-04, with an active set of 164 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,210:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.940e+04, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 227 iterations, i.e. alpha=1.533e-02, with an active set of 179 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,222:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 248 iterations, i.e. alpha=3.955e-04, with an active set of 208 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,223:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 248 iterations, i.e. alpha=3.924e-04, with an active set of 208 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,240:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 267 iterations, i.e. alpha=2.102e-02, with an active set of 207 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 306 iterations, i.e. alpha=2.161e-01, with an active set of 243 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:16:57,288:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:16:57,289:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:16:57,289:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:16:57,440:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:16:57,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:16:57,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:16:57,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:16:57,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:16:57,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,684:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,684:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,684:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,684:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,688:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,710:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,710:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,711:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,712:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,723:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,723:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:16:57,723:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,724:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,724:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,724:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:16:57,725:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,737:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,738:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:16:57,738:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:16:57,893:INFO:Calculating mean and std
2023-05-20 15:16:57,894:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:16:57,896:INFO:Creating metrics dataframe
2023-05-20 15:16:57,905:INFO:Uploading results into container
2023-05-20 15:16:57,906:INFO:Uploading model into container now
2023-05-20 15:16:57,906:INFO:_master_model_container: 5
2023-05-20 15:16:57,906:INFO:_display_container: 2
2023-05-20 15:16:57,907:INFO:Lars(random_state=1842)
2023-05-20 15:16:57,907:INFO:create_model() successfully completed......................................
2023-05-20 15:16:58,134:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:58,134:INFO:Creating metrics dataframe
2023-05-20 15:16:58,143:INFO:Initializing Lasso Least Angle Regression
2023-05-20 15:16:58,143:INFO:Total runtime is 0.2791530132293701 minutes
2023-05-20 15:16:58,146:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:58,146:INFO:Initializing create_model()
2023-05-20 15:16:58,146:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:58,146:INFO:Checking exceptions
2023-05-20 15:16:58,146:INFO:Importing libraries
2023-05-20 15:16:58,146:INFO:Copying training dataset
2023-05-20 15:16:58,158:INFO:Defining folds
2023-05-20 15:16:58,158:INFO:Declaring metric variables
2023-05-20 15:16:58,162:INFO:Importing untrained model
2023-05-20 15:16:58,165:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 15:16:58,170:INFO:Starting cross validation
2023-05-20 15:16:58,172:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:58,304:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,324:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,335:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,356:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,359:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,359:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,386:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,391:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,397:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:16:58,582:INFO:Calculating mean and std
2023-05-20 15:16:58,584:INFO:Creating metrics dataframe
2023-05-20 15:16:58,594:INFO:Uploading results into container
2023-05-20 15:16:58,595:INFO:Uploading model into container now
2023-05-20 15:16:58,595:INFO:_master_model_container: 6
2023-05-20 15:16:58,595:INFO:_display_container: 2
2023-05-20 15:16:58,596:INFO:LassoLars(random_state=1842)
2023-05-20 15:16:58,596:INFO:create_model() successfully completed......................................
2023-05-20 15:16:58,813:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:58,813:INFO:Creating metrics dataframe
2023-05-20 15:16:58,821:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 15:16:58,821:INFO:Total runtime is 0.2904666543006897 minutes
2023-05-20 15:16:58,824:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:58,825:INFO:Initializing create_model()
2023-05-20 15:16:58,825:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:58,825:INFO:Checking exceptions
2023-05-20 15:16:58,825:INFO:Importing libraries
2023-05-20 15:16:58,825:INFO:Copying training dataset
2023-05-20 15:16:58,834:INFO:Defining folds
2023-05-20 15:16:58,835:INFO:Declaring metric variables
2023-05-20 15:16:58,838:INFO:Importing untrained model
2023-05-20 15:16:58,841:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 15:16:58,846:INFO:Starting cross validation
2023-05-20 15:16:58,848:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:58,973:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:58,974:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:58,982:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:58,992:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:58,992:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,017:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,046:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,047:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,051:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,077:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:16:59,252:INFO:Calculating mean and std
2023-05-20 15:16:59,253:INFO:Creating metrics dataframe
2023-05-20 15:16:59,263:INFO:Uploading results into container
2023-05-20 15:16:59,263:INFO:Uploading model into container now
2023-05-20 15:16:59,264:INFO:_master_model_container: 7
2023-05-20 15:16:59,264:INFO:_display_container: 2
2023-05-20 15:16:59,264:INFO:OrthogonalMatchingPursuit()
2023-05-20 15:16:59,264:INFO:create_model() successfully completed......................................
2023-05-20 15:16:59,474:INFO:SubProcess create_model() end ==================================
2023-05-20 15:16:59,474:INFO:Creating metrics dataframe
2023-05-20 15:16:59,484:INFO:Initializing Bayesian Ridge
2023-05-20 15:16:59,484:INFO:Total runtime is 0.3015085577964783 minutes
2023-05-20 15:16:59,487:INFO:SubProcess create_model() called ==================================
2023-05-20 15:16:59,487:INFO:Initializing create_model()
2023-05-20 15:16:59,487:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:16:59,487:INFO:Checking exceptions
2023-05-20 15:16:59,487:INFO:Importing libraries
2023-05-20 15:16:59,487:INFO:Copying training dataset
2023-05-20 15:16:59,497:INFO:Defining folds
2023-05-20 15:16:59,497:INFO:Declaring metric variables
2023-05-20 15:16:59,501:INFO:Importing untrained model
2023-05-20 15:16:59,504:INFO:Bayesian Ridge Imported successfully
2023-05-20 15:16:59,509:INFO:Starting cross validation
2023-05-20 15:16:59,511:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:16:59,965:INFO:Calculating mean and std
2023-05-20 15:16:59,967:INFO:Creating metrics dataframe
2023-05-20 15:16:59,976:INFO:Uploading results into container
2023-05-20 15:16:59,977:INFO:Uploading model into container now
2023-05-20 15:16:59,977:INFO:_master_model_container: 8
2023-05-20 15:16:59,977:INFO:_display_container: 2
2023-05-20 15:16:59,977:INFO:BayesianRidge()
2023-05-20 15:16:59,977:INFO:create_model() successfully completed......................................
2023-05-20 15:17:00,219:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:00,219:INFO:Creating metrics dataframe
2023-05-20 15:17:00,228:INFO:Initializing Passive Aggressive Regressor
2023-05-20 15:17:00,228:INFO:Total runtime is 0.3139048496882121 minutes
2023-05-20 15:17:00,231:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:00,231:INFO:Initializing create_model()
2023-05-20 15:17:00,231:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:00,232:INFO:Checking exceptions
2023-05-20 15:17:00,232:INFO:Importing libraries
2023-05-20 15:17:00,232:INFO:Copying training dataset
2023-05-20 15:17:00,241:INFO:Defining folds
2023-05-20 15:17:00,241:INFO:Declaring metric variables
2023-05-20 15:17:00,245:INFO:Importing untrained model
2023-05-20 15:17:00,247:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 15:17:00,252:INFO:Starting cross validation
2023-05-20 15:17:00,254:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:00,696:INFO:Calculating mean and std
2023-05-20 15:17:00,697:INFO:Creating metrics dataframe
2023-05-20 15:17:00,708:INFO:Uploading results into container
2023-05-20 15:17:00,709:INFO:Uploading model into container now
2023-05-20 15:17:00,710:INFO:_master_model_container: 9
2023-05-20 15:17:00,710:INFO:_display_container: 2
2023-05-20 15:17:00,710:INFO:PassiveAggressiveRegressor(random_state=1842)
2023-05-20 15:17:00,710:INFO:create_model() successfully completed......................................
2023-05-20 15:17:00,968:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:00,968:INFO:Creating metrics dataframe
2023-05-20 15:17:00,981:INFO:Initializing Huber Regressor
2023-05-20 15:17:00,981:INFO:Total runtime is 0.3264524579048157 minutes
2023-05-20 15:17:00,986:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:00,986:INFO:Initializing create_model()
2023-05-20 15:17:00,986:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:00,986:INFO:Checking exceptions
2023-05-20 15:17:00,987:INFO:Importing libraries
2023-05-20 15:17:00,987:INFO:Copying training dataset
2023-05-20 15:17:01,005:INFO:Defining folds
2023-05-20 15:17:01,006:INFO:Declaring metric variables
2023-05-20 15:17:01,011:INFO:Importing untrained model
2023-05-20 15:17:01,014:INFO:Huber Regressor Imported successfully
2023-05-20 15:17:01,021:INFO:Starting cross validation
2023-05-20 15:17:01,022:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:04,221:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,392:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,420:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,512:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,518:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,579:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,620:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,633:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,655:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,677:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:17:04,933:INFO:Calculating mean and std
2023-05-20 15:17:04,935:INFO:Creating metrics dataframe
2023-05-20 15:17:04,950:INFO:Uploading results into container
2023-05-20 15:17:04,951:INFO:Uploading model into container now
2023-05-20 15:17:04,951:INFO:_master_model_container: 10
2023-05-20 15:17:04,953:INFO:_display_container: 2
2023-05-20 15:17:04,953:INFO:HuberRegressor()
2023-05-20 15:17:04,953:INFO:create_model() successfully completed......................................
2023-05-20 15:17:05,180:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:05,180:INFO:Creating metrics dataframe
2023-05-20 15:17:05,190:INFO:Initializing K Neighbors Regressor
2023-05-20 15:17:05,191:INFO:Total runtime is 0.3966259479522705 minutes
2023-05-20 15:17:05,194:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:05,194:INFO:Initializing create_model()
2023-05-20 15:17:05,194:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:05,194:INFO:Checking exceptions
2023-05-20 15:17:05,194:INFO:Importing libraries
2023-05-20 15:17:05,194:INFO:Copying training dataset
2023-05-20 15:17:05,206:INFO:Defining folds
2023-05-20 15:17:05,206:INFO:Declaring metric variables
2023-05-20 15:17:05,209:INFO:Importing untrained model
2023-05-20 15:17:05,212:INFO:K Neighbors Regressor Imported successfully
2023-05-20 15:17:05,218:INFO:Starting cross validation
2023-05-20 15:17:05,219:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:05,717:INFO:Calculating mean and std
2023-05-20 15:17:05,718:INFO:Creating metrics dataframe
2023-05-20 15:17:05,732:INFO:Uploading results into container
2023-05-20 15:17:05,733:INFO:Uploading model into container now
2023-05-20 15:17:05,733:INFO:_master_model_container: 11
2023-05-20 15:17:05,733:INFO:_display_container: 2
2023-05-20 15:17:05,734:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 15:17:05,734:INFO:create_model() successfully completed......................................
2023-05-20 15:17:05,947:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:05,947:INFO:Creating metrics dataframe
2023-05-20 15:17:05,958:INFO:Initializing Decision Tree Regressor
2023-05-20 15:17:05,959:INFO:Total runtime is 0.4094250241915385 minutes
2023-05-20 15:17:05,963:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:05,963:INFO:Initializing create_model()
2023-05-20 15:17:05,963:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:05,964:INFO:Checking exceptions
2023-05-20 15:17:05,964:INFO:Importing libraries
2023-05-20 15:17:05,964:INFO:Copying training dataset
2023-05-20 15:17:05,975:INFO:Defining folds
2023-05-20 15:17:05,975:INFO:Declaring metric variables
2023-05-20 15:17:05,978:INFO:Importing untrained model
2023-05-20 15:17:05,980:INFO:Decision Tree Regressor Imported successfully
2023-05-20 15:17:05,985:INFO:Starting cross validation
2023-05-20 15:17:05,987:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:06,455:INFO:Calculating mean and std
2023-05-20 15:17:06,457:INFO:Creating metrics dataframe
2023-05-20 15:17:06,474:INFO:Uploading results into container
2023-05-20 15:17:06,475:INFO:Uploading model into container now
2023-05-20 15:17:06,475:INFO:_master_model_container: 12
2023-05-20 15:17:06,476:INFO:_display_container: 2
2023-05-20 15:17:06,476:INFO:DecisionTreeRegressor(random_state=1842)
2023-05-20 15:17:06,476:INFO:create_model() successfully completed......................................
2023-05-20 15:17:06,689:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:06,690:INFO:Creating metrics dataframe
2023-05-20 15:17:06,705:INFO:Initializing Random Forest Regressor
2023-05-20 15:17:06,705:INFO:Total runtime is 0.42186043659845984 minutes
2023-05-20 15:17:06,707:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:06,708:INFO:Initializing create_model()
2023-05-20 15:17:06,708:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:06,708:INFO:Checking exceptions
2023-05-20 15:17:06,708:INFO:Importing libraries
2023-05-20 15:17:06,708:INFO:Copying training dataset
2023-05-20 15:17:06,717:INFO:Defining folds
2023-05-20 15:17:06,717:INFO:Declaring metric variables
2023-05-20 15:17:06,720:INFO:Importing untrained model
2023-05-20 15:17:06,723:INFO:Random Forest Regressor Imported successfully
2023-05-20 15:17:06,728:INFO:Starting cross validation
2023-05-20 15:17:06,731:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:07,632:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:238: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = self._memory_transform(

2023-05-20 15:17:09,020:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:09,081:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:09,101:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.94s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:09,770:INFO:Calculating mean and std
2023-05-20 15:17:09,772:INFO:Creating metrics dataframe
2023-05-20 15:17:09,791:INFO:Uploading results into container
2023-05-20 15:17:09,792:INFO:Uploading model into container now
2023-05-20 15:17:09,792:INFO:_master_model_container: 13
2023-05-20 15:17:09,792:INFO:_display_container: 2
2023-05-20 15:17:09,792:INFO:RandomForestRegressor(n_jobs=-1, random_state=1842)
2023-05-20 15:17:09,792:INFO:create_model() successfully completed......................................
2023-05-20 15:17:10,005:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:10,005:INFO:Creating metrics dataframe
2023-05-20 15:17:10,015:INFO:Initializing Extra Trees Regressor
2023-05-20 15:17:10,016:INFO:Total runtime is 0.4770419398943583 minutes
2023-05-20 15:17:10,018:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:10,018:INFO:Initializing create_model()
2023-05-20 15:17:10,019:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:10,019:INFO:Checking exceptions
2023-05-20 15:17:10,019:INFO:Importing libraries
2023-05-20 15:17:10,019:INFO:Copying training dataset
2023-05-20 15:17:10,034:INFO:Defining folds
2023-05-20 15:17:10,034:INFO:Declaring metric variables
2023-05-20 15:17:10,038:INFO:Importing untrained model
2023-05-20 15:17:10,042:INFO:Extra Trees Regressor Imported successfully
2023-05-20 15:17:10,049:INFO:Starting cross validation
2023-05-20 15:17:10,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:11,721:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.95s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:12,539:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.30s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:13,016:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:17:13,593:INFO:Calculating mean and std
2023-05-20 15:17:13,595:INFO:Creating metrics dataframe
2023-05-20 15:17:13,618:INFO:Uploading results into container
2023-05-20 15:17:13,618:INFO:Uploading model into container now
2023-05-20 15:17:13,619:INFO:_master_model_container: 14
2023-05-20 15:17:13,619:INFO:_display_container: 2
2023-05-20 15:17:13,619:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=1842)
2023-05-20 15:17:13,619:INFO:create_model() successfully completed......................................
2023-05-20 15:17:13,841:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:13,841:INFO:Creating metrics dataframe
2023-05-20 15:17:13,852:INFO:Initializing AdaBoost Regressor
2023-05-20 15:17:13,852:INFO:Total runtime is 0.5409767786661783 minutes
2023-05-20 15:17:13,855:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:13,855:INFO:Initializing create_model()
2023-05-20 15:17:13,855:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:13,855:INFO:Checking exceptions
2023-05-20 15:17:13,855:INFO:Importing libraries
2023-05-20 15:17:13,855:INFO:Copying training dataset
2023-05-20 15:17:13,865:INFO:Defining folds
2023-05-20 15:17:13,865:INFO:Declaring metric variables
2023-05-20 15:17:13,869:INFO:Importing untrained model
2023-05-20 15:17:13,872:INFO:AdaBoost Regressor Imported successfully
2023-05-20 15:17:13,878:INFO:Starting cross validation
2023-05-20 15:17:13,879:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:15,737:INFO:Calculating mean and std
2023-05-20 15:17:15,739:INFO:Creating metrics dataframe
2023-05-20 15:17:15,768:INFO:Uploading results into container
2023-05-20 15:17:15,769:INFO:Uploading model into container now
2023-05-20 15:17:15,769:INFO:_master_model_container: 15
2023-05-20 15:17:15,769:INFO:_display_container: 2
2023-05-20 15:17:15,770:INFO:AdaBoostRegressor(random_state=1842)
2023-05-20 15:17:15,770:INFO:create_model() successfully completed......................................
2023-05-20 15:17:15,990:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:15,990:INFO:Creating metrics dataframe
2023-05-20 15:17:16,002:INFO:Initializing Gradient Boosting Regressor
2023-05-20 15:17:16,002:INFO:Total runtime is 0.5768097440401713 minutes
2023-05-20 15:17:16,005:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:16,005:INFO:Initializing create_model()
2023-05-20 15:17:16,006:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:16,006:INFO:Checking exceptions
2023-05-20 15:17:16,006:INFO:Importing libraries
2023-05-20 15:17:16,006:INFO:Copying training dataset
2023-05-20 15:17:16,017:INFO:Defining folds
2023-05-20 15:17:16,017:INFO:Declaring metric variables
2023-05-20 15:17:16,021:INFO:Importing untrained model
2023-05-20 15:17:16,024:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 15:17:16,030:INFO:Starting cross validation
2023-05-20 15:17:16,031:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:17,930:INFO:Calculating mean and std
2023-05-20 15:17:17,932:INFO:Creating metrics dataframe
2023-05-20 15:17:17,967:INFO:Uploading results into container
2023-05-20 15:17:17,968:INFO:Uploading model into container now
2023-05-20 15:17:17,968:INFO:_master_model_container: 16
2023-05-20 15:17:17,968:INFO:_display_container: 2
2023-05-20 15:17:17,968:INFO:GradientBoostingRegressor(random_state=1842)
2023-05-20 15:17:17,968:INFO:create_model() successfully completed......................................
2023-05-20 15:17:18,190:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:18,190:INFO:Creating metrics dataframe
2023-05-20 15:17:18,201:INFO:Initializing Extreme Gradient Boosting
2023-05-20 15:17:18,201:INFO:Total runtime is 0.6134531140327453 minutes
2023-05-20 15:17:18,204:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:18,205:INFO:Initializing create_model()
2023-05-20 15:17:18,205:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:18,205:INFO:Checking exceptions
2023-05-20 15:17:18,205:INFO:Importing libraries
2023-05-20 15:17:18,205:INFO:Copying training dataset
2023-05-20 15:17:18,216:INFO:Defining folds
2023-05-20 15:17:18,216:INFO:Declaring metric variables
2023-05-20 15:17:18,220:INFO:Importing untrained model
2023-05-20 15:17:18,222:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 15:17:18,228:INFO:Starting cross validation
2023-05-20 15:17:18,229:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:20,903:INFO:Calculating mean and std
2023-05-20 15:17:20,905:INFO:Creating metrics dataframe
2023-05-20 15:17:20,951:INFO:Uploading results into container
2023-05-20 15:17:20,952:INFO:Uploading model into container now
2023-05-20 15:17:20,953:INFO:_master_model_container: 17
2023-05-20 15:17:20,953:INFO:_display_container: 2
2023-05-20 15:17:20,953:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=1842, ...)
2023-05-20 15:17:20,954:INFO:create_model() successfully completed......................................
2023-05-20 15:17:21,234:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:21,234:INFO:Creating metrics dataframe
2023-05-20 15:17:21,245:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 15:17:21,245:INFO:Total runtime is 0.6641971071561177 minutes
2023-05-20 15:17:21,247:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:21,249:INFO:Initializing create_model()
2023-05-20 15:17:21,249:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:21,249:INFO:Checking exceptions
2023-05-20 15:17:21,249:INFO:Importing libraries
2023-05-20 15:17:21,249:INFO:Copying training dataset
2023-05-20 15:17:21,259:INFO:Defining folds
2023-05-20 15:17:21,259:INFO:Declaring metric variables
2023-05-20 15:17:21,262:INFO:Importing untrained model
2023-05-20 15:17:21,265:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 15:17:21,270:INFO:Starting cross validation
2023-05-20 15:17:21,272:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:22,859:INFO:Calculating mean and std
2023-05-20 15:17:22,861:INFO:Creating metrics dataframe
2023-05-20 15:17:22,906:INFO:Uploading results into container
2023-05-20 15:17:22,907:INFO:Uploading model into container now
2023-05-20 15:17:22,907:INFO:_master_model_container: 18
2023-05-20 15:17:22,907:INFO:_display_container: 2
2023-05-20 15:17:22,908:INFO:LGBMRegressor(random_state=1842)
2023-05-20 15:17:22,908:INFO:create_model() successfully completed......................................
2023-05-20 15:17:23,229:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:23,229:INFO:Creating metrics dataframe
2023-05-20 15:17:23,248:INFO:Initializing CatBoost Regressor
2023-05-20 15:17:23,248:INFO:Total runtime is 0.6975840648015339 minutes
2023-05-20 15:17:23,252:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:23,252:INFO:Initializing create_model()
2023-05-20 15:17:23,252:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:23,252:INFO:Checking exceptions
2023-05-20 15:17:23,252:INFO:Importing libraries
2023-05-20 15:17:23,252:INFO:Copying training dataset
2023-05-20 15:17:23,265:INFO:Defining folds
2023-05-20 15:17:23,265:INFO:Declaring metric variables
2023-05-20 15:17:23,268:INFO:Importing untrained model
2023-05-20 15:17:23,274:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:17:23,280:INFO:Starting cross validation
2023-05-20 15:17:23,282:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:56,889:INFO:Calculating mean and std
2023-05-20 15:17:56,892:INFO:Creating metrics dataframe
2023-05-20 15:17:56,947:INFO:Uploading results into container
2023-05-20 15:17:56,947:INFO:Uploading model into container now
2023-05-20 15:17:56,948:INFO:_master_model_container: 19
2023-05-20 15:17:56,948:INFO:_display_container: 2
2023-05-20 15:17:56,948:INFO:<catboost.core.CatBoostRegressor object at 0x000002CE8B225B40>
2023-05-20 15:17:56,948:INFO:create_model() successfully completed......................................
2023-05-20 15:17:57,339:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:57,339:INFO:Creating metrics dataframe
2023-05-20 15:17:57,351:INFO:Initializing Dummy Regressor
2023-05-20 15:17:57,351:INFO:Total runtime is 1.2659606178601581 minutes
2023-05-20 15:17:57,355:INFO:SubProcess create_model() called ==================================
2023-05-20 15:17:57,356:INFO:Initializing create_model()
2023-05-20 15:17:57,356:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002CE9639EFE0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:57,356:INFO:Checking exceptions
2023-05-20 15:17:57,356:INFO:Importing libraries
2023-05-20 15:17:57,356:INFO:Copying training dataset
2023-05-20 15:17:57,372:INFO:Defining folds
2023-05-20 15:17:57,372:INFO:Declaring metric variables
2023-05-20 15:17:57,376:INFO:Importing untrained model
2023-05-20 15:17:57,379:INFO:Dummy Regressor Imported successfully
2023-05-20 15:17:57,384:INFO:Starting cross validation
2023-05-20 15:17:57,386:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:17:58,275:INFO:Calculating mean and std
2023-05-20 15:17:58,277:INFO:Creating metrics dataframe
2023-05-20 15:17:58,331:INFO:Uploading results into container
2023-05-20 15:17:58,333:INFO:Uploading model into container now
2023-05-20 15:17:58,333:INFO:_master_model_container: 20
2023-05-20 15:17:58,333:INFO:_display_container: 2
2023-05-20 15:17:58,333:INFO:DummyRegressor()
2023-05-20 15:17:58,333:INFO:create_model() successfully completed......................................
2023-05-20 15:17:58,614:INFO:SubProcess create_model() end ==================================
2023-05-20 15:17:58,614:INFO:Creating metrics dataframe
2023-05-20 15:17:58,638:INFO:Initializing create_model()
2023-05-20 15:17:58,638:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002CE97D9EEC0>, estimator=<catboost.core.CatBoostRegressor object at 0x000002CE8B225B40>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:17:58,638:INFO:Checking exceptions
2023-05-20 15:17:58,640:INFO:Importing libraries
2023-05-20 15:17:58,640:INFO:Copying training dataset
2023-05-20 15:17:58,652:INFO:Defining folds
2023-05-20 15:17:58,652:INFO:Declaring metric variables
2023-05-20 15:17:58,652:INFO:Importing untrained model
2023-05-20 15:17:58,652:INFO:Declaring custom model
2023-05-20 15:17:58,652:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:17:58,653:INFO:Cross validation set to False
2023-05-20 15:17:58,653:INFO:Fitting Model
2023-05-20 15:18:02,817:INFO:<catboost.core.CatBoostRegressor object at 0x000002CE8B225D50>
2023-05-20 15:18:02,817:INFO:create_model() successfully completed......................................
2023-05-20 15:18:03,111:INFO:_master_model_container: 20
2023-05-20 15:18:03,112:INFO:_display_container: 2
2023-05-20 15:18:03,112:INFO:<catboost.core.CatBoostRegressor object at 0x000002CE8B225D50>
2023-05-20 15:18:03,112:INFO:compare_models() successfully completed......................................
2023-05-20 15:27:57,597:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:27:57,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:27:57,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:27:57,911:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:27:59,540:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-20 15:28:03,139:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_14724\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 15:28:03,234:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_14724\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 15:28:13,696:INFO:PyCaret RegressionExperiment
2023-05-20 15:28:13,696:INFO:Logging name: reg-default-name
2023-05-20 15:28:13,696:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:28:13,696:INFO:version 3.0.2
2023-05-20 15:28:13,696:INFO:Initializing setup()
2023-05-20 15:28:13,696:INFO:self.USI: 3dd7
2023-05-20 15:28:13,696:INFO:self._variable_keys: {'log_plots_param', '_ml_usecase', 'y_test', 'fold_shuffle_param', 'X_train', 'gpu_param', 'pipeline', 'html_param', 'n_jobs_param', 'X', 'target_param', 'memory', 'y_train', 'data', 'X_test', 'idx', 'exp_id', 'transform_target_param', '_available_plots', 'exp_name_log', 'fold_generator', 'y', 'seed', 'USI', 'gpu_n_jobs_param', 'logging_param', 'fold_groups_param'}
2023-05-20 15:28:13,696:INFO:Checking environment
2023-05-20 15:28:13,696:INFO:python_version: 3.10.3
2023-05-20 15:28:13,696:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:28:13,696:INFO:machine: AMD64
2023-05-20 15:28:13,696:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:28:13,696:INFO:Memory: svmem(total=17083187200, available=6679257088, percent=60.9, used=10403930112, free=6679257088)
2023-05-20 15:28:13,696:INFO:Physical Core: 6
2023-05-20 15:28:13,696:INFO:Logical Core: 12
2023-05-20 15:28:13,696:INFO:Checking libraries
2023-05-20 15:28:13,696:INFO:System:
2023-05-20 15:28:13,696:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:28:13,696:INFO:executable: c:\Python310\python.exe
2023-05-20 15:28:13,697:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:28:13,697:INFO:PyCaret required dependencies:
2023-05-20 15:28:13,697:INFO:                 pip: 23.1.2
2023-05-20 15:28:13,697:INFO:          setuptools: 58.1.0
2023-05-20 15:28:13,697:INFO:             pycaret: 3.0.2
2023-05-20 15:28:13,697:INFO:             IPython: 8.5.0
2023-05-20 15:28:13,697:INFO:          ipywidgets: 8.0.6
2023-05-20 15:28:13,697:INFO:                tqdm: 4.65.0
2023-05-20 15:28:13,697:INFO:               numpy: 1.23.2
2023-05-20 15:28:13,697:INFO:              pandas: 1.5.2
2023-05-20 15:28:13,697:INFO:              jinja2: 3.1.2
2023-05-20 15:28:13,697:INFO:               scipy: 1.9.3
2023-05-20 15:28:13,697:INFO:              joblib: 1.2.0
2023-05-20 15:28:13,697:INFO:             sklearn: 1.1.3
2023-05-20 15:28:13,697:INFO:                pyod: 1.0.9
2023-05-20 15:28:13,697:INFO:            imblearn: 0.10.1
2023-05-20 15:28:13,697:INFO:   category_encoders: 2.6.1
2023-05-20 15:28:13,697:INFO:            lightgbm: 3.3.5
2023-05-20 15:28:13,697:INFO:               numba: 0.57.0
2023-05-20 15:28:13,697:INFO:            requests: 2.28.2
2023-05-20 15:28:13,698:INFO:          matplotlib: 3.5.3
2023-05-20 15:28:13,698:INFO:          scikitplot: 0.3.7
2023-05-20 15:28:13,698:INFO:         yellowbrick: 1.5
2023-05-20 15:28:13,698:INFO:              plotly: 5.13.1
2023-05-20 15:28:13,698:INFO:             kaleido: 0.2.1
2023-05-20 15:28:13,698:INFO:         statsmodels: 0.13.5
2023-05-20 15:28:13,698:INFO:              sktime: 0.17.0
2023-05-20 15:28:13,698:INFO:               tbats: 1.1.3
2023-05-20 15:28:13,698:INFO:            pmdarima: 2.0.3
2023-05-20 15:28:13,698:INFO:              psutil: 5.9.2
2023-05-20 15:28:13,698:INFO:PyCaret optional dependencies:
2023-05-20 15:28:13,711:INFO:                shap: Not installed
2023-05-20 15:28:13,711:INFO:           interpret: Not installed
2023-05-20 15:28:13,711:INFO:                umap: Not installed
2023-05-20 15:28:13,711:INFO:    pandas_profiling: Not installed
2023-05-20 15:28:13,711:INFO:  explainerdashboard: Not installed
2023-05-20 15:28:13,711:INFO:             autoviz: Not installed
2023-05-20 15:28:13,711:INFO:           fairlearn: Not installed
2023-05-20 15:28:13,711:INFO:             xgboost: 1.7.4
2023-05-20 15:28:13,711:INFO:            catboost: 1.1.1
2023-05-20 15:28:13,711:INFO:              kmodes: Not installed
2023-05-20 15:28:13,711:INFO:             mlxtend: Not installed
2023-05-20 15:28:13,711:INFO:       statsforecast: Not installed
2023-05-20 15:28:13,711:INFO:        tune_sklearn: Not installed
2023-05-20 15:28:13,711:INFO:                 ray: Not installed
2023-05-20 15:28:13,711:INFO:            hyperopt: Not installed
2023-05-20 15:28:13,711:INFO:              optuna: Not installed
2023-05-20 15:28:13,711:INFO:               skopt: Not installed
2023-05-20 15:28:13,711:INFO:              mlflow: Not installed
2023-05-20 15:28:13,711:INFO:              gradio: Not installed
2023-05-20 15:28:13,712:INFO:             fastapi: Not installed
2023-05-20 15:28:13,712:INFO:             uvicorn: Not installed
2023-05-20 15:28:13,712:INFO:              m2cgen: Not installed
2023-05-20 15:28:13,712:INFO:           evidently: Not installed
2023-05-20 15:28:13,712:INFO:               fugue: Not installed
2023-05-20 15:28:13,712:INFO:           streamlit: Not installed
2023-05-20 15:28:13,712:INFO:             prophet: Not installed
2023-05-20 15:28:13,712:INFO:None
2023-05-20 15:28:13,712:INFO:Set up data.
2023-05-20 15:28:13,790:INFO:Set up train/test split.
2023-05-20 15:28:13,801:INFO:Set up index.
2023-05-20 15:28:13,801:INFO:Set up folding strategy.
2023-05-20 15:28:13,801:INFO:Assigning column types.
2023-05-20 15:28:13,808:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:28:13,809:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:28:13,812:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:28:13,816:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:13,867:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:13,906:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:13,906:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:13,999:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,083:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,087:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,091:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,143:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,182:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,182:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,185:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,185:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:28:14,190:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,194:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,249:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,288:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,288:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,291:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,295:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,299:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,351:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,391:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,392:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,394:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,394:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:28:14,402:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,462:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,501:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,501:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,503:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,514:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,565:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,605:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,606:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,608:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,608:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:28:14,670:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,710:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,710:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,713:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,773:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,812:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,813:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,815:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,815:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:28:14,874:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:14,913:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:14,916:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:14,977:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:28:15,018:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:15,020:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:15,020:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:28:15,124:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:15,127:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:15,227:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:15,227:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:15,231:INFO:Preparing preprocessing pipeline...
2023-05-20 15:28:15,231:INFO:Set up simple imputation.
2023-05-20 15:28:15,233:INFO:Set up column name cleaning.
2023-05-20 15:28:15,272:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:28:15,280:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:28:15,280:INFO:Creating final display dataframe.
2023-05-20 15:28:15,428:INFO:Setup _display_container:                     Description             Value
0                    Session id              6582
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              3dd7
2023-05-20 15:28:15,541:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:15,543:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:15,645:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:28:15,647:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:28:15,648:INFO:setup() successfully completed in 2.02s...............
2023-05-20 15:28:19,737:INFO:Initializing compare_models()
2023-05-20 15:28:19,737:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 15:28:19,738:INFO:Checking exceptions
2023-05-20 15:28:19,746:INFO:Preparing display monitor
2023-05-20 15:28:19,806:INFO:Initializing Linear Regression
2023-05-20 15:28:19,806:INFO:Total runtime is 0.0 minutes
2023-05-20 15:28:19,810:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:19,810:INFO:Initializing create_model()
2023-05-20 15:28:19,810:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:19,810:INFO:Checking exceptions
2023-05-20 15:28:19,811:INFO:Importing libraries
2023-05-20 15:28:19,811:INFO:Copying training dataset
2023-05-20 15:28:19,826:INFO:Defining folds
2023-05-20 15:28:19,826:INFO:Declaring metric variables
2023-05-20 15:28:19,831:INFO:Importing untrained model
2023-05-20 15:28:19,835:INFO:Linear Regression Imported successfully
2023-05-20 15:28:19,843:INFO:Starting cross validation
2023-05-20 15:28:19,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:25,230:INFO:Calculating mean and std
2023-05-20 15:28:25,231:INFO:Creating metrics dataframe
2023-05-20 15:28:25,288:INFO:Uploading results into container
2023-05-20 15:28:25,288:INFO:Uploading model into container now
2023-05-20 15:28:25,289:INFO:_master_model_container: 1
2023-05-20 15:28:25,289:INFO:_display_container: 2
2023-05-20 15:28:25,289:INFO:LinearRegression(n_jobs=-1)
2023-05-20 15:28:25,289:INFO:create_model() successfully completed......................................
2023-05-20 15:28:25,362:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:25,362:INFO:Creating metrics dataframe
2023-05-20 15:28:25,370:INFO:Initializing Lasso Regression
2023-05-20 15:28:25,370:INFO:Total runtime is 0.09273370107014973 minutes
2023-05-20 15:28:25,375:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:25,375:INFO:Initializing create_model()
2023-05-20 15:28:25,375:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:25,375:INFO:Checking exceptions
2023-05-20 15:28:25,375:INFO:Importing libraries
2023-05-20 15:28:25,375:INFO:Copying training dataset
2023-05-20 15:28:25,387:INFO:Defining folds
2023-05-20 15:28:25,387:INFO:Declaring metric variables
2023-05-20 15:28:25,392:INFO:Importing untrained model
2023-05-20 15:28:25,397:INFO:Lasso Regression Imported successfully
2023-05-20 15:28:25,403:INFO:Starting cross validation
2023-05-20 15:28:25,405:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:27,804:INFO:Calculating mean and std
2023-05-20 15:28:27,805:INFO:Creating metrics dataframe
2023-05-20 15:28:27,856:INFO:Uploading results into container
2023-05-20 15:28:27,857:INFO:Uploading model into container now
2023-05-20 15:28:27,857:INFO:_master_model_container: 2
2023-05-20 15:28:27,857:INFO:_display_container: 2
2023-05-20 15:28:27,858:INFO:Lasso(random_state=6582)
2023-05-20 15:28:27,858:INFO:create_model() successfully completed......................................
2023-05-20 15:28:27,924:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:27,924:INFO:Creating metrics dataframe
2023-05-20 15:28:27,933:INFO:Initializing Ridge Regression
2023-05-20 15:28:27,933:INFO:Total runtime is 0.13545803229014078 minutes
2023-05-20 15:28:27,936:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:27,936:INFO:Initializing create_model()
2023-05-20 15:28:27,936:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:27,936:INFO:Checking exceptions
2023-05-20 15:28:27,937:INFO:Importing libraries
2023-05-20 15:28:27,937:INFO:Copying training dataset
2023-05-20 15:28:27,949:INFO:Defining folds
2023-05-20 15:28:27,949:INFO:Declaring metric variables
2023-05-20 15:28:27,952:INFO:Importing untrained model
2023-05-20 15:28:27,956:INFO:Ridge Regression Imported successfully
2023-05-20 15:28:27,962:INFO:Starting cross validation
2023-05-20 15:28:27,964:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:28,779:INFO:Calculating mean and std
2023-05-20 15:28:28,781:INFO:Creating metrics dataframe
2023-05-20 15:28:28,833:INFO:Uploading results into container
2023-05-20 15:28:28,834:INFO:Uploading model into container now
2023-05-20 15:28:28,834:INFO:_master_model_container: 3
2023-05-20 15:28:28,834:INFO:_display_container: 2
2023-05-20 15:28:28,834:INFO:Ridge(random_state=6582)
2023-05-20 15:28:28,834:INFO:create_model() successfully completed......................................
2023-05-20 15:28:28,906:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:28,906:INFO:Creating metrics dataframe
2023-05-20 15:28:28,915:INFO:Initializing Elastic Net
2023-05-20 15:28:28,915:INFO:Total runtime is 0.15181437333424885 minutes
2023-05-20 15:28:28,918:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:28,919:INFO:Initializing create_model()
2023-05-20 15:28:28,919:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:28,919:INFO:Checking exceptions
2023-05-20 15:28:28,919:INFO:Importing libraries
2023-05-20 15:28:28,919:INFO:Copying training dataset
2023-05-20 15:28:28,929:INFO:Defining folds
2023-05-20 15:28:28,929:INFO:Declaring metric variables
2023-05-20 15:28:28,933:INFO:Importing untrained model
2023-05-20 15:28:28,936:INFO:Elastic Net Imported successfully
2023-05-20 15:28:28,943:INFO:Starting cross validation
2023-05-20 15:28:28,944:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:29,762:INFO:Calculating mean and std
2023-05-20 15:28:29,764:INFO:Creating metrics dataframe
2023-05-20 15:28:29,820:INFO:Uploading results into container
2023-05-20 15:28:29,821:INFO:Uploading model into container now
2023-05-20 15:28:29,821:INFO:_master_model_container: 4
2023-05-20 15:28:29,822:INFO:_display_container: 2
2023-05-20 15:28:29,822:INFO:ElasticNet(random_state=6582)
2023-05-20 15:28:29,822:INFO:create_model() successfully completed......................................
2023-05-20 15:28:29,911:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:29,912:INFO:Creating metrics dataframe
2023-05-20 15:28:29,922:INFO:Initializing Least Angle Regression
2023-05-20 15:28:29,922:INFO:Total runtime is 0.1685990571975708 minutes
2023-05-20 15:28:29,925:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:29,925:INFO:Initializing create_model()
2023-05-20 15:28:29,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:29,926:INFO:Checking exceptions
2023-05-20 15:28:29,926:INFO:Importing libraries
2023-05-20 15:28:29,926:INFO:Copying training dataset
2023-05-20 15:28:29,937:INFO:Defining folds
2023-05-20 15:28:29,937:INFO:Declaring metric variables
2023-05-20 15:28:29,942:INFO:Importing untrained model
2023-05-20 15:28:29,945:INFO:Least Angle Regression Imported successfully
2023-05-20 15:28:29,952:INFO:Starting cross validation
2023-05-20 15:28:29,954:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:30,074:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,092:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,094:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,099:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.468e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,100:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.189e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,104:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.429e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,104:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.887e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,105:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,107:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=5.083e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,112:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,113:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.304e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,115:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.362e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,115:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.213e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,118:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.022e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,124:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.721e-04, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,127:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=4.830e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,129:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,130:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.827e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,135:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.068e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,138:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.939e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,139:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.691e-04, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,140:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,142:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.035e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,143:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.093e-04, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,145:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.131e-04, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,146:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.027e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,147:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=8.769e-05, with an active set of 145 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,149:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,150:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.782e-05, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,151:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=7.848e-05, with an active set of 152 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,152:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.950e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,152:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.354e-04, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=2.703e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,154:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=2.417e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,156:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.325e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,159:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.022e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,159:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.315e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,161:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=1.169e-04, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,165:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:30,165:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.552e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,166:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.984e-04, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,169:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.935e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,173:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 230 iterations, i.e. alpha=1.015e+02, with an active set of 171 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,174:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=2.023e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,182:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.055e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,186:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.186e-04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,188:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=6.041e-05, with an active set of 152 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,189:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.101e-04, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,193:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 168 iterations, i.e. alpha=5.149e-05, with an active set of 161 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,194:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.819e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,200:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=2.157e-04, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,201:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 208 iterations, i.e. alpha=2.863e-05, with an active set of 203 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,203:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.829e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,204:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 225 iterations, i.e. alpha=1.971e-04, with an active set of 188 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 199 iterations, i.e. alpha=4.343e-05, with an active set of 187 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,212:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 240 iterations, i.e. alpha=2.019e-04, with an active set of 201 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,220:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 181 iterations, i.e. alpha=6.853e-05, with an active set of 174 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,222:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=6.592e-04, with an active set of 170 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,224:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:28:30,226:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 265 iterations, i.e. alpha=2.543e-04, with an active set of 218 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,233:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=7.492e-05, with an active set of 189 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,240:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 216 iterations, i.e. alpha=7.045e-05, with an active set of 197 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,260:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:28:30,262:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:28:30,264:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:28:30,267:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:28:30,274:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:28:30,276:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:28:30,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:28:30,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:28:30,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:28:30,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:28:30,298:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:28:30,299:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:28:30,299:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:28:30,316:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 329 iterations, i.e. alpha=2.880e+00, with an active set of 241 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:28:30,362:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:28:30,536:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:30,581:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:30,583:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:30,585:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:30,615:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:30,619:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:30,625:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,625:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,626:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:30,627:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,627:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,628:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:30,629:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,630:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,631:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:30,644:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,645:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,645:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:30,655:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,657:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,657:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:30,675:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,676:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:30,676:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,238:INFO:Calculating mean and std
2023-05-20 15:28:31,239:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:28:31,241:INFO:Creating metrics dataframe
2023-05-20 15:28:31,297:INFO:Uploading results into container
2023-05-20 15:28:31,297:INFO:Uploading model into container now
2023-05-20 15:28:31,298:INFO:_master_model_container: 5
2023-05-20 15:28:31,298:INFO:_display_container: 2
2023-05-20 15:28:31,298:INFO:Lars(random_state=6582)
2023-05-20 15:28:31,298:INFO:create_model() successfully completed......................................
2023-05-20 15:28:31,365:WARNING:create_model() for Lars(random_state=6582) raised an exception or returned all 0.0, trying without fit_kwargs:
2023-05-20 15:28:31,366:WARNING:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

2023-05-20 15:28:31,366:INFO:Initializing create_model()
2023-05-20 15:28:31,366:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:31,366:INFO:Checking exceptions
2023-05-20 15:28:31,366:INFO:Importing libraries
2023-05-20 15:28:31,367:INFO:Copying training dataset
2023-05-20 15:28:31,381:INFO:Defining folds
2023-05-20 15:28:31,381:INFO:Declaring metric variables
2023-05-20 15:28:31,384:INFO:Importing untrained model
2023-05-20 15:28:31,388:INFO:Least Angle Regression Imported successfully
2023-05-20 15:28:31,394:INFO:Starting cross validation
2023-05-20 15:28:31,396:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:31,629:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:31,629:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:31,630:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:31,633:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:31,640:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,641:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,641:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,659:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,659:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,660:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,672:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:28:31,675:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:28:31,680:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,681:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,681:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,683:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,684:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,690:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,691:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,691:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:31,703:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,703:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:28:31,703:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:28:32,266:INFO:Calculating mean and std
2023-05-20 15:28:32,267:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:28:32,268:INFO:Creating metrics dataframe
2023-05-20 15:28:32,324:INFO:Uploading results into container
2023-05-20 15:28:32,324:INFO:Uploading model into container now
2023-05-20 15:28:32,324:INFO:_master_model_container: 6
2023-05-20 15:28:32,325:INFO:_display_container: 2
2023-05-20 15:28:32,325:INFO:Lars(random_state=6582)
2023-05-20 15:28:32,325:INFO:create_model() successfully completed......................................
2023-05-20 15:28:32,389:ERROR:create_model() for Lars(random_state=6582) raised an exception or returned all 0.0:
2023-05-20 15:28:32,390:ERROR:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 810, in compare_models
    assert (
AssertionError

2023-05-20 15:28:32,390:INFO:Initializing Lasso Least Angle Regression
2023-05-20 15:28:32,390:INFO:Total runtime is 0.20973158677419027 minutes
2023-05-20 15:28:32,393:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:32,393:INFO:Initializing create_model()
2023-05-20 15:28:32,394:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:32,394:INFO:Checking exceptions
2023-05-20 15:28:32,394:INFO:Importing libraries
2023-05-20 15:28:32,394:INFO:Copying training dataset
2023-05-20 15:28:32,405:INFO:Defining folds
2023-05-20 15:28:32,405:INFO:Declaring metric variables
2023-05-20 15:28:32,408:INFO:Importing untrained model
2023-05-20 15:28:32,412:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 15:28:32,417:INFO:Starting cross validation
2023-05-20 15:28:32,420:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:32,542:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,546:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,559:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,566:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,584:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,593:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,596:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,607:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,612:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:32,619:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:28:33,256:INFO:Calculating mean and std
2023-05-20 15:28:33,257:INFO:Creating metrics dataframe
2023-05-20 15:28:33,315:INFO:Uploading results into container
2023-05-20 15:28:33,315:INFO:Uploading model into container now
2023-05-20 15:28:33,316:INFO:_master_model_container: 7
2023-05-20 15:28:33,316:INFO:_display_container: 2
2023-05-20 15:28:33,316:INFO:LassoLars(random_state=6582)
2023-05-20 15:28:33,316:INFO:create_model() successfully completed......................................
2023-05-20 15:28:33,374:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:33,374:INFO:Creating metrics dataframe
2023-05-20 15:28:33,383:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 15:28:33,383:INFO:Total runtime is 0.2262887477874756 minutes
2023-05-20 15:28:33,386:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:33,386:INFO:Initializing create_model()
2023-05-20 15:28:33,386:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:33,386:INFO:Checking exceptions
2023-05-20 15:28:33,387:INFO:Importing libraries
2023-05-20 15:28:33,387:INFO:Copying training dataset
2023-05-20 15:28:33,397:INFO:Defining folds
2023-05-20 15:28:33,397:INFO:Declaring metric variables
2023-05-20 15:28:33,401:INFO:Importing untrained model
2023-05-20 15:28:33,404:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 15:28:33,411:INFO:Starting cross validation
2023-05-20 15:28:33,413:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:33,510:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,512:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,521:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,540:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,551:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,564:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,575:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,585:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:33,606:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:28:34,219:INFO:Calculating mean and std
2023-05-20 15:28:34,224:INFO:Creating metrics dataframe
2023-05-20 15:28:34,280:INFO:Uploading results into container
2023-05-20 15:28:34,281:INFO:Uploading model into container now
2023-05-20 15:28:34,281:INFO:_master_model_container: 8
2023-05-20 15:28:34,281:INFO:_display_container: 2
2023-05-20 15:28:34,282:INFO:OrthogonalMatchingPursuit()
2023-05-20 15:28:34,282:INFO:create_model() successfully completed......................................
2023-05-20 15:28:34,341:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:34,341:INFO:Creating metrics dataframe
2023-05-20 15:28:34,350:INFO:Initializing Bayesian Ridge
2023-05-20 15:28:34,350:INFO:Total runtime is 0.24241162538528443 minutes
2023-05-20 15:28:34,353:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:34,354:INFO:Initializing create_model()
2023-05-20 15:28:34,354:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:34,354:INFO:Checking exceptions
2023-05-20 15:28:34,354:INFO:Importing libraries
2023-05-20 15:28:34,354:INFO:Copying training dataset
2023-05-20 15:28:34,362:INFO:Defining folds
2023-05-20 15:28:34,362:INFO:Declaring metric variables
2023-05-20 15:28:34,367:INFO:Importing untrained model
2023-05-20 15:28:34,370:INFO:Bayesian Ridge Imported successfully
2023-05-20 15:28:34,375:INFO:Starting cross validation
2023-05-20 15:28:34,377:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:35,229:INFO:Calculating mean and std
2023-05-20 15:28:35,230:INFO:Creating metrics dataframe
2023-05-20 15:28:35,289:INFO:Uploading results into container
2023-05-20 15:28:35,290:INFO:Uploading model into container now
2023-05-20 15:28:35,291:INFO:_master_model_container: 9
2023-05-20 15:28:35,291:INFO:_display_container: 2
2023-05-20 15:28:35,291:INFO:BayesianRidge()
2023-05-20 15:28:35,291:INFO:create_model() successfully completed......................................
2023-05-20 15:28:35,351:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:35,351:INFO:Creating metrics dataframe
2023-05-20 15:28:35,359:INFO:Initializing Passive Aggressive Regressor
2023-05-20 15:28:35,359:INFO:Total runtime is 0.25921918551127116 minutes
2023-05-20 15:28:35,362:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:35,362:INFO:Initializing create_model()
2023-05-20 15:28:35,363:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:35,363:INFO:Checking exceptions
2023-05-20 15:28:35,363:INFO:Importing libraries
2023-05-20 15:28:35,363:INFO:Copying training dataset
2023-05-20 15:28:35,372:INFO:Defining folds
2023-05-20 15:28:35,373:INFO:Declaring metric variables
2023-05-20 15:28:35,376:INFO:Importing untrained model
2023-05-20 15:28:35,379:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 15:28:35,384:INFO:Starting cross validation
2023-05-20 15:28:35,386:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:36,221:INFO:Calculating mean and std
2023-05-20 15:28:36,223:INFO:Creating metrics dataframe
2023-05-20 15:28:36,275:INFO:Uploading results into container
2023-05-20 15:28:36,275:INFO:Uploading model into container now
2023-05-20 15:28:36,275:INFO:_master_model_container: 10
2023-05-20 15:28:36,275:INFO:_display_container: 2
2023-05-20 15:28:36,275:INFO:PassiveAggressiveRegressor(random_state=6582)
2023-05-20 15:28:36,275:INFO:create_model() successfully completed......................................
2023-05-20 15:28:36,339:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:36,339:INFO:Creating metrics dataframe
2023-05-20 15:28:36,348:INFO:Initializing Huber Regressor
2023-05-20 15:28:36,348:INFO:Total runtime is 0.27571051120758056 minutes
2023-05-20 15:28:36,351:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:36,351:INFO:Initializing create_model()
2023-05-20 15:28:36,351:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:36,351:INFO:Checking exceptions
2023-05-20 15:28:36,351:INFO:Importing libraries
2023-05-20 15:28:36,351:INFO:Copying training dataset
2023-05-20 15:28:36,361:INFO:Defining folds
2023-05-20 15:28:36,361:INFO:Declaring metric variables
2023-05-20 15:28:36,365:INFO:Importing untrained model
2023-05-20 15:28:36,368:INFO:Huber Regressor Imported successfully
2023-05-20 15:28:36,373:INFO:Starting cross validation
2023-05-20 15:28:36,375:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:38,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,140:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,166:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,171:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,192:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,206:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,232:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,232:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:28:39,931:INFO:Calculating mean and std
2023-05-20 15:28:39,932:INFO:Creating metrics dataframe
2023-05-20 15:28:39,990:INFO:Uploading results into container
2023-05-20 15:28:39,991:INFO:Uploading model into container now
2023-05-20 15:28:39,991:INFO:_master_model_container: 11
2023-05-20 15:28:39,991:INFO:_display_container: 2
2023-05-20 15:28:39,992:INFO:HuberRegressor()
2023-05-20 15:28:39,992:INFO:create_model() successfully completed......................................
2023-05-20 15:28:40,051:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:40,051:INFO:Creating metrics dataframe
2023-05-20 15:28:40,060:INFO:Initializing K Neighbors Regressor
2023-05-20 15:28:40,060:INFO:Total runtime is 0.3375725507736206 minutes
2023-05-20 15:28:40,063:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:40,063:INFO:Initializing create_model()
2023-05-20 15:28:40,063:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:40,063:INFO:Checking exceptions
2023-05-20 15:28:40,064:INFO:Importing libraries
2023-05-20 15:28:40,064:INFO:Copying training dataset
2023-05-20 15:28:40,074:INFO:Defining folds
2023-05-20 15:28:40,074:INFO:Declaring metric variables
2023-05-20 15:28:40,078:INFO:Importing untrained model
2023-05-20 15:28:40,083:INFO:K Neighbors Regressor Imported successfully
2023-05-20 15:28:40,089:INFO:Starting cross validation
2023-05-20 15:28:40,090:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:40,995:INFO:Calculating mean and std
2023-05-20 15:28:40,996:INFO:Creating metrics dataframe
2023-05-20 15:28:41,054:INFO:Uploading results into container
2023-05-20 15:28:41,055:INFO:Uploading model into container now
2023-05-20 15:28:41,055:INFO:_master_model_container: 12
2023-05-20 15:28:41,055:INFO:_display_container: 2
2023-05-20 15:28:41,056:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 15:28:41,056:INFO:create_model() successfully completed......................................
2023-05-20 15:28:41,115:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:41,115:INFO:Creating metrics dataframe
2023-05-20 15:28:41,125:INFO:Initializing Decision Tree Regressor
2023-05-20 15:28:41,125:INFO:Total runtime is 0.3553242246309916 minutes
2023-05-20 15:28:41,128:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:41,128:INFO:Initializing create_model()
2023-05-20 15:28:41,128:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:41,128:INFO:Checking exceptions
2023-05-20 15:28:41,128:INFO:Importing libraries
2023-05-20 15:28:41,128:INFO:Copying training dataset
2023-05-20 15:28:41,137:INFO:Defining folds
2023-05-20 15:28:41,138:INFO:Declaring metric variables
2023-05-20 15:28:41,142:INFO:Importing untrained model
2023-05-20 15:28:41,145:INFO:Decision Tree Regressor Imported successfully
2023-05-20 15:28:41,151:INFO:Starting cross validation
2023-05-20 15:28:41,153:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:42,023:INFO:Calculating mean and std
2023-05-20 15:28:42,024:INFO:Creating metrics dataframe
2023-05-20 15:28:42,085:INFO:Uploading results into container
2023-05-20 15:28:42,085:INFO:Uploading model into container now
2023-05-20 15:28:42,086:INFO:_master_model_container: 13
2023-05-20 15:28:42,086:INFO:_display_container: 2
2023-05-20 15:28:42,086:INFO:DecisionTreeRegressor(random_state=6582)
2023-05-20 15:28:42,086:INFO:create_model() successfully completed......................................
2023-05-20 15:28:42,147:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:42,147:INFO:Creating metrics dataframe
2023-05-20 15:28:42,156:INFO:Initializing Random Forest Regressor
2023-05-20 15:28:42,156:INFO:Total runtime is 0.37251183986663816 minutes
2023-05-20 15:28:42,159:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:42,159:INFO:Initializing create_model()
2023-05-20 15:28:42,159:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:42,159:INFO:Checking exceptions
2023-05-20 15:28:42,160:INFO:Importing libraries
2023-05-20 15:28:42,160:INFO:Copying training dataset
2023-05-20 15:28:42,170:INFO:Defining folds
2023-05-20 15:28:42,170:INFO:Declaring metric variables
2023-05-20 15:28:42,175:INFO:Importing untrained model
2023-05-20 15:28:42,178:INFO:Random Forest Regressor Imported successfully
2023-05-20 15:28:42,184:INFO:Starting cross validation
2023-05-20 15:28:42,186:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:43,817:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:44,013:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:44,247:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.00s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:44,468:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:45,435:INFO:Calculating mean and std
2023-05-20 15:28:45,441:INFO:Creating metrics dataframe
2023-05-20 15:28:45,507:INFO:Uploading results into container
2023-05-20 15:28:45,508:INFO:Uploading model into container now
2023-05-20 15:28:45,508:INFO:_master_model_container: 14
2023-05-20 15:28:45,508:INFO:_display_container: 2
2023-05-20 15:28:45,509:INFO:RandomForestRegressor(n_jobs=-1, random_state=6582)
2023-05-20 15:28:45,509:INFO:create_model() successfully completed......................................
2023-05-20 15:28:45,569:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:45,569:INFO:Creating metrics dataframe
2023-05-20 15:28:45,579:INFO:Initializing Extra Trees Regressor
2023-05-20 15:28:45,579:INFO:Total runtime is 0.42955882946650187 minutes
2023-05-20 15:28:45,582:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:45,582:INFO:Initializing create_model()
2023-05-20 15:28:45,582:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:45,582:INFO:Checking exceptions
2023-05-20 15:28:45,582:INFO:Importing libraries
2023-05-20 15:28:45,583:INFO:Copying training dataset
2023-05-20 15:28:45,593:INFO:Defining folds
2023-05-20 15:28:45,593:INFO:Declaring metric variables
2023-05-20 15:28:45,597:INFO:Importing untrained model
2023-05-20 15:28:45,600:INFO:Extra Trees Regressor Imported successfully
2023-05-20 15:28:45,606:INFO:Starting cross validation
2023-05-20 15:28:45,608:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:47,011:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:47,344:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:47,396:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:48,340:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:108: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, y = pipeline._memory_transform(transformer, X, y)

2023-05-20 15:28:48,341:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.22s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:48,343:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:28:49,301:INFO:Calculating mean and std
2023-05-20 15:28:49,303:INFO:Creating metrics dataframe
2023-05-20 15:28:49,374:INFO:Uploading results into container
2023-05-20 15:28:49,375:INFO:Uploading model into container now
2023-05-20 15:28:49,375:INFO:_master_model_container: 15
2023-05-20 15:28:49,375:INFO:_display_container: 2
2023-05-20 15:28:49,375:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6582)
2023-05-20 15:28:49,376:INFO:create_model() successfully completed......................................
2023-05-20 15:28:49,436:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:49,436:INFO:Creating metrics dataframe
2023-05-20 15:28:49,447:INFO:Initializing AdaBoost Regressor
2023-05-20 15:28:49,447:INFO:Total runtime is 0.4940170129140218 minutes
2023-05-20 15:28:49,450:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:49,450:INFO:Initializing create_model()
2023-05-20 15:28:49,450:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:49,450:INFO:Checking exceptions
2023-05-20 15:28:49,450:INFO:Importing libraries
2023-05-20 15:28:49,450:INFO:Copying training dataset
2023-05-20 15:28:49,459:INFO:Defining folds
2023-05-20 15:28:49,459:INFO:Declaring metric variables
2023-05-20 15:28:49,463:INFO:Importing untrained model
2023-05-20 15:28:49,466:INFO:AdaBoost Regressor Imported successfully
2023-05-20 15:28:49,471:INFO:Starting cross validation
2023-05-20 15:28:49,474:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:51,434:INFO:Calculating mean and std
2023-05-20 15:28:51,435:INFO:Creating metrics dataframe
2023-05-20 15:28:51,521:INFO:Uploading results into container
2023-05-20 15:28:51,522:INFO:Uploading model into container now
2023-05-20 15:28:51,522:INFO:_master_model_container: 16
2023-05-20 15:28:51,522:INFO:_display_container: 2
2023-05-20 15:28:51,522:INFO:AdaBoostRegressor(random_state=6582)
2023-05-20 15:28:51,523:INFO:create_model() successfully completed......................................
2023-05-20 15:28:51,582:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:51,582:INFO:Creating metrics dataframe
2023-05-20 15:28:51,593:INFO:Initializing Gradient Boosting Regressor
2023-05-20 15:28:51,593:INFO:Total runtime is 0.5297809799512228 minutes
2023-05-20 15:28:51,596:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:51,596:INFO:Initializing create_model()
2023-05-20 15:28:51,596:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:51,596:INFO:Checking exceptions
2023-05-20 15:28:51,596:INFO:Importing libraries
2023-05-20 15:28:51,596:INFO:Copying training dataset
2023-05-20 15:28:51,605:INFO:Defining folds
2023-05-20 15:28:51,605:INFO:Declaring metric variables
2023-05-20 15:28:51,608:INFO:Importing untrained model
2023-05-20 15:28:51,612:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 15:28:51,618:INFO:Starting cross validation
2023-05-20 15:28:51,619:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:53,691:INFO:Calculating mean and std
2023-05-20 15:28:53,693:INFO:Creating metrics dataframe
2023-05-20 15:28:53,772:INFO:Uploading results into container
2023-05-20 15:28:53,772:INFO:Uploading model into container now
2023-05-20 15:28:53,772:INFO:_master_model_container: 17
2023-05-20 15:28:53,772:INFO:_display_container: 2
2023-05-20 15:28:53,773:INFO:GradientBoostingRegressor(random_state=6582)
2023-05-20 15:28:53,773:INFO:create_model() successfully completed......................................
2023-05-20 15:28:53,832:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:53,832:INFO:Creating metrics dataframe
2023-05-20 15:28:53,842:INFO:Initializing Extreme Gradient Boosting
2023-05-20 15:28:53,843:INFO:Total runtime is 0.5672962109247843 minutes
2023-05-20 15:28:53,847:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:53,847:INFO:Initializing create_model()
2023-05-20 15:28:53,847:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:53,847:INFO:Checking exceptions
2023-05-20 15:28:53,848:INFO:Importing libraries
2023-05-20 15:28:53,848:INFO:Copying training dataset
2023-05-20 15:28:53,856:INFO:Defining folds
2023-05-20 15:28:53,856:INFO:Declaring metric variables
2023-05-20 15:28:53,860:INFO:Importing untrained model
2023-05-20 15:28:53,869:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 15:28:53,876:INFO:Starting cross validation
2023-05-20 15:28:53,878:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:56,755:INFO:Calculating mean and std
2023-05-20 15:28:56,756:INFO:Creating metrics dataframe
2023-05-20 15:28:56,845:INFO:Uploading results into container
2023-05-20 15:28:56,846:INFO:Uploading model into container now
2023-05-20 15:28:56,846:INFO:_master_model_container: 18
2023-05-20 15:28:56,846:INFO:_display_container: 2
2023-05-20 15:28:56,847:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6582, ...)
2023-05-20 15:28:56,847:INFO:create_model() successfully completed......................................
2023-05-20 15:28:56,904:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:56,904:INFO:Creating metrics dataframe
2023-05-20 15:28:56,915:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 15:28:56,915:INFO:Total runtime is 0.6184917489687601 minutes
2023-05-20 15:28:56,918:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:56,918:INFO:Initializing create_model()
2023-05-20 15:28:56,918:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:56,918:INFO:Checking exceptions
2023-05-20 15:28:56,918:INFO:Importing libraries
2023-05-20 15:28:56,918:INFO:Copying training dataset
2023-05-20 15:28:56,931:INFO:Defining folds
2023-05-20 15:28:56,931:INFO:Declaring metric variables
2023-05-20 15:28:56,934:INFO:Importing untrained model
2023-05-20 15:28:56,938:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 15:28:56,943:INFO:Starting cross validation
2023-05-20 15:28:56,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:28:58,647:INFO:Calculating mean and std
2023-05-20 15:28:58,648:INFO:Creating metrics dataframe
2023-05-20 15:28:58,736:INFO:Uploading results into container
2023-05-20 15:28:58,737:INFO:Uploading model into container now
2023-05-20 15:28:58,737:INFO:_master_model_container: 19
2023-05-20 15:28:58,737:INFO:_display_container: 2
2023-05-20 15:28:58,737:INFO:LGBMRegressor(random_state=6582)
2023-05-20 15:28:58,737:INFO:create_model() successfully completed......................................
2023-05-20 15:28:58,797:INFO:SubProcess create_model() end ==================================
2023-05-20 15:28:58,797:INFO:Creating metrics dataframe
2023-05-20 15:28:58,808:INFO:Initializing CatBoost Regressor
2023-05-20 15:28:58,808:INFO:Total runtime is 0.6500346819559732 minutes
2023-05-20 15:28:58,811:INFO:SubProcess create_model() called ==================================
2023-05-20 15:28:58,811:INFO:Initializing create_model()
2023-05-20 15:28:58,811:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:28:58,811:INFO:Checking exceptions
2023-05-20 15:28:58,811:INFO:Importing libraries
2023-05-20 15:28:58,811:INFO:Copying training dataset
2023-05-20 15:28:58,821:INFO:Defining folds
2023-05-20 15:28:58,821:INFO:Declaring metric variables
2023-05-20 15:28:58,825:INFO:Importing untrained model
2023-05-20 15:28:58,837:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:28:58,842:INFO:Starting cross validation
2023-05-20 15:28:58,845:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:29:27,773:INFO:Calculating mean and std
2023-05-20 15:29:27,775:INFO:Creating metrics dataframe
2023-05-20 15:29:27,881:INFO:Uploading results into container
2023-05-20 15:29:27,883:INFO:Uploading model into container now
2023-05-20 15:29:27,883:INFO:_master_model_container: 20
2023-05-20 15:29:27,883:INFO:_display_container: 2
2023-05-20 15:29:27,883:INFO:<catboost.core.CatBoostRegressor object at 0x000002B03A4B2DD0>
2023-05-20 15:29:27,884:INFO:create_model() successfully completed......................................
2023-05-20 15:29:27,953:INFO:SubProcess create_model() end ==================================
2023-05-20 15:29:27,953:INFO:Creating metrics dataframe
2023-05-20 15:29:27,965:INFO:Initializing Dummy Regressor
2023-05-20 15:29:27,965:INFO:Total runtime is 1.1359808921813963 minutes
2023-05-20 15:29:27,968:INFO:SubProcess create_model() called ==================================
2023-05-20 15:29:27,969:INFO:Initializing create_model()
2023-05-20 15:29:27,969:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B01CBF9FC0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:29:27,969:INFO:Checking exceptions
2023-05-20 15:29:27,969:INFO:Importing libraries
2023-05-20 15:29:27,969:INFO:Copying training dataset
2023-05-20 15:29:27,978:INFO:Defining folds
2023-05-20 15:29:27,978:INFO:Declaring metric variables
2023-05-20 15:29:27,981:INFO:Importing untrained model
2023-05-20 15:29:27,984:INFO:Dummy Regressor Imported successfully
2023-05-20 15:29:27,993:INFO:Starting cross validation
2023-05-20 15:29:27,994:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:29:29,245:INFO:Calculating mean and std
2023-05-20 15:29:29,249:INFO:Creating metrics dataframe
2023-05-20 15:29:29,348:INFO:Uploading results into container
2023-05-20 15:29:29,348:INFO:Uploading model into container now
2023-05-20 15:29:29,349:INFO:_master_model_container: 21
2023-05-20 15:29:29,349:INFO:_display_container: 2
2023-05-20 15:29:29,349:INFO:DummyRegressor()
2023-05-20 15:29:29,349:INFO:create_model() successfully completed......................................
2023-05-20 15:29:29,412:INFO:SubProcess create_model() end ==================================
2023-05-20 15:29:29,412:INFO:Creating metrics dataframe
2023-05-20 15:29:29,431:INFO:Initializing create_model()
2023-05-20 15:29:29,431:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002B01E22F3A0>, estimator=<catboost.core.CatBoostRegressor object at 0x000002B03A4B2DD0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:29:29,431:INFO:Checking exceptions
2023-05-20 15:29:29,434:INFO:Importing libraries
2023-05-20 15:29:29,434:INFO:Copying training dataset
2023-05-20 15:29:29,442:INFO:Defining folds
2023-05-20 15:29:29,442:INFO:Declaring metric variables
2023-05-20 15:29:29,443:INFO:Importing untrained model
2023-05-20 15:29:29,443:INFO:Declaring custom model
2023-05-20 15:29:29,443:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:29:29,444:INFO:Cross validation set to False
2023-05-20 15:29:29,444:INFO:Fitting Model
2023-05-20 15:29:33,291:INFO:<catboost.core.CatBoostRegressor object at 0x000002B03ABB89A0>
2023-05-20 15:29:33,291:INFO:create_model() successfully completed......................................
2023-05-20 15:29:33,381:INFO:_master_model_container: 21
2023-05-20 15:29:33,381:INFO:_display_container: 2
2023-05-20 15:29:33,381:INFO:<catboost.core.CatBoostRegressor object at 0x000002B03ABB89A0>
2023-05-20 15:29:33,382:INFO:compare_models() successfully completed......................................
2023-05-20 15:30:37,271:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:30:37,272:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:30:37,272:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:30:37,272:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 15:30:37,755:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-20 15:30:40,186:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 15:30:40,244:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 15:30:40,888:INFO:PyCaret RegressionExperiment
2023-05-20 15:30:40,888:INFO:Logging name: reg-default-name
2023-05-20 15:30:40,888:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:30:40,888:INFO:version 3.0.2
2023-05-20 15:30:40,888:INFO:Initializing setup()
2023-05-20 15:30:40,888:INFO:self.USI: 1ec6
2023-05-20 15:30:40,889:INFO:self._variable_keys: {'exp_id', 'logging_param', 'fold_groups_param', 'data', 'gpu_param', 'seed', 'memory', 'n_jobs_param', 'idx', 'X', 'html_param', 'transform_target_param', 'USI', '_available_plots', 'y_train', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', 'fold_shuffle_param', 'y', 'pipeline', 'X_train', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase', 'y_test'}
2023-05-20 15:30:40,889:INFO:Checking environment
2023-05-20 15:30:40,889:INFO:python_version: 3.10.3
2023-05-20 15:30:40,889:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:30:40,889:INFO:machine: AMD64
2023-05-20 15:30:40,889:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:30:40,889:INFO:Memory: svmem(total=17083187200, available=8247754752, percent=51.7, used=8835432448, free=8247754752)
2023-05-20 15:30:40,889:INFO:Physical Core: 6
2023-05-20 15:30:40,889:INFO:Logical Core: 12
2023-05-20 15:30:40,889:INFO:Checking libraries
2023-05-20 15:30:40,889:INFO:System:
2023-05-20 15:30:40,889:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:30:40,889:INFO:executable: c:\Python310\python.exe
2023-05-20 15:30:40,889:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:30:40,889:INFO:PyCaret required dependencies:
2023-05-20 15:30:40,889:INFO:                 pip: 23.1.2
2023-05-20 15:30:40,889:INFO:          setuptools: 58.1.0
2023-05-20 15:30:40,889:INFO:             pycaret: 3.0.2
2023-05-20 15:30:40,889:INFO:             IPython: 8.5.0
2023-05-20 15:30:40,889:INFO:          ipywidgets: 8.0.6
2023-05-20 15:30:40,889:INFO:                tqdm: 4.65.0
2023-05-20 15:30:40,890:INFO:               numpy: 1.23.2
2023-05-20 15:30:40,890:INFO:              pandas: 1.5.2
2023-05-20 15:30:40,890:INFO:              jinja2: 3.1.2
2023-05-20 15:30:40,890:INFO:               scipy: 1.9.3
2023-05-20 15:30:40,890:INFO:              joblib: 1.2.0
2023-05-20 15:30:40,890:INFO:             sklearn: 1.1.3
2023-05-20 15:30:40,890:INFO:                pyod: 1.0.9
2023-05-20 15:30:40,890:INFO:            imblearn: 0.10.1
2023-05-20 15:30:40,890:INFO:   category_encoders: 2.6.1
2023-05-20 15:30:40,890:INFO:            lightgbm: 3.3.5
2023-05-20 15:30:40,890:INFO:               numba: 0.57.0
2023-05-20 15:30:40,890:INFO:            requests: 2.28.2
2023-05-20 15:30:40,890:INFO:          matplotlib: 3.5.3
2023-05-20 15:30:40,890:INFO:          scikitplot: 0.3.7
2023-05-20 15:30:40,891:INFO:         yellowbrick: 1.5
2023-05-20 15:30:40,891:INFO:              plotly: 5.13.1
2023-05-20 15:30:40,891:INFO:             kaleido: 0.2.1
2023-05-20 15:30:40,891:INFO:         statsmodels: 0.13.5
2023-05-20 15:30:40,891:INFO:              sktime: 0.17.0
2023-05-20 15:30:40,891:INFO:               tbats: 1.1.3
2023-05-20 15:30:40,891:INFO:            pmdarima: 2.0.3
2023-05-20 15:30:40,891:INFO:              psutil: 5.9.2
2023-05-20 15:30:40,891:INFO:PyCaret optional dependencies:
2023-05-20 15:30:40,904:INFO:                shap: Not installed
2023-05-20 15:30:40,904:INFO:           interpret: Not installed
2023-05-20 15:30:40,904:INFO:                umap: Not installed
2023-05-20 15:30:40,904:INFO:    pandas_profiling: Not installed
2023-05-20 15:30:40,904:INFO:  explainerdashboard: Not installed
2023-05-20 15:30:40,904:INFO:             autoviz: Not installed
2023-05-20 15:30:40,904:INFO:           fairlearn: Not installed
2023-05-20 15:30:40,904:INFO:             xgboost: 1.7.4
2023-05-20 15:30:40,904:INFO:            catboost: 1.1.1
2023-05-20 15:30:40,904:INFO:              kmodes: Not installed
2023-05-20 15:30:40,904:INFO:             mlxtend: Not installed
2023-05-20 15:30:40,904:INFO:       statsforecast: Not installed
2023-05-20 15:30:40,905:INFO:        tune_sklearn: Not installed
2023-05-20 15:30:40,905:INFO:                 ray: Not installed
2023-05-20 15:30:40,905:INFO:            hyperopt: Not installed
2023-05-20 15:30:40,905:INFO:              optuna: Not installed
2023-05-20 15:30:40,905:INFO:               skopt: Not installed
2023-05-20 15:30:40,905:INFO:              mlflow: Not installed
2023-05-20 15:30:40,905:INFO:              gradio: Not installed
2023-05-20 15:30:40,905:INFO:             fastapi: Not installed
2023-05-20 15:30:40,905:INFO:             uvicorn: Not installed
2023-05-20 15:30:40,905:INFO:              m2cgen: Not installed
2023-05-20 15:30:40,905:INFO:           evidently: Not installed
2023-05-20 15:30:40,905:INFO:               fugue: Not installed
2023-05-20 15:30:40,905:INFO:           streamlit: Not installed
2023-05-20 15:30:40,905:INFO:             prophet: Not installed
2023-05-20 15:30:40,905:INFO:None
2023-05-20 15:30:40,905:INFO:Set up data.
2023-05-20 15:30:40,979:INFO:Set up train/test split.
2023-05-20 15:30:40,988:INFO:Set up index.
2023-05-20 15:30:40,989:INFO:Set up folding strategy.
2023-05-20 15:30:40,989:INFO:Assigning column types.
2023-05-20 15:30:40,995:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:30:40,996:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,000:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,009:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,066:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,107:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,107:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,166:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,185:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,189:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,193:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,251:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,298:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,298:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,301:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,303:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:30:41,308:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,312:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,377:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,429:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,430:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,432:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,437:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,442:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,513:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,567:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,569:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,570:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:30:41,581:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,663:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,720:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,721:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,726:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,748:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,816:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,866:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:41,866:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:41,869:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:41,869:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:30:41,955:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,003:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,003:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,006:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,079:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,122:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,122:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,125:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,125:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:30:42,193:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,242:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,245:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,313:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:30:42,356:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,358:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,359:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:30:42,473:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,476:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,606:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,608:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:42,609:INFO:Preparing preprocessing pipeline...
2023-05-20 15:30:42,609:INFO:Set up simple imputation.
2023-05-20 15:30:42,611:INFO:Set up column name cleaning.
2023-05-20 15:30:42,659:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:30:42,667:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:30:42,667:INFO:Creating final display dataframe.
2023-05-20 15:30:42,872:INFO:Setup _display_container:                     Description             Value
0                    Session id              5829
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              1ec6
2023-05-20 15:30:42,985:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:42,988:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:43,093:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:30:43,095:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:30:43,096:INFO:setup() successfully completed in 2.31s...............
2023-05-20 15:30:43,129:INFO:Initializing compare_models()
2023-05-20 15:30:43,129:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 15:30:43,129:INFO:Checking exceptions
2023-05-20 15:30:43,139:INFO:Preparing display monitor
2023-05-20 15:30:43,183:INFO:Initializing Linear Regression
2023-05-20 15:30:43,183:INFO:Total runtime is 0.0 minutes
2023-05-20 15:30:43,187:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:43,188:INFO:Initializing create_model()
2023-05-20 15:30:43,188:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:43,188:INFO:Checking exceptions
2023-05-20 15:30:43,188:INFO:Importing libraries
2023-05-20 15:30:43,188:INFO:Copying training dataset
2023-05-20 15:30:43,202:INFO:Defining folds
2023-05-20 15:30:43,202:INFO:Declaring metric variables
2023-05-20 15:30:43,206:INFO:Importing untrained model
2023-05-20 15:30:43,209:INFO:Linear Regression Imported successfully
2023-05-20 15:30:43,217:INFO:Starting cross validation
2023-05-20 15:30:43,225:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:48,585:INFO:Calculating mean and std
2023-05-20 15:30:48,589:INFO:Creating metrics dataframe
2023-05-20 15:30:48,684:INFO:Uploading results into container
2023-05-20 15:30:48,685:INFO:Uploading model into container now
2023-05-20 15:30:48,685:INFO:_master_model_container: 1
2023-05-20 15:30:48,685:INFO:_display_container: 2
2023-05-20 15:30:48,685:INFO:LinearRegression(n_jobs=-1)
2023-05-20 15:30:48,685:INFO:create_model() successfully completed......................................
2023-05-20 15:30:48,751:INFO:SubProcess create_model() end ==================================
2023-05-20 15:30:48,751:INFO:Creating metrics dataframe
2023-05-20 15:30:48,758:INFO:Initializing Lasso Regression
2023-05-20 15:30:48,758:INFO:Total runtime is 0.09291157325108847 minutes
2023-05-20 15:30:48,761:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:48,761:INFO:Initializing create_model()
2023-05-20 15:30:48,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:48,763:INFO:Checking exceptions
2023-05-20 15:30:48,763:INFO:Importing libraries
2023-05-20 15:30:48,763:INFO:Copying training dataset
2023-05-20 15:30:48,771:INFO:Defining folds
2023-05-20 15:30:48,771:INFO:Declaring metric variables
2023-05-20 15:30:48,775:INFO:Importing untrained model
2023-05-20 15:30:48,778:INFO:Lasso Regression Imported successfully
2023-05-20 15:30:48,783:INFO:Starting cross validation
2023-05-20 15:30:48,785:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:51,017:INFO:Calculating mean and std
2023-05-20 15:30:51,019:INFO:Creating metrics dataframe
2023-05-20 15:30:51,111:INFO:Uploading results into container
2023-05-20 15:30:51,113:INFO:Uploading model into container now
2023-05-20 15:30:51,113:INFO:_master_model_container: 2
2023-05-20 15:30:51,113:INFO:_display_container: 2
2023-05-20 15:30:51,113:INFO:Lasso(random_state=5829)
2023-05-20 15:30:51,113:INFO:create_model() successfully completed......................................
2023-05-20 15:30:51,173:INFO:SubProcess create_model() end ==================================
2023-05-20 15:30:51,173:INFO:Creating metrics dataframe
2023-05-20 15:30:51,181:INFO:Initializing Ridge Regression
2023-05-20 15:30:51,181:INFO:Total runtime is 0.1332858363787333 minutes
2023-05-20 15:30:51,184:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:51,184:INFO:Initializing create_model()
2023-05-20 15:30:51,184:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:51,184:INFO:Checking exceptions
2023-05-20 15:30:51,184:INFO:Importing libraries
2023-05-20 15:30:51,184:INFO:Copying training dataset
2023-05-20 15:30:51,195:INFO:Defining folds
2023-05-20 15:30:51,195:INFO:Declaring metric variables
2023-05-20 15:30:51,197:INFO:Importing untrained model
2023-05-20 15:30:51,201:INFO:Ridge Regression Imported successfully
2023-05-20 15:30:51,207:INFO:Starting cross validation
2023-05-20 15:30:51,209:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:52,409:INFO:Calculating mean and std
2023-05-20 15:30:52,411:INFO:Creating metrics dataframe
2023-05-20 15:30:52,505:INFO:Uploading results into container
2023-05-20 15:30:52,505:INFO:Uploading model into container now
2023-05-20 15:30:52,505:INFO:_master_model_container: 3
2023-05-20 15:30:52,505:INFO:_display_container: 2
2023-05-20 15:30:52,506:INFO:Ridge(random_state=5829)
2023-05-20 15:30:52,506:INFO:create_model() successfully completed......................................
2023-05-20 15:30:52,565:INFO:SubProcess create_model() end ==================================
2023-05-20 15:30:52,565:INFO:Creating metrics dataframe
2023-05-20 15:30:52,573:INFO:Initializing Elastic Net
2023-05-20 15:30:52,573:INFO:Total runtime is 0.1564934492111206 minutes
2023-05-20 15:30:52,575:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:52,576:INFO:Initializing create_model()
2023-05-20 15:30:52,576:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:52,576:INFO:Checking exceptions
2023-05-20 15:30:52,576:INFO:Importing libraries
2023-05-20 15:30:52,576:INFO:Copying training dataset
2023-05-20 15:30:52,585:INFO:Defining folds
2023-05-20 15:30:52,586:INFO:Declaring metric variables
2023-05-20 15:30:52,589:INFO:Importing untrained model
2023-05-20 15:30:52,592:INFO:Elastic Net Imported successfully
2023-05-20 15:30:52,598:INFO:Starting cross validation
2023-05-20 15:30:52,599:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:53,805:INFO:Calculating mean and std
2023-05-20 15:30:53,808:INFO:Creating metrics dataframe
2023-05-20 15:30:53,900:INFO:Uploading results into container
2023-05-20 15:30:53,901:INFO:Uploading model into container now
2023-05-20 15:30:53,901:INFO:_master_model_container: 4
2023-05-20 15:30:53,901:INFO:_display_container: 2
2023-05-20 15:30:53,901:INFO:ElasticNet(random_state=5829)
2023-05-20 15:30:53,901:INFO:create_model() successfully completed......................................
2023-05-20 15:30:53,964:INFO:SubProcess create_model() end ==================================
2023-05-20 15:30:53,964:INFO:Creating metrics dataframe
2023-05-20 15:30:53,973:INFO:Initializing Least Angle Regression
2023-05-20 15:30:53,973:INFO:Total runtime is 0.17982354561487834 minutes
2023-05-20 15:30:53,976:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:53,976:INFO:Initializing create_model()
2023-05-20 15:30:53,976:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:53,976:INFO:Checking exceptions
2023-05-20 15:30:53,976:INFO:Importing libraries
2023-05-20 15:30:53,976:INFO:Copying training dataset
2023-05-20 15:30:53,985:INFO:Defining folds
2023-05-20 15:30:53,986:INFO:Declaring metric variables
2023-05-20 15:30:53,989:INFO:Importing untrained model
2023-05-20 15:30:53,992:INFO:Least Angle Regression Imported successfully
2023-05-20 15:30:53,998:INFO:Starting cross validation
2023-05-20 15:30:54,000:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:54,112:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,119:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,124:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,131:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.271e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,138:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,147:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.799e-04, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,149:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.719e-04, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,150:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.610e-04, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,154:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,155:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.174e-04, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,156:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.776e-04, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,157:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,157:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=8.689e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,158:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.525e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,159:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.964e-04, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,163:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.566e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,165:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=1.686e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,167:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=4.124e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,167:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.198e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,167:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.659e-04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,174:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=3.035e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,179:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.924e-04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,180:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,180:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,181:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 120 iterations, i.e. alpha=3.039e-04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,182:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.751e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,183:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=2.986e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,183:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=2.519e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,189:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.457e-04, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,191:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.460e-04, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,195:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:54,195:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.223e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,197:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.980e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,198:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.377e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,200:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=1.319e-04, with an active set of 123 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,200:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=4.992e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,202:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.223e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,203:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=3.521e-04, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,205:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=1.151e-04, with an active set of 137 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,206:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=1.145e-04, with an active set of 137 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.103e-04, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:30:54,210:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.129e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,211:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 222 iterations, i.e. alpha=1.335e+00, with an active set of 169 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,213:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.734e-04, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,214:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.984e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,215:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.927e-04, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,215:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.585e-04, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.251e-04, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,220:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=1.821e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,220:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=1.281e-04, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,221:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.082e-04, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,222:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 192 iterations, i.e. alpha=1.102e-04, with an active set of 169 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,225:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=9.788e-05, with an active set of 120 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,231:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=3.933e-04, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,249:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:30:54,270:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:30:54,281:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 262 iterations, i.e. alpha=7.694e-04, with an active set of 208 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:30:54,346:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:30:54,346:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:30:54,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:30:54,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:30:54,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:30:54,538:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:30:54,541:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:30:54,544:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,544:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,546:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,570:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,571:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,571:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,579:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,579:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,580:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,583:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,583:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,584:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,585:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,585:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,590:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,590:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:30:54,591:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,591:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:30:54,591:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,616:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,617:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,617:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:54,629:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,629:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:54,630:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:55,619:INFO:Calculating mean and std
2023-05-20 15:30:55,619:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:30:55,619:INFO:Creating metrics dataframe
2023-05-20 15:30:55,716:INFO:Uploading results into container
2023-05-20 15:30:55,717:INFO:Uploading model into container now
2023-05-20 15:30:55,717:INFO:_master_model_container: 5
2023-05-20 15:30:55,717:INFO:_display_container: 2
2023-05-20 15:30:55,717:INFO:Lars(random_state=5829)
2023-05-20 15:30:55,718:INFO:create_model() successfully completed......................................
2023-05-20 15:30:55,776:WARNING:create_model() for Lars(random_state=5829) raised an exception or returned all 0.0, trying without fit_kwargs:
2023-05-20 15:30:55,777:WARNING:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

2023-05-20 15:30:55,777:INFO:Initializing create_model()
2023-05-20 15:30:55,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:55,777:INFO:Checking exceptions
2023-05-20 15:30:55,777:INFO:Importing libraries
2023-05-20 15:30:55,777:INFO:Copying training dataset
2023-05-20 15:30:55,786:INFO:Defining folds
2023-05-20 15:30:55,786:INFO:Declaring metric variables
2023-05-20 15:30:55,788:INFO:Importing untrained model
2023-05-20 15:30:55,792:INFO:Least Angle Regression Imported successfully
2023-05-20 15:30:55,798:INFO:Starting cross validation
2023-05-20 15:30:55,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:56,016:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,017:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,017:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,018:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:30:56,019:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:30:56,030:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,031:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,031:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,038:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,039:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,039:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,044:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,045:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:30:56,045:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,045:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-20 15:30:56,046:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,056:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,057:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,057:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,069:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,069:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,070:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,076:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,076:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,077:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:56,090:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,090:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:30:56,091:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:30:57,060:INFO:Calculating mean and std
2023-05-20 15:30:57,060:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:30:57,062:INFO:Creating metrics dataframe
2023-05-20 15:30:57,158:INFO:Uploading results into container
2023-05-20 15:30:57,159:INFO:Uploading model into container now
2023-05-20 15:30:57,159:INFO:_master_model_container: 6
2023-05-20 15:30:57,159:INFO:_display_container: 2
2023-05-20 15:30:57,160:INFO:Lars(random_state=5829)
2023-05-20 15:30:57,160:INFO:create_model() successfully completed......................................
2023-05-20 15:30:57,217:ERROR:create_model() for Lars(random_state=5829) raised an exception or returned all 0.0:
2023-05-20 15:30:57,217:ERROR:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 810, in compare_models
    assert (
AssertionError

2023-05-20 15:30:57,217:INFO:Initializing Lasso Least Angle Regression
2023-05-20 15:30:57,217:INFO:Total runtime is 0.2338932752609253 minutes
2023-05-20 15:30:57,219:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:57,219:INFO:Initializing create_model()
2023-05-20 15:30:57,219:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:57,219:INFO:Checking exceptions
2023-05-20 15:30:57,219:INFO:Importing libraries
2023-05-20 15:30:57,219:INFO:Copying training dataset
2023-05-20 15:30:57,229:INFO:Defining folds
2023-05-20 15:30:57,230:INFO:Declaring metric variables
2023-05-20 15:30:57,233:INFO:Importing untrained model
2023-05-20 15:30:57,236:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 15:30:57,241:INFO:Starting cross validation
2023-05-20 15:30:57,243:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:57,345:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,352:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,366:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,381:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,384:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,396:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,402:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,410:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,419:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:57,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:30:58,452:INFO:Calculating mean and std
2023-05-20 15:30:58,453:INFO:Creating metrics dataframe
2023-05-20 15:30:58,550:INFO:Uploading results into container
2023-05-20 15:30:58,550:INFO:Uploading model into container now
2023-05-20 15:30:58,551:INFO:_master_model_container: 7
2023-05-20 15:30:58,551:INFO:_display_container: 2
2023-05-20 15:30:58,551:INFO:LassoLars(random_state=5829)
2023-05-20 15:30:58,551:INFO:create_model() successfully completed......................................
2023-05-20 15:30:58,606:INFO:SubProcess create_model() end ==================================
2023-05-20 15:30:58,606:INFO:Creating metrics dataframe
2023-05-20 15:30:58,614:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 15:30:58,614:INFO:Total runtime is 0.25718023379643756 minutes
2023-05-20 15:30:58,617:INFO:SubProcess create_model() called ==================================
2023-05-20 15:30:58,617:INFO:Initializing create_model()
2023-05-20 15:30:58,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:30:58,617:INFO:Checking exceptions
2023-05-20 15:30:58,618:INFO:Importing libraries
2023-05-20 15:30:58,618:INFO:Copying training dataset
2023-05-20 15:30:58,628:INFO:Defining folds
2023-05-20 15:30:58,628:INFO:Declaring metric variables
2023-05-20 15:30:58,631:INFO:Importing untrained model
2023-05-20 15:30:58,635:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 15:30:58,640:INFO:Starting cross validation
2023-05-20 15:30:58,642:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:30:58,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,751:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,763:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,764:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,788:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,791:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,800:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,813:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:58,819:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:30:59,844:INFO:Calculating mean and std
2023-05-20 15:30:59,845:INFO:Creating metrics dataframe
2023-05-20 15:30:59,941:INFO:Uploading results into container
2023-05-20 15:30:59,942:INFO:Uploading model into container now
2023-05-20 15:30:59,942:INFO:_master_model_container: 8
2023-05-20 15:30:59,942:INFO:_display_container: 2
2023-05-20 15:30:59,942:INFO:OrthogonalMatchingPursuit()
2023-05-20 15:30:59,943:INFO:create_model() successfully completed......................................
2023-05-20 15:31:00,004:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:00,004:INFO:Creating metrics dataframe
2023-05-20 15:31:00,014:INFO:Initializing Bayesian Ridge
2023-05-20 15:31:00,014:INFO:Total runtime is 0.28050434986750283 minutes
2023-05-20 15:31:00,019:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:00,019:INFO:Initializing create_model()
2023-05-20 15:31:00,019:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:00,019:INFO:Checking exceptions
2023-05-20 15:31:00,019:INFO:Importing libraries
2023-05-20 15:31:00,019:INFO:Copying training dataset
2023-05-20 15:31:00,036:INFO:Defining folds
2023-05-20 15:31:00,036:INFO:Declaring metric variables
2023-05-20 15:31:00,041:INFO:Importing untrained model
2023-05-20 15:31:00,045:INFO:Bayesian Ridge Imported successfully
2023-05-20 15:31:00,050:INFO:Starting cross validation
2023-05-20 15:31:00,051:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:01,296:INFO:Calculating mean and std
2023-05-20 15:31:01,298:INFO:Creating metrics dataframe
2023-05-20 15:31:01,393:INFO:Uploading results into container
2023-05-20 15:31:01,394:INFO:Uploading model into container now
2023-05-20 15:31:01,394:INFO:_master_model_container: 9
2023-05-20 15:31:01,394:INFO:_display_container: 2
2023-05-20 15:31:01,394:INFO:BayesianRidge()
2023-05-20 15:31:01,394:INFO:create_model() successfully completed......................................
2023-05-20 15:31:01,451:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:01,451:INFO:Creating metrics dataframe
2023-05-20 15:31:01,459:INFO:Initializing Passive Aggressive Regressor
2023-05-20 15:31:01,459:INFO:Total runtime is 0.30459031263987224 minutes
2023-05-20 15:31:01,462:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:01,462:INFO:Initializing create_model()
2023-05-20 15:31:01,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:01,462:INFO:Checking exceptions
2023-05-20 15:31:01,462:INFO:Importing libraries
2023-05-20 15:31:01,462:INFO:Copying training dataset
2023-05-20 15:31:01,472:INFO:Defining folds
2023-05-20 15:31:01,472:INFO:Declaring metric variables
2023-05-20 15:31:01,475:INFO:Importing untrained model
2023-05-20 15:31:01,479:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 15:31:01,485:INFO:Starting cross validation
2023-05-20 15:31:01,486:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:02,790:INFO:Calculating mean and std
2023-05-20 15:31:02,792:INFO:Creating metrics dataframe
2023-05-20 15:31:02,886:INFO:Uploading results into container
2023-05-20 15:31:02,887:INFO:Uploading model into container now
2023-05-20 15:31:02,887:INFO:_master_model_container: 10
2023-05-20 15:31:02,887:INFO:_display_container: 2
2023-05-20 15:31:02,888:INFO:PassiveAggressiveRegressor(random_state=5829)
2023-05-20 15:31:02,888:INFO:create_model() successfully completed......................................
2023-05-20 15:31:02,945:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:02,945:INFO:Creating metrics dataframe
2023-05-20 15:31:02,954:INFO:Initializing Huber Regressor
2023-05-20 15:31:02,954:INFO:Total runtime is 0.32951108217239383 minutes
2023-05-20 15:31:02,957:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:02,957:INFO:Initializing create_model()
2023-05-20 15:31:02,958:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:02,958:INFO:Checking exceptions
2023-05-20 15:31:02,958:INFO:Importing libraries
2023-05-20 15:31:02,958:INFO:Copying training dataset
2023-05-20 15:31:02,968:INFO:Defining folds
2023-05-20 15:31:02,968:INFO:Declaring metric variables
2023-05-20 15:31:02,972:INFO:Importing untrained model
2023-05-20 15:31:02,975:INFO:Huber Regressor Imported successfully
2023-05-20 15:31:02,980:INFO:Starting cross validation
2023-05-20 15:31:02,982:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:05,115:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,426:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,458:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,485:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,496:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,507:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,518:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,542:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,557:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:05,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:31:06,670:INFO:Calculating mean and std
2023-05-20 15:31:06,671:INFO:Creating metrics dataframe
2023-05-20 15:31:06,772:INFO:Uploading results into container
2023-05-20 15:31:06,772:INFO:Uploading model into container now
2023-05-20 15:31:06,773:INFO:_master_model_container: 11
2023-05-20 15:31:06,773:INFO:_display_container: 2
2023-05-20 15:31:06,773:INFO:HuberRegressor()
2023-05-20 15:31:06,773:INFO:create_model() successfully completed......................................
2023-05-20 15:31:06,828:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:06,828:INFO:Creating metrics dataframe
2023-05-20 15:31:06,838:INFO:Initializing K Neighbors Regressor
2023-05-20 15:31:06,839:INFO:Total runtime is 0.39426385561625166 minutes
2023-05-20 15:31:06,842:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:06,842:INFO:Initializing create_model()
2023-05-20 15:31:06,843:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:06,843:INFO:Checking exceptions
2023-05-20 15:31:06,843:INFO:Importing libraries
2023-05-20 15:31:06,843:INFO:Copying training dataset
2023-05-20 15:31:06,850:INFO:Defining folds
2023-05-20 15:31:06,850:INFO:Declaring metric variables
2023-05-20 15:31:06,853:INFO:Importing untrained model
2023-05-20 15:31:06,856:INFO:K Neighbors Regressor Imported successfully
2023-05-20 15:31:06,862:INFO:Starting cross validation
2023-05-20 15:31:06,863:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:08,158:INFO:Calculating mean and std
2023-05-20 15:31:08,161:INFO:Creating metrics dataframe
2023-05-20 15:31:08,264:INFO:Uploading results into container
2023-05-20 15:31:08,265:INFO:Uploading model into container now
2023-05-20 15:31:08,265:INFO:_master_model_container: 12
2023-05-20 15:31:08,265:INFO:_display_container: 2
2023-05-20 15:31:08,265:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 15:31:08,265:INFO:create_model() successfully completed......................................
2023-05-20 15:31:08,321:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:08,321:INFO:Creating metrics dataframe
2023-05-20 15:31:08,327:INFO:Initializing Decision Tree Regressor
2023-05-20 15:31:08,327:INFO:Total runtime is 0.41906597216924035 minutes
2023-05-20 15:31:08,327:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:08,335:INFO:Initializing create_model()
2023-05-20 15:31:08,335:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:08,335:INFO:Checking exceptions
2023-05-20 15:31:08,335:INFO:Importing libraries
2023-05-20 15:31:08,335:INFO:Copying training dataset
2023-05-20 15:31:08,343:INFO:Defining folds
2023-05-20 15:31:08,344:INFO:Declaring metric variables
2023-05-20 15:31:08,346:INFO:Importing untrained model
2023-05-20 15:31:08,349:INFO:Decision Tree Regressor Imported successfully
2023-05-20 15:31:08,355:INFO:Starting cross validation
2023-05-20 15:31:08,357:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:09,660:INFO:Calculating mean and std
2023-05-20 15:31:09,662:INFO:Creating metrics dataframe
2023-05-20 15:31:09,760:INFO:Uploading results into container
2023-05-20 15:31:09,761:INFO:Uploading model into container now
2023-05-20 15:31:09,761:INFO:_master_model_container: 13
2023-05-20 15:31:09,761:INFO:_display_container: 2
2023-05-20 15:31:09,761:INFO:DecisionTreeRegressor(random_state=5829)
2023-05-20 15:31:09,761:INFO:create_model() successfully completed......................................
2023-05-20 15:31:09,816:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:09,816:INFO:Creating metrics dataframe
2023-05-20 15:31:09,825:INFO:Initializing Random Forest Regressor
2023-05-20 15:31:09,826:INFO:Total runtime is 0.4440395832061768 minutes
2023-05-20 15:31:09,828:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:09,828:INFO:Initializing create_model()
2023-05-20 15:31:09,828:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:09,828:INFO:Checking exceptions
2023-05-20 15:31:09,828:INFO:Importing libraries
2023-05-20 15:31:09,828:INFO:Copying training dataset
2023-05-20 15:31:09,838:INFO:Defining folds
2023-05-20 15:31:09,838:INFO:Declaring metric variables
2023-05-20 15:31:09,842:INFO:Importing untrained model
2023-05-20 15:31:09,846:INFO:Random Forest Regressor Imported successfully
2023-05-20 15:31:09,851:INFO:Starting cross validation
2023-05-20 15:31:09,853:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:11,830:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:31:11,938:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.02s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:31:12,093:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:31:13,367:INFO:Calculating mean and std
2023-05-20 15:31:13,368:INFO:Creating metrics dataframe
2023-05-20 15:31:13,473:INFO:Uploading results into container
2023-05-20 15:31:13,473:INFO:Uploading model into container now
2023-05-20 15:31:13,474:INFO:_master_model_container: 14
2023-05-20 15:31:13,474:INFO:_display_container: 2
2023-05-20 15:31:13,474:INFO:RandomForestRegressor(n_jobs=-1, random_state=5829)
2023-05-20 15:31:13,474:INFO:create_model() successfully completed......................................
2023-05-20 15:31:13,529:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:13,529:INFO:Creating metrics dataframe
2023-05-20 15:31:13,542:INFO:Initializing Extra Trees Regressor
2023-05-20 15:31:13,542:INFO:Total runtime is 0.5059714913368225 minutes
2023-05-20 15:31:13,544:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:13,545:INFO:Initializing create_model()
2023-05-20 15:31:13,545:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:13,545:INFO:Checking exceptions
2023-05-20 15:31:13,545:INFO:Importing libraries
2023-05-20 15:31:13,545:INFO:Copying training dataset
2023-05-20 15:31:13,553:INFO:Defining folds
2023-05-20 15:31:13,553:INFO:Declaring metric variables
2023-05-20 15:31:13,554:INFO:Importing untrained model
2023-05-20 15:31:13,554:INFO:Extra Trees Regressor Imported successfully
2023-05-20 15:31:13,562:INFO:Starting cross validation
2023-05-20 15:31:13,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:15,839:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.91s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:31:17,495:INFO:Calculating mean and std
2023-05-20 15:31:17,495:INFO:Creating metrics dataframe
2023-05-20 15:31:17,604:INFO:Uploading results into container
2023-05-20 15:31:17,605:INFO:Uploading model into container now
2023-05-20 15:31:17,605:INFO:_master_model_container: 15
2023-05-20 15:31:17,605:INFO:_display_container: 2
2023-05-20 15:31:17,605:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5829)
2023-05-20 15:31:17,606:INFO:create_model() successfully completed......................................
2023-05-20 15:31:17,662:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:17,662:INFO:Creating metrics dataframe
2023-05-20 15:31:17,669:INFO:Initializing AdaBoost Regressor
2023-05-20 15:31:17,669:INFO:Total runtime is 0.5747642517089844 minutes
2023-05-20 15:31:17,669:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:17,669:INFO:Initializing create_model()
2023-05-20 15:31:17,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:17,677:INFO:Checking exceptions
2023-05-20 15:31:17,677:INFO:Importing libraries
2023-05-20 15:31:17,677:INFO:Copying training dataset
2023-05-20 15:31:17,685:INFO:Defining folds
2023-05-20 15:31:17,685:INFO:Declaring metric variables
2023-05-20 15:31:17,687:INFO:Importing untrained model
2023-05-20 15:31:17,690:INFO:AdaBoost Regressor Imported successfully
2023-05-20 15:31:17,696:INFO:Starting cross validation
2023-05-20 15:31:17,697:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:20,159:INFO:Calculating mean and std
2023-05-20 15:31:20,161:INFO:Creating metrics dataframe
2023-05-20 15:31:20,277:INFO:Uploading results into container
2023-05-20 15:31:20,278:INFO:Uploading model into container now
2023-05-20 15:31:20,278:INFO:_master_model_container: 16
2023-05-20 15:31:20,278:INFO:_display_container: 2
2023-05-20 15:31:20,278:INFO:AdaBoostRegressor(random_state=5829)
2023-05-20 15:31:20,279:INFO:create_model() successfully completed......................................
2023-05-20 15:31:20,337:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:20,337:INFO:Creating metrics dataframe
2023-05-20 15:31:20,346:INFO:Initializing Gradient Boosting Regressor
2023-05-20 15:31:20,346:INFO:Total runtime is 0.6193805654843648 minutes
2023-05-20 15:31:20,348:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:20,348:INFO:Initializing create_model()
2023-05-20 15:31:20,348:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:20,348:INFO:Checking exceptions
2023-05-20 15:31:20,349:INFO:Importing libraries
2023-05-20 15:31:20,349:INFO:Copying training dataset
2023-05-20 15:31:20,365:INFO:Defining folds
2023-05-20 15:31:20,365:INFO:Declaring metric variables
2023-05-20 15:31:20,368:INFO:Importing untrained model
2023-05-20 15:31:20,371:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 15:31:20,377:INFO:Starting cross validation
2023-05-20 15:31:20,379:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:22,828:INFO:Calculating mean and std
2023-05-20 15:31:22,830:INFO:Creating metrics dataframe
2023-05-20 15:31:22,954:INFO:Uploading results into container
2023-05-20 15:31:22,954:INFO:Uploading model into container now
2023-05-20 15:31:22,955:INFO:_master_model_container: 17
2023-05-20 15:31:22,955:INFO:_display_container: 2
2023-05-20 15:31:22,955:INFO:GradientBoostingRegressor(random_state=5829)
2023-05-20 15:31:22,955:INFO:create_model() successfully completed......................................
2023-05-20 15:31:23,012:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:23,012:INFO:Creating metrics dataframe
2023-05-20 15:31:23,023:INFO:Initializing Extreme Gradient Boosting
2023-05-20 15:31:23,024:INFO:Total runtime is 0.6640081961949666 minutes
2023-05-20 15:31:23,028:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:23,029:INFO:Initializing create_model()
2023-05-20 15:31:23,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:23,029:INFO:Checking exceptions
2023-05-20 15:31:23,029:INFO:Importing libraries
2023-05-20 15:31:23,029:INFO:Copying training dataset
2023-05-20 15:31:23,037:INFO:Defining folds
2023-05-20 15:31:23,038:INFO:Declaring metric variables
2023-05-20 15:31:23,040:INFO:Importing untrained model
2023-05-20 15:31:23,044:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 15:31:23,049:INFO:Starting cross validation
2023-05-20 15:31:23,050:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:26,295:INFO:Calculating mean and std
2023-05-20 15:31:26,297:INFO:Creating metrics dataframe
2023-05-20 15:31:26,430:INFO:Uploading results into container
2023-05-20 15:31:26,431:INFO:Uploading model into container now
2023-05-20 15:31:26,431:INFO:_master_model_container: 18
2023-05-20 15:31:26,431:INFO:_display_container: 2
2023-05-20 15:31:26,432:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5829, ...)
2023-05-20 15:31:26,432:INFO:create_model() successfully completed......................................
2023-05-20 15:31:26,491:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:26,491:INFO:Creating metrics dataframe
2023-05-20 15:31:26,502:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 15:31:26,502:INFO:Total runtime is 0.7219757199287414 minutes
2023-05-20 15:31:26,506:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:26,506:INFO:Initializing create_model()
2023-05-20 15:31:26,507:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:26,507:INFO:Checking exceptions
2023-05-20 15:31:26,507:INFO:Importing libraries
2023-05-20 15:31:26,507:INFO:Copying training dataset
2023-05-20 15:31:26,515:INFO:Defining folds
2023-05-20 15:31:26,515:INFO:Declaring metric variables
2023-05-20 15:31:26,518:INFO:Importing untrained model
2023-05-20 15:31:26,522:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 15:31:26,528:INFO:Starting cross validation
2023-05-20 15:31:26,529:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:28,618:INFO:Calculating mean and std
2023-05-20 15:31:28,618:INFO:Creating metrics dataframe
2023-05-20 15:31:28,754:INFO:Uploading results into container
2023-05-20 15:31:28,754:INFO:Uploading model into container now
2023-05-20 15:31:28,755:INFO:_master_model_container: 19
2023-05-20 15:31:28,755:INFO:_display_container: 2
2023-05-20 15:31:28,755:INFO:LGBMRegressor(random_state=5829)
2023-05-20 15:31:28,756:INFO:create_model() successfully completed......................................
2023-05-20 15:31:28,814:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:28,814:INFO:Creating metrics dataframe
2023-05-20 15:31:28,818:INFO:Initializing CatBoost Regressor
2023-05-20 15:31:28,818:INFO:Total runtime is 0.7605814536412556 minutes
2023-05-20 15:31:28,827:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:28,827:INFO:Initializing create_model()
2023-05-20 15:31:28,828:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:28,828:INFO:Checking exceptions
2023-05-20 15:31:28,828:INFO:Importing libraries
2023-05-20 15:31:28,828:INFO:Copying training dataset
2023-05-20 15:31:28,837:INFO:Defining folds
2023-05-20 15:31:28,837:INFO:Declaring metric variables
2023-05-20 15:31:28,840:INFO:Importing untrained model
2023-05-20 15:31:28,844:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:31:28,850:INFO:Starting cross validation
2023-05-20 15:31:28,852:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:57,221:INFO:Calculating mean and std
2023-05-20 15:31:57,223:INFO:Creating metrics dataframe
2023-05-20 15:31:57,378:INFO:Uploading results into container
2023-05-20 15:31:57,379:INFO:Uploading model into container now
2023-05-20 15:31:57,379:INFO:_master_model_container: 20
2023-05-20 15:31:57,379:INFO:_display_container: 2
2023-05-20 15:31:57,379:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF67075E0>
2023-05-20 15:31:57,380:INFO:create_model() successfully completed......................................
2023-05-20 15:31:57,445:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:57,446:INFO:Creating metrics dataframe
2023-05-20 15:31:57,457:INFO:Initializing Dummy Regressor
2023-05-20 15:31:57,457:INFO:Total runtime is 1.2378875136375427 minutes
2023-05-20 15:31:57,460:INFO:SubProcess create_model() called ==================================
2023-05-20 15:31:57,460:INFO:Initializing create_model()
2023-05-20 15:31:57,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF687A110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:57,461:INFO:Checking exceptions
2023-05-20 15:31:57,461:INFO:Importing libraries
2023-05-20 15:31:57,461:INFO:Copying training dataset
2023-05-20 15:31:57,470:INFO:Defining folds
2023-05-20 15:31:57,470:INFO:Declaring metric variables
2023-05-20 15:31:57,473:INFO:Importing untrained model
2023-05-20 15:31:57,475:INFO:Dummy Regressor Imported successfully
2023-05-20 15:31:57,481:INFO:Starting cross validation
2023-05-20 15:31:57,482:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:31:59,177:INFO:Calculating mean and std
2023-05-20 15:31:59,182:INFO:Creating metrics dataframe
2023-05-20 15:31:59,334:INFO:Uploading results into container
2023-05-20 15:31:59,336:INFO:Uploading model into container now
2023-05-20 15:31:59,336:INFO:_master_model_container: 21
2023-05-20 15:31:59,336:INFO:_display_container: 2
2023-05-20 15:31:59,336:INFO:DummyRegressor()
2023-05-20 15:31:59,336:INFO:create_model() successfully completed......................................
2023-05-20 15:31:59,402:INFO:SubProcess create_model() end ==================================
2023-05-20 15:31:59,402:INFO:Creating metrics dataframe
2023-05-20 15:31:59,421:INFO:Initializing create_model()
2023-05-20 15:31:59,421:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BD979B850>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BF67075E0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:31:59,421:INFO:Checking exceptions
2023-05-20 15:31:59,422:INFO:Importing libraries
2023-05-20 15:31:59,422:INFO:Copying training dataset
2023-05-20 15:31:59,431:INFO:Defining folds
2023-05-20 15:31:59,431:INFO:Declaring metric variables
2023-05-20 15:31:59,431:INFO:Importing untrained model
2023-05-20 15:31:59,431:INFO:Declaring custom model
2023-05-20 15:31:59,432:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:31:59,433:INFO:Cross validation set to False
2023-05-20 15:31:59,433:INFO:Fitting Model
2023-05-20 15:32:03,090:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD990DF60>
2023-05-20 15:32:03,090:INFO:create_model() successfully completed......................................
2023-05-20 15:32:03,174:INFO:_master_model_container: 21
2023-05-20 15:32:03,174:INFO:_display_container: 2
2023-05-20 15:32:03,174:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD990DF60>
2023-05-20 15:32:03,174:INFO:compare_models() successfully completed......................................
2023-05-20 15:35:28,313:INFO:PyCaret RegressionExperiment
2023-05-20 15:35:28,313:INFO:Logging name: reg-default-name
2023-05-20 15:35:28,313:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:35:28,313:INFO:version 3.0.2
2023-05-20 15:35:28,313:INFO:Initializing setup()
2023-05-20 15:35:28,313:INFO:self.USI: d37f
2023-05-20 15:35:28,314:INFO:self._variable_keys: {'exp_id', 'logging_param', 'fold_groups_param', 'data', 'gpu_param', 'seed', 'memory', 'n_jobs_param', 'idx', 'X', 'html_param', 'transform_target_param', 'USI', '_available_plots', 'y_train', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', 'fold_shuffle_param', 'y', 'pipeline', 'X_train', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase', 'y_test'}
2023-05-20 15:35:28,314:INFO:Checking environment
2023-05-20 15:35:28,314:INFO:python_version: 3.10.3
2023-05-20 15:35:28,314:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:35:28,314:INFO:machine: AMD64
2023-05-20 15:35:28,314:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:35:28,314:INFO:Memory: svmem(total=17083187200, available=6867517440, percent=59.8, used=10215669760, free=6867517440)
2023-05-20 15:35:28,314:INFO:Physical Core: 6
2023-05-20 15:35:28,314:INFO:Logical Core: 12
2023-05-20 15:35:28,314:INFO:Checking libraries
2023-05-20 15:35:28,314:INFO:System:
2023-05-20 15:35:28,314:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:35:28,314:INFO:executable: c:\Python310\python.exe
2023-05-20 15:35:28,314:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:35:28,314:INFO:PyCaret required dependencies:
2023-05-20 15:35:28,314:INFO:                 pip: 23.1.2
2023-05-20 15:35:28,314:INFO:          setuptools: 58.1.0
2023-05-20 15:35:28,314:INFO:             pycaret: 3.0.2
2023-05-20 15:35:28,314:INFO:             IPython: 8.5.0
2023-05-20 15:35:28,314:INFO:          ipywidgets: 8.0.6
2023-05-20 15:35:28,314:INFO:                tqdm: 4.65.0
2023-05-20 15:35:28,314:INFO:               numpy: 1.23.2
2023-05-20 15:35:28,315:INFO:              pandas: 1.5.2
2023-05-20 15:35:28,315:INFO:              jinja2: 3.1.2
2023-05-20 15:35:28,315:INFO:               scipy: 1.9.3
2023-05-20 15:35:28,315:INFO:              joblib: 1.2.0
2023-05-20 15:35:28,315:INFO:             sklearn: 1.1.3
2023-05-20 15:35:28,315:INFO:                pyod: 1.0.9
2023-05-20 15:35:28,315:INFO:            imblearn: 0.10.1
2023-05-20 15:35:28,315:INFO:   category_encoders: 2.6.1
2023-05-20 15:35:28,315:INFO:            lightgbm: 3.3.5
2023-05-20 15:35:28,315:INFO:               numba: 0.57.0
2023-05-20 15:35:28,315:INFO:            requests: 2.28.2
2023-05-20 15:35:28,315:INFO:          matplotlib: 3.5.3
2023-05-20 15:35:28,315:INFO:          scikitplot: 0.3.7
2023-05-20 15:35:28,315:INFO:         yellowbrick: 1.5
2023-05-20 15:35:28,315:INFO:              plotly: 5.13.1
2023-05-20 15:35:28,315:INFO:             kaleido: 0.2.1
2023-05-20 15:35:28,315:INFO:         statsmodels: 0.13.5
2023-05-20 15:35:28,315:INFO:              sktime: 0.17.0
2023-05-20 15:35:28,315:INFO:               tbats: 1.1.3
2023-05-20 15:35:28,315:INFO:            pmdarima: 2.0.3
2023-05-20 15:35:28,315:INFO:              psutil: 5.9.2
2023-05-20 15:35:28,315:INFO:PyCaret optional dependencies:
2023-05-20 15:35:28,315:INFO:                shap: Not installed
2023-05-20 15:35:28,316:INFO:           interpret: Not installed
2023-05-20 15:35:28,316:INFO:                umap: Not installed
2023-05-20 15:35:28,316:INFO:    pandas_profiling: Not installed
2023-05-20 15:35:28,316:INFO:  explainerdashboard: Not installed
2023-05-20 15:35:28,316:INFO:             autoviz: Not installed
2023-05-20 15:35:28,316:INFO:           fairlearn: Not installed
2023-05-20 15:35:28,316:INFO:             xgboost: 1.7.4
2023-05-20 15:35:28,316:INFO:            catboost: 1.1.1
2023-05-20 15:35:28,316:INFO:              kmodes: Not installed
2023-05-20 15:35:28,316:INFO:             mlxtend: Not installed
2023-05-20 15:35:28,316:INFO:       statsforecast: Not installed
2023-05-20 15:35:28,316:INFO:        tune_sklearn: Not installed
2023-05-20 15:35:28,316:INFO:                 ray: Not installed
2023-05-20 15:35:28,316:INFO:            hyperopt: Not installed
2023-05-20 15:35:28,316:INFO:              optuna: Not installed
2023-05-20 15:35:28,316:INFO:               skopt: Not installed
2023-05-20 15:35:28,316:INFO:              mlflow: Not installed
2023-05-20 15:35:28,316:INFO:              gradio: Not installed
2023-05-20 15:35:28,316:INFO:             fastapi: Not installed
2023-05-20 15:35:28,316:INFO:             uvicorn: Not installed
2023-05-20 15:35:28,316:INFO:              m2cgen: Not installed
2023-05-20 15:35:28,316:INFO:           evidently: Not installed
2023-05-20 15:35:28,316:INFO:               fugue: Not installed
2023-05-20 15:35:28,316:INFO:           streamlit: Not installed
2023-05-20 15:35:28,317:INFO:             prophet: Not installed
2023-05-20 15:35:28,317:INFO:None
2023-05-20 15:35:28,317:INFO:Set up data.
2023-05-20 15:35:28,413:INFO:Set up train/test split.
2023-05-20 15:35:28,428:INFO:Set up index.
2023-05-20 15:35:28,429:INFO:Set up folding strategy.
2023-05-20 15:35:28,429:INFO:Assigning column types.
2023-05-20 15:35:28,442:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:35:28,443:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,447:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,453:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,512:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,554:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,555:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:28,557:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:28,558:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,562:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,566:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,628:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,673:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,673:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:28,676:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:28,677:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:35:28,681:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,686:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,772:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,827:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,827:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:28,830:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:28,835:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,840:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,909:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,953:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:28,954:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:28,956:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:28,956:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:35:28,965:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,043:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,088:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,089:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,090:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,100:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,155:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,195:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,195:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,198:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,198:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:35:29,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,305:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,306:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,308:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,372:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,413:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,414:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,417:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,417:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:35:29,481:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,522:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,525:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,588:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:35:29,628:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,630:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,634:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:35:29,736:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,738:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,841:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:29,844:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:29,845:INFO:Preparing preprocessing pipeline...
2023-05-20 15:35:29,845:INFO:Set up simple imputation.
2023-05-20 15:35:29,846:INFO:Set up column name cleaning.
2023-05-20 15:35:29,887:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:35:29,893:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:35:29,893:INFO:Creating final display dataframe.
2023-05-20 15:35:30,043:INFO:Setup _display_container:                     Description             Value
0                    Session id              8203
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              d37f
2023-05-20 15:35:30,153:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:30,155:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:30,260:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:35:30,262:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:35:30,262:INFO:setup() successfully completed in 2.12s...............
2023-05-20 15:35:41,674:INFO:Initializing compare_models()
2023-05-20 15:35:41,675:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 15:35:41,675:INFO:Checking exceptions
2023-05-20 15:35:41,683:INFO:Preparing display monitor
2023-05-20 15:35:41,731:INFO:Initializing Linear Regression
2023-05-20 15:35:41,731:INFO:Total runtime is 0.0 minutes
2023-05-20 15:35:41,734:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:41,735:INFO:Initializing create_model()
2023-05-20 15:35:41,735:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:41,735:INFO:Checking exceptions
2023-05-20 15:35:41,735:INFO:Importing libraries
2023-05-20 15:35:41,735:INFO:Copying training dataset
2023-05-20 15:35:41,748:INFO:Defining folds
2023-05-20 15:35:41,748:INFO:Declaring metric variables
2023-05-20 15:35:41,754:INFO:Importing untrained model
2023-05-20 15:35:41,757:INFO:Linear Regression Imported successfully
2023-05-20 15:35:41,764:INFO:Starting cross validation
2023-05-20 15:35:41,766:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:43,512:INFO:Calculating mean and std
2023-05-20 15:35:43,515:INFO:Creating metrics dataframe
2023-05-20 15:35:43,659:INFO:Uploading results into container
2023-05-20 15:35:43,660:INFO:Uploading model into container now
2023-05-20 15:35:43,660:INFO:_master_model_container: 1
2023-05-20 15:35:43,660:INFO:_display_container: 2
2023-05-20 15:35:43,660:INFO:LinearRegression(n_jobs=-1)
2023-05-20 15:35:43,661:INFO:create_model() successfully completed......................................
2023-05-20 15:35:43,731:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:43,731:INFO:Creating metrics dataframe
2023-05-20 15:35:43,738:INFO:Initializing Lasso Regression
2023-05-20 15:35:43,739:INFO:Total runtime is 0.03345442215601603 minutes
2023-05-20 15:35:43,742:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:43,743:INFO:Initializing create_model()
2023-05-20 15:35:43,743:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:43,743:INFO:Checking exceptions
2023-05-20 15:35:43,743:INFO:Importing libraries
2023-05-20 15:35:43,743:INFO:Copying training dataset
2023-05-20 15:35:43,754:INFO:Defining folds
2023-05-20 15:35:43,754:INFO:Declaring metric variables
2023-05-20 15:35:43,757:INFO:Importing untrained model
2023-05-20 15:35:43,761:INFO:Lasso Regression Imported successfully
2023-05-20 15:35:43,772:INFO:Starting cross validation
2023-05-20 15:35:43,776:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:45,414:INFO:Calculating mean and std
2023-05-20 15:35:45,415:INFO:Creating metrics dataframe
2023-05-20 15:35:45,569:INFO:Uploading results into container
2023-05-20 15:35:45,570:INFO:Uploading model into container now
2023-05-20 15:35:45,570:INFO:_master_model_container: 2
2023-05-20 15:35:45,570:INFO:_display_container: 2
2023-05-20 15:35:45,571:INFO:Lasso(random_state=8203)
2023-05-20 15:35:45,571:INFO:create_model() successfully completed......................................
2023-05-20 15:35:45,637:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:45,637:INFO:Creating metrics dataframe
2023-05-20 15:35:45,645:INFO:Initializing Ridge Regression
2023-05-20 15:35:45,645:INFO:Total runtime is 0.065233846505483 minutes
2023-05-20 15:35:45,648:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:45,648:INFO:Initializing create_model()
2023-05-20 15:35:45,648:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:45,648:INFO:Checking exceptions
2023-05-20 15:35:45,648:INFO:Importing libraries
2023-05-20 15:35:45,649:INFO:Copying training dataset
2023-05-20 15:35:45,656:INFO:Defining folds
2023-05-20 15:35:45,656:INFO:Declaring metric variables
2023-05-20 15:35:45,660:INFO:Importing untrained model
2023-05-20 15:35:45,663:INFO:Ridge Regression Imported successfully
2023-05-20 15:35:45,669:INFO:Starting cross validation
2023-05-20 15:35:45,670:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:47,443:INFO:Calculating mean and std
2023-05-20 15:35:47,445:INFO:Creating metrics dataframe
2023-05-20 15:35:47,592:INFO:Uploading results into container
2023-05-20 15:35:47,593:INFO:Uploading model into container now
2023-05-20 15:35:47,593:INFO:_master_model_container: 3
2023-05-20 15:35:47,593:INFO:_display_container: 2
2023-05-20 15:35:47,594:INFO:Ridge(random_state=8203)
2023-05-20 15:35:47,594:INFO:create_model() successfully completed......................................
2023-05-20 15:35:47,664:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:47,664:INFO:Creating metrics dataframe
2023-05-20 15:35:47,672:INFO:Initializing Elastic Net
2023-05-20 15:35:47,673:INFO:Total runtime is 0.09904308716456096 minutes
2023-05-20 15:35:47,677:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:47,677:INFO:Initializing create_model()
2023-05-20 15:35:47,677:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:47,677:INFO:Checking exceptions
2023-05-20 15:35:47,677:INFO:Importing libraries
2023-05-20 15:35:47,677:INFO:Copying training dataset
2023-05-20 15:35:47,687:INFO:Defining folds
2023-05-20 15:35:47,687:INFO:Declaring metric variables
2023-05-20 15:35:47,690:INFO:Importing untrained model
2023-05-20 15:35:47,694:INFO:Elastic Net Imported successfully
2023-05-20 15:35:47,701:INFO:Starting cross validation
2023-05-20 15:35:47,703:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:49,368:INFO:Calculating mean and std
2023-05-20 15:35:49,369:INFO:Creating metrics dataframe
2023-05-20 15:35:49,530:INFO:Uploading results into container
2023-05-20 15:35:49,531:INFO:Uploading model into container now
2023-05-20 15:35:49,531:INFO:_master_model_container: 4
2023-05-20 15:35:49,532:INFO:_display_container: 2
2023-05-20 15:35:49,532:INFO:ElasticNet(random_state=8203)
2023-05-20 15:35:49,532:INFO:create_model() successfully completed......................................
2023-05-20 15:35:49,600:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:49,601:INFO:Creating metrics dataframe
2023-05-20 15:35:49,610:INFO:Initializing Least Angle Regression
2023-05-20 15:35:49,610:INFO:Total runtime is 0.13132057587305707 minutes
2023-05-20 15:35:49,613:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:49,613:INFO:Initializing create_model()
2023-05-20 15:35:49,614:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:49,614:INFO:Checking exceptions
2023-05-20 15:35:49,614:INFO:Importing libraries
2023-05-20 15:35:49,614:INFO:Copying training dataset
2023-05-20 15:35:49,622:INFO:Defining folds
2023-05-20 15:35:49,622:INFO:Declaring metric variables
2023-05-20 15:35:49,625:INFO:Importing untrained model
2023-05-20 15:35:49,628:INFO:Least Angle Regression Imported successfully
2023-05-20 15:35:49,633:INFO:Starting cross validation
2023-05-20 15:35:49,634:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:49,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,763:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,766:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,776:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.372e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,780:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.482e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,785:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.818e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,785:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.105e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,788:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,789:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.169e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,792:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.790e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,793:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,794:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=5.432e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,797:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,799:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.838e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=2.190e-04, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.055e-04, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,807:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.932e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,815:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.533e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,816:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,817:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.706e-04, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,818:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.192e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,819:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.337e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,820:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.793e-04, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.096e-04, with an active set of 123 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,824:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,831:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=1.224e-04, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,833:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.199e-04, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,833:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,841:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:49,846:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=5.582e-04, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,849:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=5.090e-04, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,872:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=1.553e-04, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,872:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.793e-04, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,876:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.598e-04, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,884:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 257 iterations, i.e. alpha=1.649e-02, with an active set of 201 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,889:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 174 iterations, i.e. alpha=3.958e-04, with an active set of 154 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,891:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 217 iterations, i.e. alpha=2.739e-02, with an active set of 185 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,927:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 264 iterations, i.e. alpha=1.576e+00, with an active set of 216 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,929:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 209 iterations, i.e. alpha=7.321e-04, with an active set of 175 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:35:49,960:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:35:49,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:35:49,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:35:49,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:35:49,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:35:49,981:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:35:49,982:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:35:49,983:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:35:49,984:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:35:49,985:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:35:50,002:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 15:35:50,043:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:35:50,044:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:35:50,044:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:35:50,045:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:35:50,045:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:35:50,250:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:35:50,251:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:35:50,278:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,278:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,279:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:50,285:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,286:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,286:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:50,290:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,291:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,291:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:50,300:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,300:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,300:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:50,334:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,335:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,335:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:50,358:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,358:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:35:50,360:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:35:51,764:INFO:Calculating mean and std
2023-05-20 15:35:51,765:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:35:51,766:INFO:Creating metrics dataframe
2023-05-20 15:35:51,917:INFO:Uploading results into container
2023-05-20 15:35:51,918:INFO:Uploading model into container now
2023-05-20 15:35:51,918:INFO:_master_model_container: 5
2023-05-20 15:35:51,918:INFO:_display_container: 2
2023-05-20 15:35:51,919:INFO:Lars(random_state=8203)
2023-05-20 15:35:51,919:INFO:create_model() successfully completed......................................
2023-05-20 15:35:51,982:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:51,982:INFO:Creating metrics dataframe
2023-05-20 15:35:51,991:INFO:Initializing Lasso Least Angle Regression
2023-05-20 15:35:51,991:INFO:Total runtime is 0.1709998408953349 minutes
2023-05-20 15:35:51,994:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:51,995:INFO:Initializing create_model()
2023-05-20 15:35:51,995:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:51,995:INFO:Checking exceptions
2023-05-20 15:35:51,995:INFO:Importing libraries
2023-05-20 15:35:51,995:INFO:Copying training dataset
2023-05-20 15:35:52,003:INFO:Defining folds
2023-05-20 15:35:52,003:INFO:Declaring metric variables
2023-05-20 15:35:52,006:INFO:Importing untrained model
2023-05-20 15:35:52,009:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 15:35:52,014:INFO:Starting cross validation
2023-05-20 15:35:52,016:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:52,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,138:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,141:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,155:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,160:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,179:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,182:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,202:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,207:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:52,220:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:35:53,708:INFO:Calculating mean and std
2023-05-20 15:35:53,710:INFO:Creating metrics dataframe
2023-05-20 15:35:53,861:INFO:Uploading results into container
2023-05-20 15:35:53,862:INFO:Uploading model into container now
2023-05-20 15:35:53,863:INFO:_master_model_container: 6
2023-05-20 15:35:53,863:INFO:_display_container: 2
2023-05-20 15:35:53,863:INFO:LassoLars(random_state=8203)
2023-05-20 15:35:53,863:INFO:create_model() successfully completed......................................
2023-05-20 15:35:53,930:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:53,930:INFO:Creating metrics dataframe
2023-05-20 15:35:53,940:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 15:35:53,940:INFO:Total runtime is 0.20349561373392744 minutes
2023-05-20 15:35:53,945:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:53,945:INFO:Initializing create_model()
2023-05-20 15:35:53,945:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:53,945:INFO:Checking exceptions
2023-05-20 15:35:53,945:INFO:Importing libraries
2023-05-20 15:35:53,945:INFO:Copying training dataset
2023-05-20 15:35:53,961:INFO:Defining folds
2023-05-20 15:35:53,961:INFO:Declaring metric variables
2023-05-20 15:35:53,967:INFO:Importing untrained model
2023-05-20 15:35:53,971:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 15:35:53,977:INFO:Starting cross validation
2023-05-20 15:35:53,978:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:54,090:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,095:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,102:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,108:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,138:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,148:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,168:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,169:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,178:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:54,196:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:35:55,688:INFO:Calculating mean and std
2023-05-20 15:35:55,690:INFO:Creating metrics dataframe
2023-05-20 15:35:55,844:INFO:Uploading results into container
2023-05-20 15:35:55,845:INFO:Uploading model into container now
2023-05-20 15:35:55,846:INFO:_master_model_container: 7
2023-05-20 15:35:55,846:INFO:_display_container: 2
2023-05-20 15:35:55,846:INFO:OrthogonalMatchingPursuit()
2023-05-20 15:35:55,846:INFO:create_model() successfully completed......................................
2023-05-20 15:35:55,912:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:55,912:INFO:Creating metrics dataframe
2023-05-20 15:35:55,920:INFO:Initializing Bayesian Ridge
2023-05-20 15:35:55,921:INFO:Total runtime is 0.2365135987599691 minutes
2023-05-20 15:35:55,924:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:55,924:INFO:Initializing create_model()
2023-05-20 15:35:55,924:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:55,924:INFO:Checking exceptions
2023-05-20 15:35:55,924:INFO:Importing libraries
2023-05-20 15:35:55,924:INFO:Copying training dataset
2023-05-20 15:35:55,933:INFO:Defining folds
2023-05-20 15:35:55,933:INFO:Declaring metric variables
2023-05-20 15:35:55,936:INFO:Importing untrained model
2023-05-20 15:35:55,938:INFO:Bayesian Ridge Imported successfully
2023-05-20 15:35:55,945:INFO:Starting cross validation
2023-05-20 15:35:55,946:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:57,690:INFO:Calculating mean and std
2023-05-20 15:35:57,692:INFO:Creating metrics dataframe
2023-05-20 15:35:57,843:INFO:Uploading results into container
2023-05-20 15:35:57,844:INFO:Uploading model into container now
2023-05-20 15:35:57,845:INFO:_master_model_container: 8
2023-05-20 15:35:57,845:INFO:_display_container: 2
2023-05-20 15:35:57,846:INFO:BayesianRidge()
2023-05-20 15:35:57,846:INFO:create_model() successfully completed......................................
2023-05-20 15:35:57,912:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:57,912:INFO:Creating metrics dataframe
2023-05-20 15:35:57,920:INFO:Initializing Passive Aggressive Regressor
2023-05-20 15:35:57,921:INFO:Total runtime is 0.26983929077784224 minutes
2023-05-20 15:35:57,924:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:57,924:INFO:Initializing create_model()
2023-05-20 15:35:57,924:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:57,924:INFO:Checking exceptions
2023-05-20 15:35:57,925:INFO:Importing libraries
2023-05-20 15:35:57,925:INFO:Copying training dataset
2023-05-20 15:35:57,933:INFO:Defining folds
2023-05-20 15:35:57,933:INFO:Declaring metric variables
2023-05-20 15:35:57,936:INFO:Importing untrained model
2023-05-20 15:35:57,939:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 15:35:57,945:INFO:Starting cross validation
2023-05-20 15:35:57,947:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:35:59,707:INFO:Calculating mean and std
2023-05-20 15:35:59,710:INFO:Creating metrics dataframe
2023-05-20 15:35:59,860:INFO:Uploading results into container
2023-05-20 15:35:59,861:INFO:Uploading model into container now
2023-05-20 15:35:59,862:INFO:_master_model_container: 9
2023-05-20 15:35:59,862:INFO:_display_container: 2
2023-05-20 15:35:59,862:INFO:PassiveAggressiveRegressor(random_state=8203)
2023-05-20 15:35:59,862:INFO:create_model() successfully completed......................................
2023-05-20 15:35:59,925:INFO:SubProcess create_model() end ==================================
2023-05-20 15:35:59,925:INFO:Creating metrics dataframe
2023-05-20 15:35:59,935:INFO:Initializing Huber Regressor
2023-05-20 15:35:59,935:INFO:Total runtime is 0.3034041007359823 minutes
2023-05-20 15:35:59,938:INFO:SubProcess create_model() called ==================================
2023-05-20 15:35:59,938:INFO:Initializing create_model()
2023-05-20 15:35:59,938:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:35:59,938:INFO:Checking exceptions
2023-05-20 15:35:59,938:INFO:Importing libraries
2023-05-20 15:35:59,938:INFO:Copying training dataset
2023-05-20 15:35:59,946:INFO:Defining folds
2023-05-20 15:35:59,946:INFO:Declaring metric variables
2023-05-20 15:35:59,949:INFO:Importing untrained model
2023-05-20 15:35:59,951:INFO:Huber Regressor Imported successfully
2023-05-20 15:35:59,958:INFO:Starting cross validation
2023-05-20 15:35:59,959:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:02,436:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,772:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,826:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,855:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,857:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,867:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,867:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,917:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,933:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:02,936:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:36:04,470:INFO:Calculating mean and std
2023-05-20 15:36:04,472:INFO:Creating metrics dataframe
2023-05-20 15:36:04,668:INFO:Uploading results into container
2023-05-20 15:36:04,669:INFO:Uploading model into container now
2023-05-20 15:36:04,669:INFO:_master_model_container: 10
2023-05-20 15:36:04,671:INFO:_display_container: 2
2023-05-20 15:36:04,671:INFO:HuberRegressor()
2023-05-20 15:36:04,671:INFO:create_model() successfully completed......................................
2023-05-20 15:36:04,780:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:04,780:INFO:Creating metrics dataframe
2023-05-20 15:36:04,791:INFO:Initializing K Neighbors Regressor
2023-05-20 15:36:04,791:INFO:Total runtime is 0.3843398769696554 minutes
2023-05-20 15:36:04,794:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:04,795:INFO:Initializing create_model()
2023-05-20 15:36:04,795:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:04,795:INFO:Checking exceptions
2023-05-20 15:36:04,795:INFO:Importing libraries
2023-05-20 15:36:04,795:INFO:Copying training dataset
2023-05-20 15:36:04,806:INFO:Defining folds
2023-05-20 15:36:04,806:INFO:Declaring metric variables
2023-05-20 15:36:04,810:INFO:Importing untrained model
2023-05-20 15:36:04,813:INFO:K Neighbors Regressor Imported successfully
2023-05-20 15:36:04,820:INFO:Starting cross validation
2023-05-20 15:36:04,821:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:06,669:INFO:Calculating mean and std
2023-05-20 15:36:06,670:INFO:Creating metrics dataframe
2023-05-20 15:36:06,826:INFO:Uploading results into container
2023-05-20 15:36:06,827:INFO:Uploading model into container now
2023-05-20 15:36:06,828:INFO:_master_model_container: 11
2023-05-20 15:36:06,828:INFO:_display_container: 2
2023-05-20 15:36:06,828:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 15:36:06,828:INFO:create_model() successfully completed......................................
2023-05-20 15:36:06,890:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:06,890:INFO:Creating metrics dataframe
2023-05-20 15:36:06,900:INFO:Initializing Decision Tree Regressor
2023-05-20 15:36:06,900:INFO:Total runtime is 0.4194975018501282 minutes
2023-05-20 15:36:06,903:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:06,903:INFO:Initializing create_model()
2023-05-20 15:36:06,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:06,903:INFO:Checking exceptions
2023-05-20 15:36:06,903:INFO:Importing libraries
2023-05-20 15:36:06,903:INFO:Copying training dataset
2023-05-20 15:36:06,911:INFO:Defining folds
2023-05-20 15:36:06,911:INFO:Declaring metric variables
2023-05-20 15:36:06,914:INFO:Importing untrained model
2023-05-20 15:36:06,917:INFO:Decision Tree Regressor Imported successfully
2023-05-20 15:36:06,922:INFO:Starting cross validation
2023-05-20 15:36:06,924:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:08,711:INFO:Calculating mean and std
2023-05-20 15:36:08,713:INFO:Creating metrics dataframe
2023-05-20 15:36:08,870:INFO:Uploading results into container
2023-05-20 15:36:08,872:INFO:Uploading model into container now
2023-05-20 15:36:08,872:INFO:_master_model_container: 12
2023-05-20 15:36:08,872:INFO:_display_container: 2
2023-05-20 15:36:08,872:INFO:DecisionTreeRegressor(random_state=8203)
2023-05-20 15:36:08,872:INFO:create_model() successfully completed......................................
2023-05-20 15:36:08,936:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:08,936:INFO:Creating metrics dataframe
2023-05-20 15:36:08,946:INFO:Initializing Random Forest Regressor
2023-05-20 15:36:08,946:INFO:Total runtime is 0.4535829424858093 minutes
2023-05-20 15:36:08,949:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:08,949:INFO:Initializing create_model()
2023-05-20 15:36:08,949:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:08,949:INFO:Checking exceptions
2023-05-20 15:36:08,950:INFO:Importing libraries
2023-05-20 15:36:08,950:INFO:Copying training dataset
2023-05-20 15:36:08,958:INFO:Defining folds
2023-05-20 15:36:08,958:INFO:Declaring metric variables
2023-05-20 15:36:08,960:INFO:Importing untrained model
2023-05-20 15:36:08,964:INFO:Random Forest Regressor Imported successfully
2023-05-20 15:36:08,970:INFO:Starting cross validation
2023-05-20 15:36:08,971:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:11,041:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.05s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:36:11,042:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:36:11,314:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:36:13,113:INFO:Calculating mean and std
2023-05-20 15:36:13,115:INFO:Creating metrics dataframe
2023-05-20 15:36:13,279:INFO:Uploading results into container
2023-05-20 15:36:13,279:INFO:Uploading model into container now
2023-05-20 15:36:13,281:INFO:_master_model_container: 13
2023-05-20 15:36:13,281:INFO:_display_container: 2
2023-05-20 15:36:13,281:INFO:RandomForestRegressor(n_jobs=-1, random_state=8203)
2023-05-20 15:36:13,281:INFO:create_model() successfully completed......................................
2023-05-20 15:36:13,358:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:13,358:INFO:Creating metrics dataframe
2023-05-20 15:36:13,368:INFO:Initializing Extra Trees Regressor
2023-05-20 15:36:13,368:INFO:Total runtime is 0.5272847572962442 minutes
2023-05-20 15:36:13,371:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:13,371:INFO:Initializing create_model()
2023-05-20 15:36:13,372:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:13,372:INFO:Checking exceptions
2023-05-20 15:36:13,372:INFO:Importing libraries
2023-05-20 15:36:13,372:INFO:Copying training dataset
2023-05-20 15:36:13,382:INFO:Defining folds
2023-05-20 15:36:13,382:INFO:Declaring metric variables
2023-05-20 15:36:13,386:INFO:Importing untrained model
2023-05-20 15:36:13,389:INFO:Extra Trees Regressor Imported successfully
2023-05-20 15:36:13,395:INFO:Starting cross validation
2023-05-20 15:36:13,397:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:15,672:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.19s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:36:17,862:INFO:Calculating mean and std
2023-05-20 15:36:17,863:INFO:Creating metrics dataframe
2023-05-20 15:36:18,033:INFO:Uploading results into container
2023-05-20 15:36:18,033:INFO:Uploading model into container now
2023-05-20 15:36:18,034:INFO:_master_model_container: 14
2023-05-20 15:36:18,034:INFO:_display_container: 2
2023-05-20 15:36:18,034:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8203)
2023-05-20 15:36:18,034:INFO:create_model() successfully completed......................................
2023-05-20 15:36:18,095:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:18,095:INFO:Creating metrics dataframe
2023-05-20 15:36:18,107:INFO:Initializing AdaBoost Regressor
2023-05-20 15:36:18,107:INFO:Total runtime is 0.6062649051348368 minutes
2023-05-20 15:36:18,110:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:18,110:INFO:Initializing create_model()
2023-05-20 15:36:18,110:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:18,111:INFO:Checking exceptions
2023-05-20 15:36:18,111:INFO:Importing libraries
2023-05-20 15:36:18,111:INFO:Copying training dataset
2023-05-20 15:36:18,118:INFO:Defining folds
2023-05-20 15:36:18,118:INFO:Declaring metric variables
2023-05-20 15:36:18,121:INFO:Importing untrained model
2023-05-20 15:36:18,123:INFO:AdaBoost Regressor Imported successfully
2023-05-20 15:36:18,128:INFO:Starting cross validation
2023-05-20 15:36:18,131:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:20,988:INFO:Calculating mean and std
2023-05-20 15:36:20,989:INFO:Creating metrics dataframe
2023-05-20 15:36:21,164:INFO:Uploading results into container
2023-05-20 15:36:21,165:INFO:Uploading model into container now
2023-05-20 15:36:21,165:INFO:_master_model_container: 15
2023-05-20 15:36:21,165:INFO:_display_container: 2
2023-05-20 15:36:21,165:INFO:AdaBoostRegressor(random_state=8203)
2023-05-20 15:36:21,165:INFO:create_model() successfully completed......................................
2023-05-20 15:36:21,228:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:21,228:INFO:Creating metrics dataframe
2023-05-20 15:36:21,239:INFO:Initializing Gradient Boosting Regressor
2023-05-20 15:36:21,239:INFO:Total runtime is 0.6584732135136921 minutes
2023-05-20 15:36:21,243:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:21,243:INFO:Initializing create_model()
2023-05-20 15:36:21,243:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:21,244:INFO:Checking exceptions
2023-05-20 15:36:21,244:INFO:Importing libraries
2023-05-20 15:36:21,244:INFO:Copying training dataset
2023-05-20 15:36:21,253:INFO:Defining folds
2023-05-20 15:36:21,253:INFO:Declaring metric variables
2023-05-20 15:36:21,255:INFO:Importing untrained model
2023-05-20 15:36:21,258:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 15:36:21,265:INFO:Starting cross validation
2023-05-20 15:36:21,267:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:24,267:INFO:Calculating mean and std
2023-05-20 15:36:24,269:INFO:Creating metrics dataframe
2023-05-20 15:36:24,453:INFO:Uploading results into container
2023-05-20 15:36:24,454:INFO:Uploading model into container now
2023-05-20 15:36:24,454:INFO:_master_model_container: 16
2023-05-20 15:36:24,454:INFO:_display_container: 2
2023-05-20 15:36:24,455:INFO:GradientBoostingRegressor(random_state=8203)
2023-05-20 15:36:24,455:INFO:create_model() successfully completed......................................
2023-05-20 15:36:24,516:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:24,516:INFO:Creating metrics dataframe
2023-05-20 15:36:24,528:INFO:Initializing Extreme Gradient Boosting
2023-05-20 15:36:24,528:INFO:Total runtime is 0.7132827321688333 minutes
2023-05-20 15:36:24,530:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:24,531:INFO:Initializing create_model()
2023-05-20 15:36:24,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:24,531:INFO:Checking exceptions
2023-05-20 15:36:24,531:INFO:Importing libraries
2023-05-20 15:36:24,531:INFO:Copying training dataset
2023-05-20 15:36:24,538:INFO:Defining folds
2023-05-20 15:36:24,538:INFO:Declaring metric variables
2023-05-20 15:36:24,541:INFO:Importing untrained model
2023-05-20 15:36:24,547:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 15:36:24,556:INFO:Starting cross validation
2023-05-20 15:36:24,560:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:28,212:INFO:Calculating mean and std
2023-05-20 15:36:28,214:INFO:Creating metrics dataframe
2023-05-20 15:36:28,403:INFO:Uploading results into container
2023-05-20 15:36:28,404:INFO:Uploading model into container now
2023-05-20 15:36:28,405:INFO:_master_model_container: 17
2023-05-20 15:36:28,405:INFO:_display_container: 2
2023-05-20 15:36:28,406:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=8203, ...)
2023-05-20 15:36:28,406:INFO:create_model() successfully completed......................................
2023-05-20 15:36:28,467:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:28,467:INFO:Creating metrics dataframe
2023-05-20 15:36:28,477:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 15:36:28,478:INFO:Total runtime is 0.7791180332501728 minutes
2023-05-20 15:36:28,480:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:28,480:INFO:Initializing create_model()
2023-05-20 15:36:28,481:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:28,481:INFO:Checking exceptions
2023-05-20 15:36:28,481:INFO:Importing libraries
2023-05-20 15:36:28,481:INFO:Copying training dataset
2023-05-20 15:36:28,490:INFO:Defining folds
2023-05-20 15:36:28,490:INFO:Declaring metric variables
2023-05-20 15:36:28,492:INFO:Importing untrained model
2023-05-20 15:36:28,495:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 15:36:28,500:INFO:Starting cross validation
2023-05-20 15:36:28,502:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:36:31,011:INFO:Calculating mean and std
2023-05-20 15:36:31,013:INFO:Creating metrics dataframe
2023-05-20 15:36:31,217:INFO:Uploading results into container
2023-05-20 15:36:31,218:INFO:Uploading model into container now
2023-05-20 15:36:31,219:INFO:_master_model_container: 18
2023-05-20 15:36:31,219:INFO:_display_container: 2
2023-05-20 15:36:31,219:INFO:LGBMRegressor(random_state=8203)
2023-05-20 15:36:31,220:INFO:create_model() successfully completed......................................
2023-05-20 15:36:31,284:INFO:SubProcess create_model() end ==================================
2023-05-20 15:36:31,284:INFO:Creating metrics dataframe
2023-05-20 15:36:31,296:INFO:Initializing CatBoost Regressor
2023-05-20 15:36:31,296:INFO:Total runtime is 0.8260944366455077 minutes
2023-05-20 15:36:31,298:INFO:SubProcess create_model() called ==================================
2023-05-20 15:36:31,300:INFO:Initializing create_model()
2023-05-20 15:36:31,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:36:31,300:INFO:Checking exceptions
2023-05-20 15:36:31,300:INFO:Importing libraries
2023-05-20 15:36:31,300:INFO:Copying training dataset
2023-05-20 15:36:31,310:INFO:Defining folds
2023-05-20 15:36:31,310:INFO:Declaring metric variables
2023-05-20 15:36:31,313:INFO:Importing untrained model
2023-05-20 15:36:31,315:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:36:31,321:INFO:Starting cross validation
2023-05-20 15:36:31,322:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:37:00,456:INFO:Calculating mean and std
2023-05-20 15:37:00,458:INFO:Creating metrics dataframe
2023-05-20 15:37:00,666:INFO:Uploading results into container
2023-05-20 15:37:00,667:INFO:Uploading model into container now
2023-05-20 15:37:00,667:INFO:_master_model_container: 19
2023-05-20 15:37:00,668:INFO:_display_container: 2
2023-05-20 15:37:00,668:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF4869B10>
2023-05-20 15:37:00,668:INFO:create_model() successfully completed......................................
2023-05-20 15:37:00,737:INFO:SubProcess create_model() end ==================================
2023-05-20 15:37:00,737:INFO:Creating metrics dataframe
2023-05-20 15:37:00,748:INFO:Initializing Dummy Regressor
2023-05-20 15:37:00,748:INFO:Total runtime is 1.3169641892115274 minutes
2023-05-20 15:37:00,753:INFO:SubProcess create_model() called ==================================
2023-05-20 15:37:00,754:INFO:Initializing create_model()
2023-05-20 15:37:00,754:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD990DB40>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:37:00,754:INFO:Checking exceptions
2023-05-20 15:37:00,754:INFO:Importing libraries
2023-05-20 15:37:00,754:INFO:Copying training dataset
2023-05-20 15:37:00,762:INFO:Defining folds
2023-05-20 15:37:00,762:INFO:Declaring metric variables
2023-05-20 15:37:00,765:INFO:Importing untrained model
2023-05-20 15:37:00,769:INFO:Dummy Regressor Imported successfully
2023-05-20 15:37:00,776:INFO:Starting cross validation
2023-05-20 15:37:00,778:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:37:02,901:INFO:Calculating mean and std
2023-05-20 15:37:02,903:INFO:Creating metrics dataframe
2023-05-20 15:37:03,143:INFO:Uploading results into container
2023-05-20 15:37:03,143:INFO:Uploading model into container now
2023-05-20 15:37:03,144:INFO:_master_model_container: 20
2023-05-20 15:37:03,144:INFO:_display_container: 2
2023-05-20 15:37:03,144:INFO:DummyRegressor()
2023-05-20 15:37:03,144:INFO:create_model() successfully completed......................................
2023-05-20 15:37:03,238:INFO:SubProcess create_model() end ==================================
2023-05-20 15:37:03,238:INFO:Creating metrics dataframe
2023-05-20 15:37:03,266:INFO:Initializing create_model()
2023-05-20 15:37:03,267:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF4920160>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BF4869B10>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:37:03,267:INFO:Checking exceptions
2023-05-20 15:37:03,269:INFO:Importing libraries
2023-05-20 15:37:03,269:INFO:Copying training dataset
2023-05-20 15:37:03,280:INFO:Defining folds
2023-05-20 15:37:03,280:INFO:Declaring metric variables
2023-05-20 15:37:03,281:INFO:Importing untrained model
2023-05-20 15:37:03,281:INFO:Declaring custom model
2023-05-20 15:37:03,281:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:37:03,283:INFO:Cross validation set to False
2023-05-20 15:37:03,283:INFO:Fitting Model
2023-05-20 15:37:07,287:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF48A5EA0>
2023-05-20 15:37:07,287:INFO:create_model() successfully completed......................................
2023-05-20 15:37:07,388:INFO:_master_model_container: 20
2023-05-20 15:37:07,388:INFO:_display_container: 2
2023-05-20 15:37:07,388:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF48A5EA0>
2023-05-20 15:37:07,388:INFO:compare_models() successfully completed......................................
2023-05-20 15:51:35,040:INFO:PyCaret RegressionExperiment
2023-05-20 15:51:35,041:INFO:Logging name: reg-default-name
2023-05-20 15:51:35,041:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 15:51:35,041:INFO:version 3.0.2
2023-05-20 15:51:35,041:INFO:Initializing setup()
2023-05-20 15:51:35,041:INFO:self.USI: 151e
2023-05-20 15:51:35,041:INFO:self._variable_keys: {'exp_id', 'logging_param', 'fold_groups_param', 'data', 'gpu_param', 'seed', 'memory', 'n_jobs_param', 'idx', 'X', 'html_param', 'transform_target_param', 'USI', '_available_plots', 'y_train', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', 'fold_shuffle_param', 'y', 'pipeline', 'X_train', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase', 'y_test'}
2023-05-20 15:51:35,041:INFO:Checking environment
2023-05-20 15:51:35,041:INFO:python_version: 3.10.3
2023-05-20 15:51:35,041:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 15:51:35,041:INFO:machine: AMD64
2023-05-20 15:51:35,041:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 15:51:35,041:INFO:Memory: svmem(total=17083187200, available=7154032640, percent=58.1, used=9929154560, free=7154032640)
2023-05-20 15:51:35,041:INFO:Physical Core: 6
2023-05-20 15:51:35,041:INFO:Logical Core: 12
2023-05-20 15:51:35,041:INFO:Checking libraries
2023-05-20 15:51:35,041:INFO:System:
2023-05-20 15:51:35,041:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 15:51:35,041:INFO:executable: c:\Python310\python.exe
2023-05-20 15:51:35,041:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 15:51:35,041:INFO:PyCaret required dependencies:
2023-05-20 15:51:35,041:INFO:                 pip: 23.1.2
2023-05-20 15:51:35,041:INFO:          setuptools: 58.1.0
2023-05-20 15:51:35,042:INFO:             pycaret: 3.0.2
2023-05-20 15:51:35,042:INFO:             IPython: 8.5.0
2023-05-20 15:51:35,042:INFO:          ipywidgets: 8.0.6
2023-05-20 15:51:35,042:INFO:                tqdm: 4.65.0
2023-05-20 15:51:35,042:INFO:               numpy: 1.23.2
2023-05-20 15:51:35,042:INFO:              pandas: 1.5.2
2023-05-20 15:51:35,042:INFO:              jinja2: 3.1.2
2023-05-20 15:51:35,042:INFO:               scipy: 1.9.3
2023-05-20 15:51:35,042:INFO:              joblib: 1.2.0
2023-05-20 15:51:35,042:INFO:             sklearn: 1.1.3
2023-05-20 15:51:35,042:INFO:                pyod: 1.0.9
2023-05-20 15:51:35,042:INFO:            imblearn: 0.10.1
2023-05-20 15:51:35,042:INFO:   category_encoders: 2.6.1
2023-05-20 15:51:35,042:INFO:            lightgbm: 3.3.5
2023-05-20 15:51:35,042:INFO:               numba: 0.57.0
2023-05-20 15:51:35,042:INFO:            requests: 2.28.2
2023-05-20 15:51:35,042:INFO:          matplotlib: 3.5.3
2023-05-20 15:51:35,042:INFO:          scikitplot: 0.3.7
2023-05-20 15:51:35,042:INFO:         yellowbrick: 1.5
2023-05-20 15:51:35,042:INFO:              plotly: 5.13.1
2023-05-20 15:51:35,042:INFO:             kaleido: 0.2.1
2023-05-20 15:51:35,042:INFO:         statsmodels: 0.13.5
2023-05-20 15:51:35,042:INFO:              sktime: 0.17.0
2023-05-20 15:51:35,042:INFO:               tbats: 1.1.3
2023-05-20 15:51:35,042:INFO:            pmdarima: 2.0.3
2023-05-20 15:51:35,042:INFO:              psutil: 5.9.2
2023-05-20 15:51:35,042:INFO:PyCaret optional dependencies:
2023-05-20 15:51:35,043:INFO:                shap: Not installed
2023-05-20 15:51:35,043:INFO:           interpret: Not installed
2023-05-20 15:51:35,043:INFO:                umap: Not installed
2023-05-20 15:51:35,043:INFO:    pandas_profiling: Not installed
2023-05-20 15:51:35,043:INFO:  explainerdashboard: Not installed
2023-05-20 15:51:35,043:INFO:             autoviz: Not installed
2023-05-20 15:51:35,043:INFO:           fairlearn: Not installed
2023-05-20 15:51:35,043:INFO:             xgboost: 1.7.4
2023-05-20 15:51:35,043:INFO:            catboost: 1.1.1
2023-05-20 15:51:35,043:INFO:              kmodes: Not installed
2023-05-20 15:51:35,043:INFO:             mlxtend: Not installed
2023-05-20 15:51:35,043:INFO:       statsforecast: Not installed
2023-05-20 15:51:35,043:INFO:        tune_sklearn: Not installed
2023-05-20 15:51:35,043:INFO:                 ray: Not installed
2023-05-20 15:51:35,043:INFO:            hyperopt: Not installed
2023-05-20 15:51:35,043:INFO:              optuna: Not installed
2023-05-20 15:51:35,043:INFO:               skopt: Not installed
2023-05-20 15:51:35,043:INFO:              mlflow: Not installed
2023-05-20 15:51:35,043:INFO:              gradio: Not installed
2023-05-20 15:51:35,043:INFO:             fastapi: Not installed
2023-05-20 15:51:35,043:INFO:             uvicorn: Not installed
2023-05-20 15:51:35,043:INFO:              m2cgen: Not installed
2023-05-20 15:51:35,043:INFO:           evidently: Not installed
2023-05-20 15:51:35,043:INFO:               fugue: Not installed
2023-05-20 15:51:35,043:INFO:           streamlit: Not installed
2023-05-20 15:51:35,043:INFO:             prophet: Not installed
2023-05-20 15:51:35,044:INFO:None
2023-05-20 15:51:35,044:INFO:Set up data.
2023-05-20 15:51:35,122:INFO:Set up train/test split.
2023-05-20 15:51:35,131:INFO:Set up index.
2023-05-20 15:51:35,131:INFO:Set up folding strategy.
2023-05-20 15:51:35,131:INFO:Assigning column types.
2023-05-20 15:51:35,139:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 15:51:35,139:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,143:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,148:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,203:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,243:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,244:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,246:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,247:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,253:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,257:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,314:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,352:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,352:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,356:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,357:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 15:51:35,361:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,365:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,422:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,464:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,464:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,467:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,472:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,476:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,538:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,580:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,580:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,583:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,583:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 15:51:35,592:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,652:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,695:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,698:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,707:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,762:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,803:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,803:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,806:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,807:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 15:51:35,876:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,918:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:35,918:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:35,921:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:35,989:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:36,051:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 15:51:36,052:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,055:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,057:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 15:51:36,128:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:36,169:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,176:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,244:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 15:51:36,286:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,289:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,290:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 15:51:36,397:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,399:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,505:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,507:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,508:INFO:Preparing preprocessing pipeline...
2023-05-20 15:51:36,508:INFO:Set up simple imputation.
2023-05-20 15:51:36,511:INFO:Set up column name cleaning.
2023-05-20 15:51:36,553:INFO:Finished creating preprocessing pipeline.
2023-05-20 15:51:36,559:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 15:51:36,560:INFO:Creating final display dataframe.
2023-05-20 15:51:36,728:INFO:Setup _display_container:                     Description             Value
0                    Session id              6177
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              151e
2023-05-20 15:51:36,848:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,850:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,958:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 15:51:36,961:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 15:51:36,961:INFO:setup() successfully completed in 2.13s...............
2023-05-20 15:53:23,954:INFO:Initializing compare_models()
2023-05-20 15:53:23,954:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 15:53:23,955:INFO:Checking exceptions
2023-05-20 15:53:23,961:INFO:Preparing display monitor
2023-05-20 15:53:24,009:INFO:Initializing Linear Regression
2023-05-20 15:53:24,010:INFO:Total runtime is 1.6629695892333984e-05 minutes
2023-05-20 15:53:24,013:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:24,014:INFO:Initializing create_model()
2023-05-20 15:53:24,014:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:24,014:INFO:Checking exceptions
2023-05-20 15:53:24,014:INFO:Importing libraries
2023-05-20 15:53:24,014:INFO:Copying training dataset
2023-05-20 15:53:24,024:INFO:Defining folds
2023-05-20 15:53:24,024:INFO:Declaring metric variables
2023-05-20 15:53:24,027:INFO:Importing untrained model
2023-05-20 15:53:24,032:INFO:Linear Regression Imported successfully
2023-05-20 15:53:24,038:INFO:Starting cross validation
2023-05-20 15:53:24,040:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:31,389:INFO:Calculating mean and std
2023-05-20 15:53:31,391:INFO:Creating metrics dataframe
2023-05-20 15:53:31,645:INFO:Uploading results into container
2023-05-20 15:53:31,647:INFO:Uploading model into container now
2023-05-20 15:53:31,647:INFO:_master_model_container: 1
2023-05-20 15:53:31,648:INFO:_display_container: 2
2023-05-20 15:53:31,648:INFO:LinearRegression(n_jobs=-1)
2023-05-20 15:53:31,648:INFO:create_model() successfully completed......................................
2023-05-20 15:53:31,774:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:31,774:INFO:Creating metrics dataframe
2023-05-20 15:53:31,783:INFO:Initializing Lasso Regression
2023-05-20 15:53:31,784:INFO:Total runtime is 0.12958503166834515 minutes
2023-05-20 15:53:31,788:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:31,789:INFO:Initializing create_model()
2023-05-20 15:53:31,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:31,789:INFO:Checking exceptions
2023-05-20 15:53:31,789:INFO:Importing libraries
2023-05-20 15:53:31,789:INFO:Copying training dataset
2023-05-20 15:53:31,804:INFO:Defining folds
2023-05-20 15:53:31,804:INFO:Declaring metric variables
2023-05-20 15:53:31,809:INFO:Importing untrained model
2023-05-20 15:53:31,816:INFO:Lasso Regression Imported successfully
2023-05-20 15:53:31,825:INFO:Starting cross validation
2023-05-20 15:53:31,827:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:35,197:INFO:Calculating mean and std
2023-05-20 15:53:35,199:INFO:Creating metrics dataframe
2023-05-20 15:53:35,470:INFO:Uploading results into container
2023-05-20 15:53:35,471:INFO:Uploading model into container now
2023-05-20 15:53:35,471:INFO:_master_model_container: 2
2023-05-20 15:53:35,472:INFO:_display_container: 2
2023-05-20 15:53:35,472:INFO:Lasso(random_state=6177)
2023-05-20 15:53:35,472:INFO:create_model() successfully completed......................................
2023-05-20 15:53:35,562:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:35,562:INFO:Creating metrics dataframe
2023-05-20 15:53:35,572:INFO:Initializing Ridge Regression
2023-05-20 15:53:35,572:INFO:Total runtime is 0.19271990458170574 minutes
2023-05-20 15:53:35,576:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:35,576:INFO:Initializing create_model()
2023-05-20 15:53:35,576:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:35,576:INFO:Checking exceptions
2023-05-20 15:53:35,576:INFO:Importing libraries
2023-05-20 15:53:35,576:INFO:Copying training dataset
2023-05-20 15:53:35,591:INFO:Defining folds
2023-05-20 15:53:35,591:INFO:Declaring metric variables
2023-05-20 15:53:35,594:INFO:Importing untrained model
2023-05-20 15:53:35,599:INFO:Ridge Regression Imported successfully
2023-05-20 15:53:35,606:INFO:Starting cross validation
2023-05-20 15:53:35,608:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:38,075:INFO:Calculating mean and std
2023-05-20 15:53:38,077:INFO:Creating metrics dataframe
2023-05-20 15:53:38,317:INFO:Uploading results into container
2023-05-20 15:53:38,318:INFO:Uploading model into container now
2023-05-20 15:53:38,318:INFO:_master_model_container: 3
2023-05-20 15:53:38,318:INFO:_display_container: 2
2023-05-20 15:53:38,318:INFO:Ridge(random_state=6177)
2023-05-20 15:53:38,319:INFO:create_model() successfully completed......................................
2023-05-20 15:53:38,396:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:38,396:INFO:Creating metrics dataframe
2023-05-20 15:53:38,405:INFO:Initializing Elastic Net
2023-05-20 15:53:38,405:INFO:Total runtime is 0.23993143637975056 minutes
2023-05-20 15:53:38,409:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:38,409:INFO:Initializing create_model()
2023-05-20 15:53:38,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:38,409:INFO:Checking exceptions
2023-05-20 15:53:38,409:INFO:Importing libraries
2023-05-20 15:53:38,409:INFO:Copying training dataset
2023-05-20 15:53:38,421:INFO:Defining folds
2023-05-20 15:53:38,421:INFO:Declaring metric variables
2023-05-20 15:53:38,424:INFO:Importing untrained model
2023-05-20 15:53:38,428:INFO:Elastic Net Imported successfully
2023-05-20 15:53:38,438:INFO:Starting cross validation
2023-05-20 15:53:38,440:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:40,733:INFO:Calculating mean and std
2023-05-20 15:53:40,735:INFO:Creating metrics dataframe
2023-05-20 15:53:40,951:INFO:Uploading results into container
2023-05-20 15:53:40,952:INFO:Uploading model into container now
2023-05-20 15:53:40,953:INFO:_master_model_container: 4
2023-05-20 15:53:40,953:INFO:_display_container: 2
2023-05-20 15:53:40,954:INFO:ElasticNet(random_state=6177)
2023-05-20 15:53:40,954:INFO:create_model() successfully completed......................................
2023-05-20 15:53:41,063:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:41,063:INFO:Creating metrics dataframe
2023-05-20 15:53:41,075:INFO:Initializing Least Angle Regression
2023-05-20 15:53:41,075:INFO:Total runtime is 0.2844295620918274 minutes
2023-05-20 15:53:41,079:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:41,080:INFO:Initializing create_model()
2023-05-20 15:53:41,080:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:41,080:INFO:Checking exceptions
2023-05-20 15:53:41,080:INFO:Importing libraries
2023-05-20 15:53:41,080:INFO:Copying training dataset
2023-05-20 15:53:41,094:INFO:Defining folds
2023-05-20 15:53:41,094:INFO:Declaring metric variables
2023-05-20 15:53:41,100:INFO:Importing untrained model
2023-05-20 15:53:41,109:INFO:Least Angle Regression Imported successfully
2023-05-20 15:53:41,122:INFO:Starting cross validation
2023-05-20 15:53:41,124:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:41,287:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,309:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,314:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.200e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,315:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,316:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.117e-03, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,318:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=9.785e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,324:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,327:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=5.633e-04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.854e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=4.753e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,342:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,344:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.162e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,348:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=7.212e-04, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,353:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.107e-04, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,357:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=5.293e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,362:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=4.991e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,364:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.113e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,366:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=4.840e-04, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,366:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.331e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,368:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,374:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=4.529e-04, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,376:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,384:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.803e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,390:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=4.316e-04, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,393:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=2.216e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,397:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,399:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=1.812e-04, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,399:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:41,423:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.252e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,424:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.220e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,426:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=4.056e-04, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,426:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:53:41,426:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.466e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,427:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:53:41,427:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:53:41,427:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:53:41,427:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:53:41,431:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 194 iterations, i.e. alpha=1.988e+02, with an active set of 148 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,433:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=7.759e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,433:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=8.727e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,435:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=4.591e-04, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,444:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.811e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,490:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 15:53:41,490:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 15:53:41,490:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 15:53:41,493:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=1.707e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,500:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 116 iterations, i.e. alpha=1.361e-04, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 15:53:41,722:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 15:53:41,722:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 15:53:41,906:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:41,906:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:41,907:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:41,972:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:41,972:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:41,973:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:42,063:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,064:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,065:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:42,081:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,081:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,082:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:42,085:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 15:53:42,091:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,091:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 15:53:42,092:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,092:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:42,104:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,106:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 15:53:42,107:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 15:53:44,434:INFO:Calculating mean and std
2023-05-20 15:53:44,435:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 15:53:44,437:INFO:Creating metrics dataframe
2023-05-20 15:53:44,691:INFO:Uploading results into container
2023-05-20 15:53:44,692:INFO:Uploading model into container now
2023-05-20 15:53:44,692:INFO:_master_model_container: 5
2023-05-20 15:53:44,693:INFO:_display_container: 2
2023-05-20 15:53:44,693:INFO:Lars(random_state=6177)
2023-05-20 15:53:44,693:INFO:create_model() successfully completed......................................
2023-05-20 15:53:44,783:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:44,784:INFO:Creating metrics dataframe
2023-05-20 15:53:44,795:INFO:Initializing Lasso Least Angle Regression
2023-05-20 15:53:44,796:INFO:Total runtime is 0.3464459498723348 minutes
2023-05-20 15:53:44,800:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:44,800:INFO:Initializing create_model()
2023-05-20 15:53:44,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:44,801:INFO:Checking exceptions
2023-05-20 15:53:44,801:INFO:Importing libraries
2023-05-20 15:53:44,801:INFO:Copying training dataset
2023-05-20 15:53:44,822:INFO:Defining folds
2023-05-20 15:53:44,822:INFO:Declaring metric variables
2023-05-20 15:53:44,827:INFO:Importing untrained model
2023-05-20 15:53:44,832:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 15:53:44,840:INFO:Starting cross validation
2023-05-20 15:53:44,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:44,991:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:44,993:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:44,999:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,017:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,048:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,091:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,092:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,102:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,124:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:45,131:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 15:53:47,389:INFO:Calculating mean and std
2023-05-20 15:53:47,391:INFO:Creating metrics dataframe
2023-05-20 15:53:47,637:INFO:Uploading results into container
2023-05-20 15:53:47,638:INFO:Uploading model into container now
2023-05-20 15:53:47,639:INFO:_master_model_container: 6
2023-05-20 15:53:47,639:INFO:_display_container: 2
2023-05-20 15:53:47,639:INFO:LassoLars(random_state=6177)
2023-05-20 15:53:47,639:INFO:create_model() successfully completed......................................
2023-05-20 15:53:47,787:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:47,787:INFO:Creating metrics dataframe
2023-05-20 15:53:47,807:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 15:53:47,807:INFO:Total runtime is 0.39662813345591225 minutes
2023-05-20 15:53:47,811:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:47,811:INFO:Initializing create_model()
2023-05-20 15:53:47,811:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:47,811:INFO:Checking exceptions
2023-05-20 15:53:47,811:INFO:Importing libraries
2023-05-20 15:53:47,811:INFO:Copying training dataset
2023-05-20 15:53:47,828:INFO:Defining folds
2023-05-20 15:53:47,828:INFO:Declaring metric variables
2023-05-20 15:53:47,833:INFO:Importing untrained model
2023-05-20 15:53:47,837:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 15:53:47,847:INFO:Starting cross validation
2023-05-20 15:53:47,849:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:48,017:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,018:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,036:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,036:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,040:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,041:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,074:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,102:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:48,134:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 15:53:50,349:INFO:Calculating mean and std
2023-05-20 15:53:50,351:INFO:Creating metrics dataframe
2023-05-20 15:53:50,587:INFO:Uploading results into container
2023-05-20 15:53:50,588:INFO:Uploading model into container now
2023-05-20 15:53:50,589:INFO:_master_model_container: 7
2023-05-20 15:53:50,589:INFO:_display_container: 2
2023-05-20 15:53:50,589:INFO:OrthogonalMatchingPursuit()
2023-05-20 15:53:50,589:INFO:create_model() successfully completed......................................
2023-05-20 15:53:50,656:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:50,657:INFO:Creating metrics dataframe
2023-05-20 15:53:50,667:INFO:Initializing Bayesian Ridge
2023-05-20 15:53:50,667:INFO:Total runtime is 0.4442939718564351 minutes
2023-05-20 15:53:50,670:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:50,670:INFO:Initializing create_model()
2023-05-20 15:53:50,670:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:50,670:INFO:Checking exceptions
2023-05-20 15:53:50,670:INFO:Importing libraries
2023-05-20 15:53:50,670:INFO:Copying training dataset
2023-05-20 15:53:50,678:INFO:Defining folds
2023-05-20 15:53:50,679:INFO:Declaring metric variables
2023-05-20 15:53:50,681:INFO:Importing untrained model
2023-05-20 15:53:50,684:INFO:Bayesian Ridge Imported successfully
2023-05-20 15:53:50,690:INFO:Starting cross validation
2023-05-20 15:53:50,692:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:52,925:INFO:Calculating mean and std
2023-05-20 15:53:52,927:INFO:Creating metrics dataframe
2023-05-20 15:53:53,143:INFO:Uploading results into container
2023-05-20 15:53:53,144:INFO:Uploading model into container now
2023-05-20 15:53:53,145:INFO:_master_model_container: 8
2023-05-20 15:53:53,145:INFO:_display_container: 2
2023-05-20 15:53:53,146:INFO:BayesianRidge()
2023-05-20 15:53:53,146:INFO:create_model() successfully completed......................................
2023-05-20 15:53:53,218:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:53,218:INFO:Creating metrics dataframe
2023-05-20 15:53:53,230:INFO:Initializing Passive Aggressive Regressor
2023-05-20 15:53:53,230:INFO:Total runtime is 0.4870132446289062 minutes
2023-05-20 15:53:53,233:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:53,233:INFO:Initializing create_model()
2023-05-20 15:53:53,234:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:53,234:INFO:Checking exceptions
2023-05-20 15:53:53,234:INFO:Importing libraries
2023-05-20 15:53:53,234:INFO:Copying training dataset
2023-05-20 15:53:53,244:INFO:Defining folds
2023-05-20 15:53:53,244:INFO:Declaring metric variables
2023-05-20 15:53:53,248:INFO:Importing untrained model
2023-05-20 15:53:53,251:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 15:53:53,257:INFO:Starting cross validation
2023-05-20 15:53:53,259:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:55,596:INFO:Calculating mean and std
2023-05-20 15:53:55,597:INFO:Creating metrics dataframe
2023-05-20 15:53:55,816:INFO:Uploading results into container
2023-05-20 15:53:55,817:INFO:Uploading model into container now
2023-05-20 15:53:55,817:INFO:_master_model_container: 9
2023-05-20 15:53:55,818:INFO:_display_container: 2
2023-05-20 15:53:55,818:INFO:PassiveAggressiveRegressor(random_state=6177)
2023-05-20 15:53:55,818:INFO:create_model() successfully completed......................................
2023-05-20 15:53:55,883:INFO:SubProcess create_model() end ==================================
2023-05-20 15:53:55,883:INFO:Creating metrics dataframe
2023-05-20 15:53:55,893:INFO:Initializing Huber Regressor
2023-05-20 15:53:55,894:INFO:Total runtime is 0.5314115524291991 minutes
2023-05-20 15:53:55,897:INFO:SubProcess create_model() called ==================================
2023-05-20 15:53:55,897:INFO:Initializing create_model()
2023-05-20 15:53:55,897:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:53:55,897:INFO:Checking exceptions
2023-05-20 15:53:55,897:INFO:Importing libraries
2023-05-20 15:53:55,898:INFO:Copying training dataset
2023-05-20 15:53:55,904:INFO:Defining folds
2023-05-20 15:53:55,904:INFO:Declaring metric variables
2023-05-20 15:53:55,908:INFO:Importing untrained model
2023-05-20 15:53:55,910:INFO:Huber Regressor Imported successfully
2023-05-20 15:53:55,917:INFO:Starting cross validation
2023-05-20 15:53:55,918:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:53:59,027:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,098:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,165:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,310:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,310:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,322:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,375:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,387:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,401:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:53:59,418:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 15:54:01,495:INFO:Calculating mean and std
2023-05-20 15:54:01,497:INFO:Creating metrics dataframe
2023-05-20 15:54:01,717:INFO:Uploading results into container
2023-05-20 15:54:01,718:INFO:Uploading model into container now
2023-05-20 15:54:01,718:INFO:_master_model_container: 10
2023-05-20 15:54:01,718:INFO:_display_container: 2
2023-05-20 15:54:01,719:INFO:HuberRegressor()
2023-05-20 15:54:01,719:INFO:create_model() successfully completed......................................
2023-05-20 15:54:01,794:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:01,794:INFO:Creating metrics dataframe
2023-05-20 15:54:01,805:INFO:Initializing K Neighbors Regressor
2023-05-20 15:54:01,806:INFO:Total runtime is 0.6299481193224588 minutes
2023-05-20 15:54:01,808:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:01,809:INFO:Initializing create_model()
2023-05-20 15:54:01,809:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:01,809:INFO:Checking exceptions
2023-05-20 15:54:01,809:INFO:Importing libraries
2023-05-20 15:54:01,809:INFO:Copying training dataset
2023-05-20 15:54:01,820:INFO:Defining folds
2023-05-20 15:54:01,820:INFO:Declaring metric variables
2023-05-20 15:54:01,824:INFO:Importing untrained model
2023-05-20 15:54:01,827:INFO:K Neighbors Regressor Imported successfully
2023-05-20 15:54:01,832:INFO:Starting cross validation
2023-05-20 15:54:01,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:04,191:INFO:Calculating mean and std
2023-05-20 15:54:04,193:INFO:Creating metrics dataframe
2023-05-20 15:54:04,415:INFO:Uploading results into container
2023-05-20 15:54:04,416:INFO:Uploading model into container now
2023-05-20 15:54:04,416:INFO:_master_model_container: 11
2023-05-20 15:54:04,416:INFO:_display_container: 2
2023-05-20 15:54:04,417:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 15:54:04,417:INFO:create_model() successfully completed......................................
2023-05-20 15:54:04,479:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:04,479:INFO:Creating metrics dataframe
2023-05-20 15:54:04,490:INFO:Initializing Decision Tree Regressor
2023-05-20 15:54:04,490:INFO:Total runtime is 0.6746802846590676 minutes
2023-05-20 15:54:04,492:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:04,493:INFO:Initializing create_model()
2023-05-20 15:54:04,493:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:04,493:INFO:Checking exceptions
2023-05-20 15:54:04,493:INFO:Importing libraries
2023-05-20 15:54:04,493:INFO:Copying training dataset
2023-05-20 15:54:04,503:INFO:Defining folds
2023-05-20 15:54:04,503:INFO:Declaring metric variables
2023-05-20 15:54:04,506:INFO:Importing untrained model
2023-05-20 15:54:04,508:INFO:Decision Tree Regressor Imported successfully
2023-05-20 15:54:04,515:INFO:Starting cross validation
2023-05-20 15:54:04,517:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:06,921:INFO:Calculating mean and std
2023-05-20 15:54:06,924:INFO:Creating metrics dataframe
2023-05-20 15:54:07,173:INFO:Uploading results into container
2023-05-20 15:54:07,174:INFO:Uploading model into container now
2023-05-20 15:54:07,175:INFO:_master_model_container: 12
2023-05-20 15:54:07,175:INFO:_display_container: 2
2023-05-20 15:54:07,175:INFO:DecisionTreeRegressor(random_state=6177)
2023-05-20 15:54:07,175:INFO:create_model() successfully completed......................................
2023-05-20 15:54:07,257:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:07,257:INFO:Creating metrics dataframe
2023-05-20 15:54:07,268:INFO:Initializing Random Forest Regressor
2023-05-20 15:54:07,268:INFO:Total runtime is 0.7209842244784035 minutes
2023-05-20 15:54:07,271:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:07,272:INFO:Initializing create_model()
2023-05-20 15:54:07,272:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:07,272:INFO:Checking exceptions
2023-05-20 15:54:07,272:INFO:Importing libraries
2023-05-20 15:54:07,272:INFO:Copying training dataset
2023-05-20 15:54:07,284:INFO:Defining folds
2023-05-20 15:54:07,284:INFO:Declaring metric variables
2023-05-20 15:54:07,287:INFO:Importing untrained model
2023-05-20 15:54:07,291:INFO:Random Forest Regressor Imported successfully
2023-05-20 15:54:07,296:INFO:Starting cross validation
2023-05-20 15:54:07,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:09,576:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.23s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:54:09,688:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.37s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:54:12,102:INFO:Calculating mean and std
2023-05-20 15:54:12,104:INFO:Creating metrics dataframe
2023-05-20 15:54:12,327:INFO:Uploading results into container
2023-05-20 15:54:12,328:INFO:Uploading model into container now
2023-05-20 15:54:12,329:INFO:_master_model_container: 13
2023-05-20 15:54:12,329:INFO:_display_container: 2
2023-05-20 15:54:12,330:INFO:RandomForestRegressor(n_jobs=-1, random_state=6177)
2023-05-20 15:54:12,330:INFO:create_model() successfully completed......................................
2023-05-20 15:54:12,394:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:12,394:INFO:Creating metrics dataframe
2023-05-20 15:54:12,405:INFO:Initializing Extra Trees Regressor
2023-05-20 15:54:12,405:INFO:Total runtime is 0.8065913597742714 minutes
2023-05-20 15:54:12,408:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:12,408:INFO:Initializing create_model()
2023-05-20 15:54:12,409:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:12,409:INFO:Checking exceptions
2023-05-20 15:54:12,409:INFO:Importing libraries
2023-05-20 15:54:12,409:INFO:Copying training dataset
2023-05-20 15:54:12,417:INFO:Defining folds
2023-05-20 15:54:12,418:INFO:Declaring metric variables
2023-05-20 15:54:12,421:INFO:Importing untrained model
2023-05-20 15:54:12,424:INFO:Extra Trees Regressor Imported successfully
2023-05-20 15:54:12,429:INFO:Starting cross validation
2023-05-20 15:54:12,430:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:14,807:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:54:15,443:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:54:15,508:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.07s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 15:54:17,888:INFO:Calculating mean and std
2023-05-20 15:54:17,890:INFO:Creating metrics dataframe
2023-05-20 15:54:18,144:INFO:Uploading results into container
2023-05-20 15:54:18,145:INFO:Uploading model into container now
2023-05-20 15:54:18,146:INFO:_master_model_container: 14
2023-05-20 15:54:18,146:INFO:_display_container: 2
2023-05-20 15:54:18,147:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6177)
2023-05-20 15:54:18,147:INFO:create_model() successfully completed......................................
2023-05-20 15:54:18,210:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:18,210:INFO:Creating metrics dataframe
2023-05-20 15:54:18,221:INFO:Initializing AdaBoost Regressor
2023-05-20 15:54:18,222:INFO:Total runtime is 0.9035458604494728 minutes
2023-05-20 15:54:18,225:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:18,225:INFO:Initializing create_model()
2023-05-20 15:54:18,225:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:18,226:INFO:Checking exceptions
2023-05-20 15:54:18,226:INFO:Importing libraries
2023-05-20 15:54:18,226:INFO:Copying training dataset
2023-05-20 15:54:18,234:INFO:Defining folds
2023-05-20 15:54:18,234:INFO:Declaring metric variables
2023-05-20 15:54:18,237:INFO:Importing untrained model
2023-05-20 15:54:18,241:INFO:AdaBoost Regressor Imported successfully
2023-05-20 15:54:18,246:INFO:Starting cross validation
2023-05-20 15:54:18,248:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:22,017:INFO:Calculating mean and std
2023-05-20 15:54:22,019:INFO:Creating metrics dataframe
2023-05-20 15:54:22,274:INFO:Uploading results into container
2023-05-20 15:54:22,276:INFO:Uploading model into container now
2023-05-20 15:54:22,276:INFO:_master_model_container: 15
2023-05-20 15:54:22,276:INFO:_display_container: 2
2023-05-20 15:54:22,276:INFO:AdaBoostRegressor(random_state=6177)
2023-05-20 15:54:22,276:INFO:create_model() successfully completed......................................
2023-05-20 15:54:22,348:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:22,348:INFO:Creating metrics dataframe
2023-05-20 15:54:22,360:INFO:Initializing Gradient Boosting Regressor
2023-05-20 15:54:22,361:INFO:Total runtime is 0.9725290497144061 minutes
2023-05-20 15:54:22,365:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:22,365:INFO:Initializing create_model()
2023-05-20 15:54:22,366:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:22,366:INFO:Checking exceptions
2023-05-20 15:54:22,366:INFO:Importing libraries
2023-05-20 15:54:22,366:INFO:Copying training dataset
2023-05-20 15:54:22,376:INFO:Defining folds
2023-05-20 15:54:22,376:INFO:Declaring metric variables
2023-05-20 15:54:22,380:INFO:Importing untrained model
2023-05-20 15:54:22,383:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 15:54:22,390:INFO:Starting cross validation
2023-05-20 15:54:22,391:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:26,093:INFO:Calculating mean and std
2023-05-20 15:54:26,096:INFO:Creating metrics dataframe
2023-05-20 15:54:26,354:INFO:Uploading results into container
2023-05-20 15:54:26,356:INFO:Uploading model into container now
2023-05-20 15:54:26,356:INFO:_master_model_container: 16
2023-05-20 15:54:26,356:INFO:_display_container: 2
2023-05-20 15:54:26,356:INFO:GradientBoostingRegressor(random_state=6177)
2023-05-20 15:54:26,356:INFO:create_model() successfully completed......................................
2023-05-20 15:54:26,422:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:26,422:INFO:Creating metrics dataframe
2023-05-20 15:54:26,434:INFO:Initializing Extreme Gradient Boosting
2023-05-20 15:54:26,434:INFO:Total runtime is 1.0404126842816668 minutes
2023-05-20 15:54:26,438:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:26,439:INFO:Initializing create_model()
2023-05-20 15:54:26,439:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:26,439:INFO:Checking exceptions
2023-05-20 15:54:26,439:INFO:Importing libraries
2023-05-20 15:54:26,439:INFO:Copying training dataset
2023-05-20 15:54:26,449:INFO:Defining folds
2023-05-20 15:54:26,449:INFO:Declaring metric variables
2023-05-20 15:54:26,452:INFO:Importing untrained model
2023-05-20 15:54:26,456:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 15:54:26,467:INFO:Starting cross validation
2023-05-20 15:54:26,469:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:31,524:INFO:Calculating mean and std
2023-05-20 15:54:31,526:INFO:Creating metrics dataframe
2023-05-20 15:54:31,804:INFO:Uploading results into container
2023-05-20 15:54:31,805:INFO:Uploading model into container now
2023-05-20 15:54:31,805:INFO:_master_model_container: 17
2023-05-20 15:54:31,806:INFO:_display_container: 2
2023-05-20 15:54:31,807:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=6177, ...)
2023-05-20 15:54:31,808:INFO:create_model() successfully completed......................................
2023-05-20 15:54:31,876:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:31,877:INFO:Creating metrics dataframe
2023-05-20 15:54:31,889:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 15:54:31,889:INFO:Total runtime is 1.1313371102015175 minutes
2023-05-20 15:54:31,893:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:31,894:INFO:Initializing create_model()
2023-05-20 15:54:31,894:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:31,894:INFO:Checking exceptions
2023-05-20 15:54:31,895:INFO:Importing libraries
2023-05-20 15:54:31,895:INFO:Copying training dataset
2023-05-20 15:54:31,914:INFO:Defining folds
2023-05-20 15:54:31,915:INFO:Declaring metric variables
2023-05-20 15:54:31,918:INFO:Importing untrained model
2023-05-20 15:54:31,923:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 15:54:31,930:INFO:Starting cross validation
2023-05-20 15:54:31,931:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:54:35,350:INFO:Calculating mean and std
2023-05-20 15:54:35,352:INFO:Creating metrics dataframe
2023-05-20 15:54:35,625:INFO:Uploading results into container
2023-05-20 15:54:35,626:INFO:Uploading model into container now
2023-05-20 15:54:35,627:INFO:_master_model_container: 18
2023-05-20 15:54:35,627:INFO:_display_container: 2
2023-05-20 15:54:35,627:INFO:LGBMRegressor(random_state=6177)
2023-05-20 15:54:35,627:INFO:create_model() successfully completed......................................
2023-05-20 15:54:35,703:INFO:SubProcess create_model() end ==================================
2023-05-20 15:54:35,704:INFO:Creating metrics dataframe
2023-05-20 15:54:35,717:INFO:Initializing CatBoost Regressor
2023-05-20 15:54:35,717:INFO:Total runtime is 1.1951329151789345 minutes
2023-05-20 15:54:35,720:INFO:SubProcess create_model() called ==================================
2023-05-20 15:54:35,720:INFO:Initializing create_model()
2023-05-20 15:54:35,721:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:54:35,721:INFO:Checking exceptions
2023-05-20 15:54:35,721:INFO:Importing libraries
2023-05-20 15:54:35,721:INFO:Copying training dataset
2023-05-20 15:54:35,734:INFO:Defining folds
2023-05-20 15:54:35,734:INFO:Declaring metric variables
2023-05-20 15:54:35,739:INFO:Importing untrained model
2023-05-20 15:54:35,743:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:54:35,750:INFO:Starting cross validation
2023-05-20 15:54:35,751:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:55:08,601:INFO:Calculating mean and std
2023-05-20 15:55:08,603:INFO:Creating metrics dataframe
2023-05-20 15:55:08,877:INFO:Uploading results into container
2023-05-20 15:55:08,878:INFO:Uploading model into container now
2023-05-20 15:55:08,879:INFO:_master_model_container: 19
2023-05-20 15:55:08,879:INFO:_display_container: 2
2023-05-20 15:55:08,879:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF4868460>
2023-05-20 15:55:08,879:INFO:create_model() successfully completed......................................
2023-05-20 15:55:08,946:INFO:SubProcess create_model() end ==================================
2023-05-20 15:55:08,946:INFO:Creating metrics dataframe
2023-05-20 15:55:08,957:INFO:Initializing Dummy Regressor
2023-05-20 15:55:08,958:INFO:Total runtime is 1.749147510528564 minutes
2023-05-20 15:55:08,961:INFO:SubProcess create_model() called ==================================
2023-05-20 15:55:08,962:INFO:Initializing create_model()
2023-05-20 15:55:08,962:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BA497DB10>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:55:08,962:INFO:Checking exceptions
2023-05-20 15:55:08,962:INFO:Importing libraries
2023-05-20 15:55:08,962:INFO:Copying training dataset
2023-05-20 15:55:08,970:INFO:Defining folds
2023-05-20 15:55:08,970:INFO:Declaring metric variables
2023-05-20 15:55:08,973:INFO:Importing untrained model
2023-05-20 15:55:08,975:INFO:Dummy Regressor Imported successfully
2023-05-20 15:55:08,981:INFO:Starting cross validation
2023-05-20 15:55:08,983:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 15:55:11,506:INFO:Calculating mean and std
2023-05-20 15:55:11,508:INFO:Creating metrics dataframe
2023-05-20 15:55:11,769:INFO:Uploading results into container
2023-05-20 15:55:11,769:INFO:Uploading model into container now
2023-05-20 15:55:11,770:INFO:_master_model_container: 20
2023-05-20 15:55:11,770:INFO:_display_container: 2
2023-05-20 15:55:11,770:INFO:DummyRegressor()
2023-05-20 15:55:11,771:INFO:create_model() successfully completed......................................
2023-05-20 15:55:11,834:INFO:SubProcess create_model() end ==================================
2023-05-20 15:55:11,836:INFO:Creating metrics dataframe
2023-05-20 15:55:11,855:INFO:Initializing create_model()
2023-05-20 15:55:11,856:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BF6324760>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BF4868460>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 15:55:11,856:INFO:Checking exceptions
2023-05-20 15:55:11,858:INFO:Importing libraries
2023-05-20 15:55:11,858:INFO:Copying training dataset
2023-05-20 15:55:11,867:INFO:Defining folds
2023-05-20 15:55:11,867:INFO:Declaring metric variables
2023-05-20 15:55:11,868:INFO:Importing untrained model
2023-05-20 15:55:11,868:INFO:Declaring custom model
2023-05-20 15:55:11,868:INFO:CatBoost Regressor Imported successfully
2023-05-20 15:55:11,869:INFO:Cross validation set to False
2023-05-20 15:55:11,869:INFO:Fitting Model
2023-05-20 15:55:15,614:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD990E380>
2023-05-20 15:55:15,614:INFO:create_model() successfully completed......................................
2023-05-20 15:55:15,717:INFO:_master_model_container: 20
2023-05-20 15:55:15,717:INFO:_display_container: 2
2023-05-20 15:55:15,718:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD990E380>
2023-05-20 15:55:15,718:INFO:compare_models() successfully completed......................................
2023-05-20 16:18:58,691:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 16:18:58,773:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 16:19:05,264:INFO:PyCaret RegressionExperiment
2023-05-20 16:19:05,264:INFO:Logging name: reg-default-name
2023-05-20 16:19:05,264:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 16:19:05,265:INFO:version 3.0.2
2023-05-20 16:19:05,265:INFO:Initializing setup()
2023-05-20 16:19:05,265:INFO:self.USI: 1fc7
2023-05-20 16:19:05,265:INFO:self._variable_keys: {'exp_id', 'logging_param', 'fold_groups_param', 'data', 'gpu_param', 'seed', 'memory', 'n_jobs_param', 'idx', 'X', 'html_param', 'transform_target_param', 'USI', '_available_plots', 'y_train', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', 'fold_shuffle_param', 'y', 'pipeline', 'X_train', 'gpu_n_jobs_param', 'log_plots_param', '_ml_usecase', 'y_test'}
2023-05-20 16:19:05,265:INFO:Checking environment
2023-05-20 16:19:05,265:INFO:python_version: 3.10.3
2023-05-20 16:19:05,265:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 16:19:05,265:INFO:machine: AMD64
2023-05-20 16:19:05,265:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 16:19:05,265:INFO:Memory: svmem(total=17083187200, available=7334592512, percent=57.1, used=9748594688, free=7334592512)
2023-05-20 16:19:05,265:INFO:Physical Core: 6
2023-05-20 16:19:05,265:INFO:Logical Core: 12
2023-05-20 16:19:05,265:INFO:Checking libraries
2023-05-20 16:19:05,265:INFO:System:
2023-05-20 16:19:05,265:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 16:19:05,265:INFO:executable: c:\Python310\python.exe
2023-05-20 16:19:05,265:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 16:19:05,265:INFO:PyCaret required dependencies:
2023-05-20 16:19:05,265:INFO:                 pip: 23.1.2
2023-05-20 16:19:05,265:INFO:          setuptools: 58.1.0
2023-05-20 16:19:05,265:INFO:             pycaret: 3.0.2
2023-05-20 16:19:05,265:INFO:             IPython: 8.5.0
2023-05-20 16:19:05,265:INFO:          ipywidgets: 8.0.6
2023-05-20 16:19:05,265:INFO:                tqdm: 4.65.0
2023-05-20 16:19:05,266:INFO:               numpy: 1.23.2
2023-05-20 16:19:05,266:INFO:              pandas: 1.5.2
2023-05-20 16:19:05,266:INFO:              jinja2: 3.1.2
2023-05-20 16:19:05,266:INFO:               scipy: 1.9.3
2023-05-20 16:19:05,266:INFO:              joblib: 1.2.0
2023-05-20 16:19:05,266:INFO:             sklearn: 1.1.3
2023-05-20 16:19:05,266:INFO:                pyod: 1.0.9
2023-05-20 16:19:05,266:INFO:            imblearn: 0.10.1
2023-05-20 16:19:05,266:INFO:   category_encoders: 2.6.1
2023-05-20 16:19:05,266:INFO:            lightgbm: 3.3.5
2023-05-20 16:19:05,266:INFO:               numba: 0.57.0
2023-05-20 16:19:05,266:INFO:            requests: 2.28.2
2023-05-20 16:19:05,266:INFO:          matplotlib: 3.5.3
2023-05-20 16:19:05,266:INFO:          scikitplot: 0.3.7
2023-05-20 16:19:05,266:INFO:         yellowbrick: 1.5
2023-05-20 16:19:05,266:INFO:              plotly: 5.13.1
2023-05-20 16:19:05,266:INFO:             kaleido: 0.2.1
2023-05-20 16:19:05,266:INFO:         statsmodels: 0.13.5
2023-05-20 16:19:05,266:INFO:              sktime: 0.17.0
2023-05-20 16:19:05,266:INFO:               tbats: 1.1.3
2023-05-20 16:19:05,266:INFO:            pmdarima: 2.0.3
2023-05-20 16:19:05,266:INFO:              psutil: 5.9.2
2023-05-20 16:19:05,266:INFO:PyCaret optional dependencies:
2023-05-20 16:19:05,266:INFO:                shap: Not installed
2023-05-20 16:19:05,266:INFO:           interpret: Not installed
2023-05-20 16:19:05,266:INFO:                umap: Not installed
2023-05-20 16:19:05,266:INFO:    pandas_profiling: Not installed
2023-05-20 16:19:05,266:INFO:  explainerdashboard: Not installed
2023-05-20 16:19:05,266:INFO:             autoviz: Not installed
2023-05-20 16:19:05,266:INFO:           fairlearn: Not installed
2023-05-20 16:19:05,266:INFO:             xgboost: 1.7.4
2023-05-20 16:19:05,267:INFO:            catboost: 1.1.1
2023-05-20 16:19:05,267:INFO:              kmodes: Not installed
2023-05-20 16:19:05,267:INFO:             mlxtend: Not installed
2023-05-20 16:19:05,267:INFO:       statsforecast: Not installed
2023-05-20 16:19:05,267:INFO:        tune_sklearn: Not installed
2023-05-20 16:19:05,267:INFO:                 ray: Not installed
2023-05-20 16:19:05,267:INFO:            hyperopt: Not installed
2023-05-20 16:19:05,267:INFO:              optuna: Not installed
2023-05-20 16:19:05,267:INFO:               skopt: Not installed
2023-05-20 16:19:05,267:INFO:              mlflow: Not installed
2023-05-20 16:19:05,267:INFO:              gradio: Not installed
2023-05-20 16:19:05,267:INFO:             fastapi: Not installed
2023-05-20 16:19:05,267:INFO:             uvicorn: Not installed
2023-05-20 16:19:05,267:INFO:              m2cgen: Not installed
2023-05-20 16:19:05,267:INFO:           evidently: Not installed
2023-05-20 16:19:05,267:INFO:               fugue: Not installed
2023-05-20 16:19:05,267:INFO:           streamlit: Not installed
2023-05-20 16:19:05,267:INFO:             prophet: Not installed
2023-05-20 16:19:05,267:INFO:None
2023-05-20 16:19:05,267:INFO:Set up data.
2023-05-20 16:19:05,340:INFO:Set up train/test split.
2023-05-20 16:19:05,348:INFO:Set up index.
2023-05-20 16:19:05,348:INFO:Set up folding strategy.
2023-05-20 16:19:05,348:INFO:Assigning column types.
2023-05-20 16:19:05,355:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 16:19:05,355:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,359:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,363:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,415:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,455:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,455:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,457:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,458:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,462:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,466:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,518:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,558:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,559:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,561:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,561:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 16:19:05,565:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,569:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,623:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,661:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,662:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,663:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,668:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,672:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,725:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,766:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,767:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,769:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,769:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 16:19:05,777:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,833:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,873:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,874:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,876:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,885:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,939:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,979:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:05,980:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:05,981:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:05,981:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 16:19:06,043:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,083:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,084:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,086:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,150:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,190:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,191:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,193:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,193:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 16:19:06,255:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,297:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,299:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,365:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 16:19:06,405:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,407:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,410:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 16:19:06,516:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,519:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,629:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,631:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:06,633:INFO:Preparing preprocessing pipeline...
2023-05-20 16:19:06,633:INFO:Set up simple imputation.
2023-05-20 16:19:06,634:INFO:Set up column name cleaning.
2023-05-20 16:19:06,682:INFO:Finished creating preprocessing pipeline.
2023-05-20 16:19:06,688:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 16:19:06,688:INFO:Creating final display dataframe.
2023-05-20 16:19:06,859:INFO:Setup _display_container:                     Description             Value
0                    Session id              5308
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              1fc7
2023-05-20 16:19:06,973:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:06,975:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:07,079:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 16:19:07,082:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 16:19:07,082:INFO:setup() successfully completed in 2.07s...............
2023-05-20 16:19:10,584:INFO:Initializing compare_models()
2023-05-20 16:19:10,584:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:19:10,584:INFO:Checking exceptions
2023-05-20 16:19:10,595:INFO:Preparing display monitor
2023-05-20 16:19:10,644:INFO:Initializing Linear Regression
2023-05-20 16:19:10,644:INFO:Total runtime is 0.0 minutes
2023-05-20 16:19:10,648:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:10,648:INFO:Initializing create_model()
2023-05-20 16:19:10,648:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:10,648:INFO:Checking exceptions
2023-05-20 16:19:10,649:INFO:Importing libraries
2023-05-20 16:19:10,649:INFO:Copying training dataset
2023-05-20 16:19:10,660:INFO:Defining folds
2023-05-20 16:19:10,661:INFO:Declaring metric variables
2023-05-20 16:19:10,664:INFO:Importing untrained model
2023-05-20 16:19:10,670:INFO:Linear Regression Imported successfully
2023-05-20 16:19:10,679:INFO:Starting cross validation
2023-05-20 16:19:10,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:17,370:INFO:Calculating mean and std
2023-05-20 16:19:17,374:INFO:Creating metrics dataframe
2023-05-20 16:19:17,643:INFO:Uploading results into container
2023-05-20 16:19:17,644:INFO:Uploading model into container now
2023-05-20 16:19:17,645:INFO:_master_model_container: 1
2023-05-20 16:19:17,645:INFO:_display_container: 2
2023-05-20 16:19:17,645:INFO:LinearRegression(n_jobs=-1)
2023-05-20 16:19:17,645:INFO:create_model() successfully completed......................................
2023-05-20 16:19:17,727:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:17,727:INFO:Creating metrics dataframe
2023-05-20 16:19:17,735:INFO:Initializing Lasso Regression
2023-05-20 16:19:17,735:INFO:Total runtime is 0.1181812842686971 minutes
2023-05-20 16:19:17,739:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:17,739:INFO:Initializing create_model()
2023-05-20 16:19:17,739:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:17,740:INFO:Checking exceptions
2023-05-20 16:19:17,740:INFO:Importing libraries
2023-05-20 16:19:17,740:INFO:Copying training dataset
2023-05-20 16:19:17,751:INFO:Defining folds
2023-05-20 16:19:17,751:INFO:Declaring metric variables
2023-05-20 16:19:17,754:INFO:Importing untrained model
2023-05-20 16:19:17,757:INFO:Lasso Regression Imported successfully
2023-05-20 16:19:17,763:INFO:Starting cross validation
2023-05-20 16:19:17,765:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:20,516:INFO:Calculating mean and std
2023-05-20 16:19:20,518:INFO:Creating metrics dataframe
2023-05-20 16:19:20,774:INFO:Uploading results into container
2023-05-20 16:19:20,775:INFO:Uploading model into container now
2023-05-20 16:19:20,775:INFO:_master_model_container: 2
2023-05-20 16:19:20,775:INFO:_display_container: 2
2023-05-20 16:19:20,776:INFO:Lasso(random_state=5308)
2023-05-20 16:19:20,776:INFO:create_model() successfully completed......................................
2023-05-20 16:19:20,846:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:20,847:INFO:Creating metrics dataframe
2023-05-20 16:19:20,854:INFO:Initializing Ridge Regression
2023-05-20 16:19:20,854:INFO:Total runtime is 0.17016064723332724 minutes
2023-05-20 16:19:20,857:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:20,858:INFO:Initializing create_model()
2023-05-20 16:19:20,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:20,858:INFO:Checking exceptions
2023-05-20 16:19:20,858:INFO:Importing libraries
2023-05-20 16:19:20,858:INFO:Copying training dataset
2023-05-20 16:19:20,867:INFO:Defining folds
2023-05-20 16:19:20,868:INFO:Declaring metric variables
2023-05-20 16:19:20,871:INFO:Importing untrained model
2023-05-20 16:19:20,873:INFO:Ridge Regression Imported successfully
2023-05-20 16:19:20,881:INFO:Starting cross validation
2023-05-20 16:19:20,882:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:23,458:INFO:Calculating mean and std
2023-05-20 16:19:23,460:INFO:Creating metrics dataframe
2023-05-20 16:19:23,711:INFO:Uploading results into container
2023-05-20 16:19:23,712:INFO:Uploading model into container now
2023-05-20 16:19:23,713:INFO:_master_model_container: 3
2023-05-20 16:19:23,713:INFO:_display_container: 2
2023-05-20 16:19:23,713:INFO:Ridge(random_state=5308)
2023-05-20 16:19:23,713:INFO:create_model() successfully completed......................................
2023-05-20 16:19:23,776:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:23,776:INFO:Creating metrics dataframe
2023-05-20 16:19:23,783:INFO:Initializing Elastic Net
2023-05-20 16:19:23,785:INFO:Total runtime is 0.2190055807431539 minutes
2023-05-20 16:19:23,787:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:23,787:INFO:Initializing create_model()
2023-05-20 16:19:23,787:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:23,787:INFO:Checking exceptions
2023-05-20 16:19:23,787:INFO:Importing libraries
2023-05-20 16:19:23,787:INFO:Copying training dataset
2023-05-20 16:19:23,796:INFO:Defining folds
2023-05-20 16:19:23,796:INFO:Declaring metric variables
2023-05-20 16:19:23,799:INFO:Importing untrained model
2023-05-20 16:19:23,801:INFO:Elastic Net Imported successfully
2023-05-20 16:19:23,806:INFO:Starting cross validation
2023-05-20 16:19:23,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:26,309:INFO:Calculating mean and std
2023-05-20 16:19:26,311:INFO:Creating metrics dataframe
2023-05-20 16:19:26,563:INFO:Uploading results into container
2023-05-20 16:19:26,563:INFO:Uploading model into container now
2023-05-20 16:19:26,564:INFO:_master_model_container: 4
2023-05-20 16:19:26,564:INFO:_display_container: 2
2023-05-20 16:19:26,564:INFO:ElasticNet(random_state=5308)
2023-05-20 16:19:26,565:INFO:create_model() successfully completed......................................
2023-05-20 16:19:26,626:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:26,626:INFO:Creating metrics dataframe
2023-05-20 16:19:26,635:INFO:Initializing Least Angle Regression
2023-05-20 16:19:26,636:INFO:Total runtime is 0.2665338079134623 minutes
2023-05-20 16:19:26,638:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:26,639:INFO:Initializing create_model()
2023-05-20 16:19:26,639:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:26,639:INFO:Checking exceptions
2023-05-20 16:19:26,639:INFO:Importing libraries
2023-05-20 16:19:26,639:INFO:Copying training dataset
2023-05-20 16:19:26,648:INFO:Defining folds
2023-05-20 16:19:26,648:INFO:Declaring metric variables
2023-05-20 16:19:26,650:INFO:Importing untrained model
2023-05-20 16:19:26,653:INFO:Least Angle Regression Imported successfully
2023-05-20 16:19:26,658:INFO:Starting cross validation
2023-05-20 16:19:26,659:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:26,763:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,766:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,784:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,791:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,794:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.163e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,796:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=9.288e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,797:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=8.438e-04, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,798:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.956e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=4.401e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,810:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.506e-04, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,813:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=2.576e-04, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,814:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.100e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,815:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=8.956e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,816:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=8.394e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,817:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=7.739e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,818:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,824:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.045e-04, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,824:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=8.514e-05, with an active set of 125 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,824:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.331e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,828:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.568e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,828:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,836:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=2.179e-04, with an active set of 86 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,839:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=1.886e-04, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,840:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=6.017e-05, with an active set of 156 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,841:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.387e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,856:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 188 iterations, i.e. alpha=5.032e-05, with an active set of 177 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

el = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:26,893:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 303 iterations, i.e. alpha=2.912e+03, with an active set of 226 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,953:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 226 iterations, i.e. alpha=3.924e+00, with an active set of 177 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,900:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 16:19:26,955:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 16:19:26,955:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 16:19:26,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 221 iterations, i.e. alpha=2.336e+00, with an active set of 168 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,964:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 16:19:26,974:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.624e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,975:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.174e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,976:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.693e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,978:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=7.945e-04, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,981:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=7.142e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,981:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=5.839e-04, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,983:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=5.291e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,984:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.785e-04, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,990:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.870e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:26,997:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.992e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,000:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.744e-04, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,007:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=7.519e-05, with an active set of 136 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,014:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=4.954e-04, with an active set of 126 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,018:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=5.729e-05, with an active set of 162 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,043:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=5.135e-04, with an active set of 181 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,045:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 208 iterations, i.e. alpha=5.276e-04, with an active set of 182 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,056:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 16:19:27,061:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 239 iterations, i.e. alpha=6.469e-04, with an active set of 203 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,068:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 16:19:27,068:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 16:19:27,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 16:19:27,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 16:19:27,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 16:19:27,090:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 285 iterations, i.e. alpha=5.522e-03, with an active set of 226 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 16:19:27,123:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,124:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,124:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,319:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,321:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,321:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,338:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,339:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,339:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,339:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,339:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,340:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,342:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 16:19:27,398:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,399:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,399:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,428:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,429:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,430:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:27,430:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,431:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:19:27,431:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:19:29,570:INFO:Calculating mean and std
2023-05-20 16:19:29,571:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 16:19:29,572:INFO:Creating metrics dataframe
2023-05-20 16:19:29,831:INFO:Uploading results into container
2023-05-20 16:19:29,831:INFO:Uploading model into container now
2023-05-20 16:19:29,832:INFO:_master_model_container: 5
2023-05-20 16:19:29,832:INFO:_display_container: 2
2023-05-20 16:19:29,832:INFO:Lars(random_state=5308)
2023-05-20 16:19:29,832:INFO:create_model() successfully completed......................................
2023-05-20 16:19:29,899:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:29,899:INFO:Creating metrics dataframe
2023-05-20 16:19:29,908:INFO:Initializing Lasso Least Angle Regression
2023-05-20 16:19:29,908:INFO:Total runtime is 0.32106459538141885 minutes
2023-05-20 16:19:29,911:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:29,911:INFO:Initializing create_model()
2023-05-20 16:19:29,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:29,911:INFO:Checking exceptions
2023-05-20 16:19:29,911:INFO:Importing libraries
2023-05-20 16:19:29,911:INFO:Copying training dataset
2023-05-20 16:19:29,919:INFO:Defining folds
2023-05-20 16:19:29,919:INFO:Declaring metric variables
2023-05-20 16:19:29,923:INFO:Importing untrained model
2023-05-20 16:19:29,926:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 16:19:29,931:INFO:Starting cross validation
2023-05-20 16:19:29,933:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:30,048:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,052:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,071:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,081:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,091:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,108:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,119:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,124:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:30,142:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:19:32,490:INFO:Calculating mean and std
2023-05-20 16:19:32,492:INFO:Creating metrics dataframe
2023-05-20 16:19:32,753:INFO:Uploading results into container
2023-05-20 16:19:32,753:INFO:Uploading model into container now
2023-05-20 16:19:32,754:INFO:_master_model_container: 6
2023-05-20 16:19:32,754:INFO:_display_container: 2
2023-05-20 16:19:32,754:INFO:LassoLars(random_state=5308)
2023-05-20 16:19:32,754:INFO:create_model() successfully completed......................................
2023-05-20 16:19:32,819:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:32,820:INFO:Creating metrics dataframe
2023-05-20 16:19:32,829:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 16:19:32,830:INFO:Total runtime is 0.36977168718973796 minutes
2023-05-20 16:19:32,833:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:32,833:INFO:Initializing create_model()
2023-05-20 16:19:32,833:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:32,833:INFO:Checking exceptions
2023-05-20 16:19:32,833:INFO:Importing libraries
2023-05-20 16:19:32,833:INFO:Copying training dataset
2023-05-20 16:19:32,841:INFO:Defining folds
2023-05-20 16:19:32,841:INFO:Declaring metric variables
2023-05-20 16:19:32,844:INFO:Importing untrained model
2023-05-20 16:19:32,847:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 16:19:32,853:INFO:Starting cross validation
2023-05-20 16:19:32,855:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:32,962:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:32,964:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:32,973:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:32,976:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:32,994:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:33,010:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:33,021:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:33,033:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:33,043:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:33,051:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:19:35,470:INFO:Calculating mean and std
2023-05-20 16:19:35,472:INFO:Creating metrics dataframe
2023-05-20 16:19:35,728:INFO:Uploading results into container
2023-05-20 16:19:35,728:INFO:Uploading model into container now
2023-05-20 16:19:35,729:INFO:_master_model_container: 7
2023-05-20 16:19:35,729:INFO:_display_container: 2
2023-05-20 16:19:35,729:INFO:OrthogonalMatchingPursuit()
2023-05-20 16:19:35,729:INFO:create_model() successfully completed......................................
2023-05-20 16:19:35,798:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:35,798:INFO:Creating metrics dataframe
2023-05-20 16:19:35,806:INFO:Initializing Bayesian Ridge
2023-05-20 16:19:35,808:INFO:Total runtime is 0.4193893512090047 minutes
2023-05-20 16:19:35,810:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:35,811:INFO:Initializing create_model()
2023-05-20 16:19:35,811:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:35,811:INFO:Checking exceptions
2023-05-20 16:19:35,811:INFO:Importing libraries
2023-05-20 16:19:35,811:INFO:Copying training dataset
2023-05-20 16:19:35,820:INFO:Defining folds
2023-05-20 16:19:35,820:INFO:Declaring metric variables
2023-05-20 16:19:35,823:INFO:Importing untrained model
2023-05-20 16:19:35,826:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:19:35,832:INFO:Starting cross validation
2023-05-20 16:19:35,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:38,561:INFO:Calculating mean and std
2023-05-20 16:19:38,563:INFO:Creating metrics dataframe
2023-05-20 16:19:38,825:INFO:Uploading results into container
2023-05-20 16:19:38,826:INFO:Uploading model into container now
2023-05-20 16:19:38,826:INFO:_master_model_container: 8
2023-05-20 16:19:38,826:INFO:_display_container: 2
2023-05-20 16:19:38,827:INFO:BayesianRidge()
2023-05-20 16:19:38,827:INFO:create_model() successfully completed......................................
2023-05-20 16:19:38,925:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:38,925:INFO:Creating metrics dataframe
2023-05-20 16:19:38,934:INFO:Initializing Passive Aggressive Regressor
2023-05-20 16:19:38,935:INFO:Total runtime is 0.47151560386021935 minutes
2023-05-20 16:19:38,939:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:38,939:INFO:Initializing create_model()
2023-05-20 16:19:38,939:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:38,939:INFO:Checking exceptions
2023-05-20 16:19:38,939:INFO:Importing libraries
2023-05-20 16:19:38,939:INFO:Copying training dataset
2023-05-20 16:19:38,948:INFO:Defining folds
2023-05-20 16:19:38,949:INFO:Declaring metric variables
2023-05-20 16:19:38,951:INFO:Importing untrained model
2023-05-20 16:19:38,955:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 16:19:38,961:INFO:Starting cross validation
2023-05-20 16:19:38,963:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:41,591:INFO:Calculating mean and std
2023-05-20 16:19:41,593:INFO:Creating metrics dataframe
2023-05-20 16:19:41,876:INFO:Uploading results into container
2023-05-20 16:19:41,877:INFO:Uploading model into container now
2023-05-20 16:19:41,878:INFO:_master_model_container: 9
2023-05-20 16:19:41,878:INFO:_display_container: 2
2023-05-20 16:19:41,878:INFO:PassiveAggressiveRegressor(random_state=5308)
2023-05-20 16:19:41,878:INFO:create_model() successfully completed......................................
2023-05-20 16:19:41,947:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:41,947:INFO:Creating metrics dataframe
2023-05-20 16:19:41,956:INFO:Initializing Huber Regressor
2023-05-20 16:19:41,956:INFO:Total runtime is 0.5218642036120097 minutes
2023-05-20 16:19:41,960:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:41,960:INFO:Initializing create_model()
2023-05-20 16:19:41,961:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:41,961:INFO:Checking exceptions
2023-05-20 16:19:41,961:INFO:Importing libraries
2023-05-20 16:19:41,961:INFO:Copying training dataset
2023-05-20 16:19:41,978:INFO:Defining folds
2023-05-20 16:19:41,978:INFO:Declaring metric variables
2023-05-20 16:19:41,984:INFO:Importing untrained model
2023-05-20 16:19:41,990:INFO:Huber Regressor Imported successfully
2023-05-20 16:19:41,999:INFO:Starting cross validation
2023-05-20 16:19:42,000:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:44,966:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:44,988:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,000:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,133:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,298:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,299:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,327:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,337:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,375:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:45,429:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 16:19:48,475:INFO:Calculating mean and std
2023-05-20 16:19:48,477:INFO:Creating metrics dataframe
2023-05-20 16:19:48,794:INFO:Uploading results into container
2023-05-20 16:19:48,795:INFO:Uploading model into container now
2023-05-20 16:19:48,795:INFO:_master_model_container: 10
2023-05-20 16:19:48,796:INFO:_display_container: 2
2023-05-20 16:19:48,796:INFO:HuberRegressor()
2023-05-20 16:19:48,796:INFO:create_model() successfully completed......................................
2023-05-20 16:19:48,880:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:48,880:INFO:Creating metrics dataframe
2023-05-20 16:19:48,891:INFO:Initializing K Neighbors Regressor
2023-05-20 16:19:48,891:INFO:Total runtime is 0.6374483863512675 minutes
2023-05-20 16:19:48,894:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:48,895:INFO:Initializing create_model()
2023-05-20 16:19:48,895:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:48,895:INFO:Checking exceptions
2023-05-20 16:19:48,895:INFO:Importing libraries
2023-05-20 16:19:48,895:INFO:Copying training dataset
2023-05-20 16:19:48,918:INFO:Defining folds
2023-05-20 16:19:48,918:INFO:Declaring metric variables
2023-05-20 16:19:48,923:INFO:Importing untrained model
2023-05-20 16:19:48,927:INFO:K Neighbors Regressor Imported successfully
2023-05-20 16:19:48,934:INFO:Starting cross validation
2023-05-20 16:19:48,936:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:51,809:INFO:Calculating mean and std
2023-05-20 16:19:51,813:INFO:Creating metrics dataframe
2023-05-20 16:19:52,132:INFO:Uploading results into container
2023-05-20 16:19:52,132:INFO:Uploading model into container now
2023-05-20 16:19:52,133:INFO:_master_model_container: 11
2023-05-20 16:19:52,133:INFO:_display_container: 2
2023-05-20 16:19:52,133:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 16:19:52,133:INFO:create_model() successfully completed......................................
2023-05-20 16:19:52,216:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:52,216:INFO:Creating metrics dataframe
2023-05-20 16:19:52,233:INFO:Initializing Decision Tree Regressor
2023-05-20 16:19:52,233:INFO:Total runtime is 0.6931462725003561 minutes
2023-05-20 16:19:52,238:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:52,238:INFO:Initializing create_model()
2023-05-20 16:19:52,238:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:52,238:INFO:Checking exceptions
2023-05-20 16:19:52,238:INFO:Importing libraries
2023-05-20 16:19:52,238:INFO:Copying training dataset
2023-05-20 16:19:52,254:INFO:Defining folds
2023-05-20 16:19:52,255:INFO:Declaring metric variables
2023-05-20 16:19:52,258:INFO:Importing untrained model
2023-05-20 16:19:52,263:INFO:Decision Tree Regressor Imported successfully
2023-05-20 16:19:52,269:INFO:Starting cross validation
2023-05-20 16:19:52,271:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:55,137:INFO:Calculating mean and std
2023-05-20 16:19:55,139:INFO:Creating metrics dataframe
2023-05-20 16:19:55,423:INFO:Uploading results into container
2023-05-20 16:19:55,424:INFO:Uploading model into container now
2023-05-20 16:19:55,424:INFO:_master_model_container: 12
2023-05-20 16:19:55,425:INFO:_display_container: 2
2023-05-20 16:19:55,425:INFO:DecisionTreeRegressor(random_state=5308)
2023-05-20 16:19:55,425:INFO:create_model() successfully completed......................................
2023-05-20 16:19:55,496:INFO:SubProcess create_model() end ==================================
2023-05-20 16:19:55,496:INFO:Creating metrics dataframe
2023-05-20 16:19:55,507:INFO:Initializing Random Forest Regressor
2023-05-20 16:19:55,507:INFO:Total runtime is 0.7477080861727398 minutes
2023-05-20 16:19:55,509:INFO:SubProcess create_model() called ==================================
2023-05-20 16:19:55,510:INFO:Initializing create_model()
2023-05-20 16:19:55,510:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:19:55,510:INFO:Checking exceptions
2023-05-20 16:19:55,510:INFO:Importing libraries
2023-05-20 16:19:55,510:INFO:Copying training dataset
2023-05-20 16:19:55,521:INFO:Defining folds
2023-05-20 16:19:55,521:INFO:Declaring metric variables
2023-05-20 16:19:55,523:INFO:Importing untrained model
2023-05-20 16:19:55,527:INFO:Random Forest Regressor Imported successfully
2023-05-20 16:19:55,535:INFO:Starting cross validation
2023-05-20 16:19:55,537:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:19:58,129:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 16:19:58,138:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 16:20:01,070:INFO:Calculating mean and std
2023-05-20 16:20:01,071:INFO:Creating metrics dataframe
2023-05-20 16:20:01,368:INFO:Uploading results into container
2023-05-20 16:20:01,369:INFO:Uploading model into container now
2023-05-20 16:20:01,370:INFO:_master_model_container: 13
2023-05-20 16:20:01,370:INFO:_display_container: 2
2023-05-20 16:20:01,370:INFO:RandomForestRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:20:01,370:INFO:create_model() successfully completed......................................
2023-05-20 16:20:01,444:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:01,444:INFO:Creating metrics dataframe
2023-05-20 16:20:01,455:INFO:Initializing Extra Trees Regressor
2023-05-20 16:20:01,456:INFO:Total runtime is 0.8468593200047811 minutes
2023-05-20 16:20:01,459:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:01,459:INFO:Initializing create_model()
2023-05-20 16:20:01,459:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:01,459:INFO:Checking exceptions
2023-05-20 16:20:01,459:INFO:Importing libraries
2023-05-20 16:20:01,459:INFO:Copying training dataset
2023-05-20 16:20:01,473:INFO:Defining folds
2023-05-20 16:20:01,474:INFO:Declaring metric variables
2023-05-20 16:20:01,476:INFO:Importing untrained model
2023-05-20 16:20:01,485:INFO:Extra Trees Regressor Imported successfully
2023-05-20 16:20:01,497:INFO:Starting cross validation
2023-05-20 16:20:01,500:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:20:04,188:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 16:20:07,851:INFO:Calculating mean and std
2023-05-20 16:20:07,858:INFO:Creating metrics dataframe
2023-05-20 16:20:08,161:INFO:Uploading results into container
2023-05-20 16:20:08,162:INFO:Uploading model into container now
2023-05-20 16:20:08,163:INFO:_master_model_container: 14
2023-05-20 16:20:08,163:INFO:_display_container: 2
2023-05-20 16:20:08,163:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:20:08,164:INFO:create_model() successfully completed......................................
2023-05-20 16:20:08,229:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:08,229:INFO:Creating metrics dataframe
2023-05-20 16:20:08,239:INFO:Initializing AdaBoost Regressor
2023-05-20 16:20:08,239:INFO:Total runtime is 0.9599092841148377 minutes
2023-05-20 16:20:08,242:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:08,242:INFO:Initializing create_model()
2023-05-20 16:20:08,242:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:08,242:INFO:Checking exceptions
2023-05-20 16:20:08,242:INFO:Importing libraries
2023-05-20 16:20:08,242:INFO:Copying training dataset
2023-05-20 16:20:08,251:INFO:Defining folds
2023-05-20 16:20:08,251:INFO:Declaring metric variables
2023-05-20 16:20:08,254:INFO:Importing untrained model
2023-05-20 16:20:08,257:INFO:AdaBoost Regressor Imported successfully
2023-05-20 16:20:08,262:INFO:Starting cross validation
2023-05-20 16:20:08,265:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:20:12,106:INFO:Calculating mean and std
2023-05-20 16:20:12,108:INFO:Creating metrics dataframe
2023-05-20 16:20:12,390:INFO:Uploading results into container
2023-05-20 16:20:12,391:INFO:Uploading model into container now
2023-05-20 16:20:12,391:INFO:_master_model_container: 15
2023-05-20 16:20:12,391:INFO:_display_container: 2
2023-05-20 16:20:12,393:INFO:AdaBoostRegressor(random_state=5308)
2023-05-20 16:20:12,393:INFO:create_model() successfully completed......................................
2023-05-20 16:20:12,455:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:12,456:INFO:Creating metrics dataframe
2023-05-20 16:20:12,465:INFO:Initializing Gradient Boosting Regressor
2023-05-20 16:20:12,465:INFO:Total runtime is 1.0303536613782247 minutes
2023-05-20 16:20:12,469:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:12,469:INFO:Initializing create_model()
2023-05-20 16:20:12,469:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:12,469:INFO:Checking exceptions
2023-05-20 16:20:12,470:INFO:Importing libraries
2023-05-20 16:20:12,470:INFO:Copying training dataset
2023-05-20 16:20:12,477:INFO:Defining folds
2023-05-20 16:20:12,477:INFO:Declaring metric variables
2023-05-20 16:20:12,480:INFO:Importing untrained model
2023-05-20 16:20:12,484:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:20:12,489:INFO:Starting cross validation
2023-05-20 16:20:12,491:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:20:16,364:INFO:Calculating mean and std
2023-05-20 16:20:16,366:INFO:Creating metrics dataframe
2023-05-20 16:20:16,652:INFO:Uploading results into container
2023-05-20 16:20:16,652:INFO:Uploading model into container now
2023-05-20 16:20:16,653:INFO:_master_model_container: 16
2023-05-20 16:20:16,653:INFO:_display_container: 2
2023-05-20 16:20:16,653:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:20:16,653:INFO:create_model() successfully completed......................................
2023-05-20 16:20:16,722:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:16,722:INFO:Creating metrics dataframe
2023-05-20 16:20:16,733:INFO:Initializing Extreme Gradient Boosting
2023-05-20 16:20:16,733:INFO:Total runtime is 1.1014760772387187 minutes
2023-05-20 16:20:16,737:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:16,737:INFO:Initializing create_model()
2023-05-20 16:20:16,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:16,738:INFO:Checking exceptions
2023-05-20 16:20:16,738:INFO:Importing libraries
2023-05-20 16:20:16,738:INFO:Copying training dataset
2023-05-20 16:20:16,746:INFO:Defining folds
2023-05-20 16:20:16,746:INFO:Declaring metric variables
2023-05-20 16:20:16,748:INFO:Importing untrained model
2023-05-20 16:20:16,752:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 16:20:16,759:INFO:Starting cross validation
2023-05-20 16:20:16,760:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:20:22,261:INFO:Calculating mean and std
2023-05-20 16:20:22,265:INFO:Creating metrics dataframe
2023-05-20 16:20:22,621:INFO:Uploading results into container
2023-05-20 16:20:22,623:INFO:Uploading model into container now
2023-05-20 16:20:22,623:INFO:_master_model_container: 17
2023-05-20 16:20:22,624:INFO:_display_container: 2
2023-05-20 16:20:22,624:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5308, ...)
2023-05-20 16:20:22,624:INFO:create_model() successfully completed......................................
2023-05-20 16:20:22,776:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:22,777:INFO:Creating metrics dataframe
2023-05-20 16:20:22,797:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 16:20:22,797:INFO:Total runtime is 1.202543596426646 minutes
2023-05-20 16:20:22,802:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:22,803:INFO:Initializing create_model()
2023-05-20 16:20:22,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:22,803:INFO:Checking exceptions
2023-05-20 16:20:22,803:INFO:Importing libraries
2023-05-20 16:20:22,804:INFO:Copying training dataset
2023-05-20 16:20:22,825:INFO:Defining folds
2023-05-20 16:20:22,825:INFO:Declaring metric variables
2023-05-20 16:20:22,830:INFO:Importing untrained model
2023-05-20 16:20:22,835:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:20:22,843:INFO:Starting cross validation
2023-05-20 16:20:22,845:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:20:27,259:INFO:Calculating mean and std
2023-05-20 16:20:27,261:INFO:Creating metrics dataframe
2023-05-20 16:20:27,646:INFO:Uploading results into container
2023-05-20 16:20:27,649:INFO:Uploading model into container now
2023-05-20 16:20:27,650:INFO:_master_model_container: 18
2023-05-20 16:20:27,650:INFO:_display_container: 2
2023-05-20 16:20:27,650:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:20:27,651:INFO:create_model() successfully completed......................................
2023-05-20 16:20:27,748:INFO:SubProcess create_model() end ==================================
2023-05-20 16:20:27,748:INFO:Creating metrics dataframe
2023-05-20 16:20:27,762:INFO:Initializing CatBoost Regressor
2023-05-20 16:20:27,762:INFO:Total runtime is 1.285290785630544 minutes
2023-05-20 16:20:27,765:INFO:SubProcess create_model() called ==================================
2023-05-20 16:20:27,765:INFO:Initializing create_model()
2023-05-20 16:20:27,765:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:20:27,766:INFO:Checking exceptions
2023-05-20 16:20:27,766:INFO:Importing libraries
2023-05-20 16:20:27,766:INFO:Copying training dataset
2023-05-20 16:20:27,783:INFO:Defining folds
2023-05-20 16:20:27,783:INFO:Declaring metric variables
2023-05-20 16:20:27,790:INFO:Importing untrained model
2023-05-20 16:20:27,796:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:20:27,811:INFO:Starting cross validation
2023-05-20 16:20:27,815:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:21:00,415:INFO:Calculating mean and std
2023-05-20 16:21:00,417:INFO:Creating metrics dataframe
2023-05-20 16:21:00,834:INFO:Uploading results into container
2023-05-20 16:21:00,835:INFO:Uploading model into container now
2023-05-20 16:21:00,836:INFO:_master_model_container: 19
2023-05-20 16:21:00,836:INFO:_display_container: 2
2023-05-20 16:21:00,836:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF649B6A0>
2023-05-20 16:21:00,836:INFO:create_model() successfully completed......................................
2023-05-20 16:21:00,963:INFO:SubProcess create_model() end ==================================
2023-05-20 16:21:00,964:INFO:Creating metrics dataframe
2023-05-20 16:21:00,975:INFO:Initializing Dummy Regressor
2023-05-20 16:21:00,976:INFO:Total runtime is 1.8388687729835511 minutes
2023-05-20 16:21:00,979:INFO:SubProcess create_model() called ==================================
2023-05-20 16:21:00,980:INFO:Initializing create_model()
2023-05-20 16:21:00,980:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF5C7BD30>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:21:00,980:INFO:Checking exceptions
2023-05-20 16:21:00,980:INFO:Importing libraries
2023-05-20 16:21:00,980:INFO:Copying training dataset
2023-05-20 16:21:00,991:INFO:Defining folds
2023-05-20 16:21:00,991:INFO:Declaring metric variables
2023-05-20 16:21:00,995:INFO:Importing untrained model
2023-05-20 16:21:00,999:INFO:Dummy Regressor Imported successfully
2023-05-20 16:21:01,008:INFO:Starting cross validation
2023-05-20 16:21:01,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:21:04,447:INFO:Calculating mean and std
2023-05-20 16:21:04,449:INFO:Creating metrics dataframe
2023-05-20 16:21:04,762:INFO:Uploading results into container
2023-05-20 16:21:04,763:INFO:Uploading model into container now
2023-05-20 16:21:04,764:INFO:_master_model_container: 20
2023-05-20 16:21:04,764:INFO:_display_container: 2
2023-05-20 16:21:04,764:INFO:DummyRegressor()
2023-05-20 16:21:04,765:INFO:create_model() successfully completed......................................
2023-05-20 16:21:04,830:INFO:SubProcess create_model() end ==================================
2023-05-20 16:21:04,830:INFO:Creating metrics dataframe
2023-05-20 16:21:04,851:INFO:Initializing create_model()
2023-05-20 16:21:04,851:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BF649B6A0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:21:04,851:INFO:Checking exceptions
2023-05-20 16:21:04,852:INFO:Importing libraries
2023-05-20 16:21:04,852:INFO:Copying training dataset
2023-05-20 16:21:04,860:INFO:Defining folds
2023-05-20 16:21:04,860:INFO:Declaring metric variables
2023-05-20 16:21:04,860:INFO:Importing untrained model
2023-05-20 16:21:04,860:INFO:Declaring custom model
2023-05-20 16:21:04,860:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:21:04,862:INFO:Cross validation set to False
2023-05-20 16:21:04,862:INFO:Fitting Model
2023-05-20 16:21:08,415:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD98ED0F0>
2023-05-20 16:21:08,416:INFO:create_model() successfully completed......................................
2023-05-20 16:21:08,512:INFO:_master_model_container: 20
2023-05-20 16:21:08,512:INFO:_display_container: 2
2023-05-20 16:21:08,513:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BD98ED0F0>
2023-05-20 16:21:08,513:INFO:compare_models() successfully completed......................................
2023-05-20 16:23:27,403:INFO:Initializing compare_models()
2023-05-20 16:23:27,407:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:23:27,407:INFO:Checking exceptions
2023-05-20 16:23:27,412:INFO:Preparing display monitor
2023-05-20 16:23:27,473:INFO:Initializing Linear Regression
2023-05-20 16:23:27,474:INFO:Total runtime is 0.0 minutes
2023-05-20 16:23:27,478:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:27,479:INFO:Initializing create_model()
2023-05-20 16:23:27,479:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:27,479:INFO:Checking exceptions
2023-05-20 16:23:27,479:INFO:Importing libraries
2023-05-20 16:23:27,479:INFO:Copying training dataset
2023-05-20 16:23:27,500:INFO:Defining folds
2023-05-20 16:23:27,500:INFO:Declaring metric variables
2023-05-20 16:23:27,506:INFO:Importing untrained model
2023-05-20 16:23:27,511:INFO:Linear Regression Imported successfully
2023-05-20 16:23:27,519:INFO:Starting cross validation
2023-05-20 16:23:27,521:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:30,615:INFO:Calculating mean and std
2023-05-20 16:23:30,618:INFO:Creating metrics dataframe
2023-05-20 16:23:30,927:INFO:Uploading results into container
2023-05-20 16:23:30,929:INFO:Uploading model into container now
2023-05-20 16:23:30,929:INFO:_master_model_container: 21
2023-05-20 16:23:30,929:INFO:_display_container: 3
2023-05-20 16:23:30,930:INFO:LinearRegression(n_jobs=-1)
2023-05-20 16:23:30,930:INFO:create_model() successfully completed......................................
2023-05-20 16:23:31,036:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:31,036:INFO:Creating metrics dataframe
2023-05-20 16:23:31,049:INFO:Initializing Lasso Regression
2023-05-20 16:23:31,050:INFO:Total runtime is 0.059606043497721355 minutes
2023-05-20 16:23:31,055:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:31,055:INFO:Initializing create_model()
2023-05-20 16:23:31,056:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:31,056:INFO:Checking exceptions
2023-05-20 16:23:31,056:INFO:Importing libraries
2023-05-20 16:23:31,056:INFO:Copying training dataset
2023-05-20 16:23:31,080:INFO:Defining folds
2023-05-20 16:23:31,081:INFO:Declaring metric variables
2023-05-20 16:23:31,089:INFO:Importing untrained model
2023-05-20 16:23:31,098:INFO:Lasso Regression Imported successfully
2023-05-20 16:23:31,113:INFO:Starting cross validation
2023-05-20 16:23:31,115:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:34,302:INFO:Calculating mean and std
2023-05-20 16:23:34,303:INFO:Creating metrics dataframe
2023-05-20 16:23:34,636:INFO:Uploading results into container
2023-05-20 16:23:34,637:INFO:Uploading model into container now
2023-05-20 16:23:34,638:INFO:_master_model_container: 22
2023-05-20 16:23:34,638:INFO:_display_container: 3
2023-05-20 16:23:34,639:INFO:Lasso(random_state=5308)
2023-05-20 16:23:34,639:INFO:create_model() successfully completed......................................
2023-05-20 16:23:34,706:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:34,706:INFO:Creating metrics dataframe
2023-05-20 16:23:34,714:INFO:Initializing Ridge Regression
2023-05-20 16:23:34,714:INFO:Total runtime is 0.1206764539082845 minutes
2023-05-20 16:23:34,718:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:34,718:INFO:Initializing create_model()
2023-05-20 16:23:34,719:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:34,719:INFO:Checking exceptions
2023-05-20 16:23:34,719:INFO:Importing libraries
2023-05-20 16:23:34,719:INFO:Copying training dataset
2023-05-20 16:23:34,729:INFO:Defining folds
2023-05-20 16:23:34,729:INFO:Declaring metric variables
2023-05-20 16:23:34,732:INFO:Importing untrained model
2023-05-20 16:23:34,735:INFO:Ridge Regression Imported successfully
2023-05-20 16:23:34,742:INFO:Starting cross validation
2023-05-20 16:23:34,744:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:37,802:INFO:Calculating mean and std
2023-05-20 16:23:37,804:INFO:Creating metrics dataframe
2023-05-20 16:23:38,114:INFO:Uploading results into container
2023-05-20 16:23:38,115:INFO:Uploading model into container now
2023-05-20 16:23:38,115:INFO:_master_model_container: 23
2023-05-20 16:23:38,116:INFO:_display_container: 3
2023-05-20 16:23:38,116:INFO:Ridge(random_state=5308)
2023-05-20 16:23:38,116:INFO:create_model() successfully completed......................................
2023-05-20 16:23:38,184:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:38,184:INFO:Creating metrics dataframe
2023-05-20 16:23:38,193:INFO:Initializing Elastic Net
2023-05-20 16:23:38,193:INFO:Total runtime is 0.17865747213363647 minutes
2023-05-20 16:23:38,197:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:38,197:INFO:Initializing create_model()
2023-05-20 16:23:38,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:38,197:INFO:Checking exceptions
2023-05-20 16:23:38,197:INFO:Importing libraries
2023-05-20 16:23:38,197:INFO:Copying training dataset
2023-05-20 16:23:38,205:INFO:Defining folds
2023-05-20 16:23:38,205:INFO:Declaring metric variables
2023-05-20 16:23:38,207:INFO:Importing untrained model
2023-05-20 16:23:38,211:INFO:Elastic Net Imported successfully
2023-05-20 16:23:38,217:INFO:Starting cross validation
2023-05-20 16:23:38,219:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:41,231:INFO:Calculating mean and std
2023-05-20 16:23:41,233:INFO:Creating metrics dataframe
2023-05-20 16:23:41,550:INFO:Uploading results into container
2023-05-20 16:23:41,551:INFO:Uploading model into container now
2023-05-20 16:23:41,551:INFO:_master_model_container: 24
2023-05-20 16:23:41,552:INFO:_display_container: 3
2023-05-20 16:23:41,552:INFO:ElasticNet(random_state=5308)
2023-05-20 16:23:41,552:INFO:create_model() successfully completed......................................
2023-05-20 16:23:41,615:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:41,615:INFO:Creating metrics dataframe
2023-05-20 16:23:41,623:INFO:Initializing Least Angle Regression
2023-05-20 16:23:41,623:INFO:Total runtime is 0.23583112955093383 minutes
2023-05-20 16:23:41,626:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:41,626:INFO:Initializing create_model()
2023-05-20 16:23:41,626:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:41,626:INFO:Checking exceptions
2023-05-20 16:23:41,626:INFO:Importing libraries
2023-05-20 16:23:41,626:INFO:Copying training dataset
2023-05-20 16:23:41,635:INFO:Defining folds
2023-05-20 16:23:41,635:INFO:Declaring metric variables
2023-05-20 16:23:41,638:INFO:Importing untrained model
2023-05-20 16:23:41,641:INFO:Least Angle Regression Imported successfully
2023-05-20 16:23:41,647:INFO:Starting cross validation
2023-05-20 16:23:41,649:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:41,862:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,863:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,864:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,865:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,865:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,865:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,866:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,866:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,867:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,890:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,891:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,891:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,910:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 16:23:41,914:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 16:23:41,915:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,915:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,916:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,927:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,927:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,928:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:41,931:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,933:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:23:41,933:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:23:44,648:INFO:Calculating mean and std
2023-05-20 16:23:44,648:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 16:23:44,650:INFO:Creating metrics dataframe
2023-05-20 16:23:44,991:INFO:Uploading results into container
2023-05-20 16:23:44,992:INFO:Uploading model into container now
2023-05-20 16:23:44,993:INFO:_master_model_container: 25
2023-05-20 16:23:44,993:INFO:_display_container: 3
2023-05-20 16:23:44,993:INFO:Lars(random_state=5308)
2023-05-20 16:23:44,993:INFO:create_model() successfully completed......................................
2023-05-20 16:23:45,062:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:45,062:INFO:Creating metrics dataframe
2023-05-20 16:23:45,071:INFO:Initializing Lasso Least Angle Regression
2023-05-20 16:23:45,071:INFO:Total runtime is 0.2933023651440938 minutes
2023-05-20 16:23:45,074:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:45,074:INFO:Initializing create_model()
2023-05-20 16:23:45,074:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:45,074:INFO:Checking exceptions
2023-05-20 16:23:45,075:INFO:Importing libraries
2023-05-20 16:23:45,075:INFO:Copying training dataset
2023-05-20 16:23:45,085:INFO:Defining folds
2023-05-20 16:23:45,085:INFO:Declaring metric variables
2023-05-20 16:23:45,088:INFO:Importing untrained model
2023-05-20 16:23:45,090:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 16:23:45,097:INFO:Starting cross validation
2023-05-20 16:23:45,099:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:45,224:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,234:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,250:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,261:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,271:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,284:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,288:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,303:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:45,311:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:23:48,154:INFO:Calculating mean and std
2023-05-20 16:23:48,156:INFO:Creating metrics dataframe
2023-05-20 16:23:48,490:INFO:Uploading results into container
2023-05-20 16:23:48,491:INFO:Uploading model into container now
2023-05-20 16:23:48,492:INFO:_master_model_container: 26
2023-05-20 16:23:48,492:INFO:_display_container: 3
2023-05-20 16:23:48,492:INFO:LassoLars(random_state=5308)
2023-05-20 16:23:48,492:INFO:create_model() successfully completed......................................
2023-05-20 16:23:48,562:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:48,562:INFO:Creating metrics dataframe
2023-05-20 16:23:48,570:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 16:23:48,570:INFO:Total runtime is 0.35162149270375564 minutes
2023-05-20 16:23:48,574:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:48,574:INFO:Initializing create_model()
2023-05-20 16:23:48,574:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:48,574:INFO:Checking exceptions
2023-05-20 16:23:48,574:INFO:Importing libraries
2023-05-20 16:23:48,574:INFO:Copying training dataset
2023-05-20 16:23:48,583:INFO:Defining folds
2023-05-20 16:23:48,583:INFO:Declaring metric variables
2023-05-20 16:23:48,586:INFO:Importing untrained model
2023-05-20 16:23:48,589:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 16:23:48,597:INFO:Starting cross validation
2023-05-20 16:23:48,598:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:48,731:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,736:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,738:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,748:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,759:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,767:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,768:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,797:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,801:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:48,811:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:23:51,854:INFO:Calculating mean and std
2023-05-20 16:23:51,856:INFO:Creating metrics dataframe
2023-05-20 16:23:52,167:INFO:Uploading results into container
2023-05-20 16:23:52,169:INFO:Uploading model into container now
2023-05-20 16:23:52,169:INFO:_master_model_container: 27
2023-05-20 16:23:52,169:INFO:_display_container: 3
2023-05-20 16:23:52,170:INFO:OrthogonalMatchingPursuit()
2023-05-20 16:23:52,170:INFO:create_model() successfully completed......................................
2023-05-20 16:23:52,239:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:52,240:INFO:Creating metrics dataframe
2023-05-20 16:23:52,249:INFO:Initializing Bayesian Ridge
2023-05-20 16:23:52,249:INFO:Total runtime is 0.412930945555369 minutes
2023-05-20 16:23:52,253:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:52,253:INFO:Initializing create_model()
2023-05-20 16:23:52,253:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:52,253:INFO:Checking exceptions
2023-05-20 16:23:52,253:INFO:Importing libraries
2023-05-20 16:23:52,253:INFO:Copying training dataset
2023-05-20 16:23:52,260:INFO:Defining folds
2023-05-20 16:23:52,260:INFO:Declaring metric variables
2023-05-20 16:23:52,263:INFO:Importing untrained model
2023-05-20 16:23:52,266:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:23:52,272:INFO:Starting cross validation
2023-05-20 16:23:52,273:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:55,369:INFO:Calculating mean and std
2023-05-20 16:23:55,371:INFO:Creating metrics dataframe
2023-05-20 16:23:55,678:INFO:Uploading results into container
2023-05-20 16:23:55,679:INFO:Uploading model into container now
2023-05-20 16:23:55,679:INFO:_master_model_container: 28
2023-05-20 16:23:55,679:INFO:_display_container: 3
2023-05-20 16:23:55,680:INFO:BayesianRidge()
2023-05-20 16:23:55,680:INFO:create_model() successfully completed......................................
2023-05-20 16:23:55,758:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:55,759:INFO:Creating metrics dataframe
2023-05-20 16:23:55,769:INFO:Initializing Passive Aggressive Regressor
2023-05-20 16:23:55,769:INFO:Total runtime is 0.4716047048568725 minutes
2023-05-20 16:23:55,773:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:55,773:INFO:Initializing create_model()
2023-05-20 16:23:55,773:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:55,773:INFO:Checking exceptions
2023-05-20 16:23:55,773:INFO:Importing libraries
2023-05-20 16:23:55,773:INFO:Copying training dataset
2023-05-20 16:23:55,782:INFO:Defining folds
2023-05-20 16:23:55,782:INFO:Declaring metric variables
2023-05-20 16:23:55,786:INFO:Importing untrained model
2023-05-20 16:23:55,789:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 16:23:55,794:INFO:Starting cross validation
2023-05-20 16:23:55,795:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:23:58,907:INFO:Calculating mean and std
2023-05-20 16:23:58,909:INFO:Creating metrics dataframe
2023-05-20 16:23:59,224:INFO:Uploading results into container
2023-05-20 16:23:59,225:INFO:Uploading model into container now
2023-05-20 16:23:59,226:INFO:_master_model_container: 29
2023-05-20 16:23:59,226:INFO:_display_container: 3
2023-05-20 16:23:59,226:INFO:PassiveAggressiveRegressor(random_state=5308)
2023-05-20 16:23:59,226:INFO:create_model() successfully completed......................................
2023-05-20 16:23:59,290:INFO:SubProcess create_model() end ==================================
2023-05-20 16:23:59,290:INFO:Creating metrics dataframe
2023-05-20 16:23:59,300:INFO:Initializing Huber Regressor
2023-05-20 16:23:59,301:INFO:Total runtime is 0.5304632226626078 minutes
2023-05-20 16:23:59,304:INFO:SubProcess create_model() called ==================================
2023-05-20 16:23:59,304:INFO:Initializing create_model()
2023-05-20 16:23:59,304:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:23:59,304:INFO:Checking exceptions
2023-05-20 16:23:59,304:INFO:Importing libraries
2023-05-20 16:23:59,305:INFO:Copying training dataset
2023-05-20 16:23:59,314:INFO:Defining folds
2023-05-20 16:23:59,314:INFO:Declaring metric variables
2023-05-20 16:23:59,317:INFO:Importing untrained model
2023-05-20 16:23:59,319:INFO:Huber Regressor Imported successfully
2023-05-20 16:23:59,325:INFO:Starting cross validation
2023-05-20 16:23:59,327:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:02,555:INFO:Calculating mean and std
2023-05-20 16:24:02,557:INFO:Creating metrics dataframe
2023-05-20 16:24:02,874:INFO:Uploading results into container
2023-05-20 16:24:02,875:INFO:Uploading model into container now
2023-05-20 16:24:02,876:INFO:_master_model_container: 30
2023-05-20 16:24:02,876:INFO:_display_container: 3
2023-05-20 16:24:02,876:INFO:HuberRegressor()
2023-05-20 16:24:02,876:INFO:create_model() successfully completed......................................
2023-05-20 16:24:02,943:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:02,943:INFO:Creating metrics dataframe
2023-05-20 16:24:02,953:INFO:Initializing K Neighbors Regressor
2023-05-20 16:24:02,953:INFO:Total runtime is 0.5913364211718242 minutes
2023-05-20 16:24:02,956:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:02,957:INFO:Initializing create_model()
2023-05-20 16:24:02,957:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:02,957:INFO:Checking exceptions
2023-05-20 16:24:02,957:INFO:Importing libraries
2023-05-20 16:24:02,957:INFO:Copying training dataset
2023-05-20 16:24:02,965:INFO:Defining folds
2023-05-20 16:24:02,965:INFO:Declaring metric variables
2023-05-20 16:24:02,968:INFO:Importing untrained model
2023-05-20 16:24:02,971:INFO:K Neighbors Regressor Imported successfully
2023-05-20 16:24:02,977:INFO:Starting cross validation
2023-05-20 16:24:02,979:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:06,048:INFO:Calculating mean and std
2023-05-20 16:24:06,050:INFO:Creating metrics dataframe
2023-05-20 16:24:06,379:INFO:Uploading results into container
2023-05-20 16:24:06,380:INFO:Uploading model into container now
2023-05-20 16:24:06,381:INFO:_master_model_container: 31
2023-05-20 16:24:06,381:INFO:_display_container: 3
2023-05-20 16:24:06,381:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 16:24:06,381:INFO:create_model() successfully completed......................................
2023-05-20 16:24:06,446:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:06,446:INFO:Creating metrics dataframe
2023-05-20 16:24:06,456:INFO:Initializing Decision Tree Regressor
2023-05-20 16:24:06,456:INFO:Total runtime is 0.6497128287951152 minutes
2023-05-20 16:24:06,460:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:06,460:INFO:Initializing create_model()
2023-05-20 16:24:06,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:06,460:INFO:Checking exceptions
2023-05-20 16:24:06,460:INFO:Importing libraries
2023-05-20 16:24:06,460:INFO:Copying training dataset
2023-05-20 16:24:06,467:INFO:Defining folds
2023-05-20 16:24:06,468:INFO:Declaring metric variables
2023-05-20 16:24:06,470:INFO:Importing untrained model
2023-05-20 16:24:06,473:INFO:Decision Tree Regressor Imported successfully
2023-05-20 16:24:06,478:INFO:Starting cross validation
2023-05-20 16:24:06,480:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:09,587:INFO:Calculating mean and std
2023-05-20 16:24:09,589:INFO:Creating metrics dataframe
2023-05-20 16:24:09,901:INFO:Uploading results into container
2023-05-20 16:24:09,903:INFO:Uploading model into container now
2023-05-20 16:24:09,903:INFO:_master_model_container: 32
2023-05-20 16:24:09,904:INFO:_display_container: 3
2023-05-20 16:24:09,904:INFO:DecisionTreeRegressor(random_state=5308)
2023-05-20 16:24:09,904:INFO:create_model() successfully completed......................................
2023-05-20 16:24:09,972:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:09,973:INFO:Creating metrics dataframe
2023-05-20 16:24:09,982:INFO:Initializing Random Forest Regressor
2023-05-20 16:24:09,982:INFO:Total runtime is 0.7084751605987549 minutes
2023-05-20 16:24:09,986:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:09,986:INFO:Initializing create_model()
2023-05-20 16:24:09,986:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:09,986:INFO:Checking exceptions
2023-05-20 16:24:09,986:INFO:Importing libraries
2023-05-20 16:24:09,987:INFO:Copying training dataset
2023-05-20 16:24:09,995:INFO:Defining folds
2023-05-20 16:24:09,995:INFO:Declaring metric variables
2023-05-20 16:24:09,998:INFO:Importing untrained model
2023-05-20 16:24:10,001:INFO:Random Forest Regressor Imported successfully
2023-05-20 16:24:10,009:INFO:Starting cross validation
2023-05-20 16:24:10,010:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:13,221:INFO:Calculating mean and std
2023-05-20 16:24:13,223:INFO:Creating metrics dataframe
2023-05-20 16:24:13,535:INFO:Uploading results into container
2023-05-20 16:24:13,535:INFO:Uploading model into container now
2023-05-20 16:24:13,536:INFO:_master_model_container: 33
2023-05-20 16:24:13,536:INFO:_display_container: 3
2023-05-20 16:24:13,536:INFO:RandomForestRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:24:13,537:INFO:create_model() successfully completed......................................
2023-05-20 16:24:13,600:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:13,600:INFO:Creating metrics dataframe
2023-05-20 16:24:13,610:INFO:Initializing Extra Trees Regressor
2023-05-20 16:24:13,610:INFO:Total runtime is 0.7689509272575379 minutes
2023-05-20 16:24:13,614:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:13,615:INFO:Initializing create_model()
2023-05-20 16:24:13,615:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:13,615:INFO:Checking exceptions
2023-05-20 16:24:13,615:INFO:Importing libraries
2023-05-20 16:24:13,615:INFO:Copying training dataset
2023-05-20 16:24:13,622:INFO:Defining folds
2023-05-20 16:24:13,623:INFO:Declaring metric variables
2023-05-20 16:24:13,625:INFO:Importing untrained model
2023-05-20 16:24:13,628:INFO:Extra Trees Regressor Imported successfully
2023-05-20 16:24:13,633:INFO:Starting cross validation
2023-05-20 16:24:13,635:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:16,741:INFO:Calculating mean and std
2023-05-20 16:24:16,743:INFO:Creating metrics dataframe
2023-05-20 16:24:17,046:INFO:Uploading results into container
2023-05-20 16:24:17,047:INFO:Uploading model into container now
2023-05-20 16:24:17,047:INFO:_master_model_container: 34
2023-05-20 16:24:17,047:INFO:_display_container: 3
2023-05-20 16:24:17,047:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:24:17,048:INFO:create_model() successfully completed......................................
2023-05-20 16:24:17,110:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:17,110:INFO:Creating metrics dataframe
2023-05-20 16:24:17,120:INFO:Initializing AdaBoost Regressor
2023-05-20 16:24:17,120:INFO:Total runtime is 0.8274482647577922 minutes
2023-05-20 16:24:17,123:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:17,123:INFO:Initializing create_model()
2023-05-20 16:24:17,123:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:17,123:INFO:Checking exceptions
2023-05-20 16:24:17,123:INFO:Importing libraries
2023-05-20 16:24:17,123:INFO:Copying training dataset
2023-05-20 16:24:17,132:INFO:Defining folds
2023-05-20 16:24:17,132:INFO:Declaring metric variables
2023-05-20 16:24:17,135:INFO:Importing untrained model
2023-05-20 16:24:17,138:INFO:AdaBoost Regressor Imported successfully
2023-05-20 16:24:17,143:INFO:Starting cross validation
2023-05-20 16:24:17,145:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:20,144:INFO:Calculating mean and std
2023-05-20 16:24:20,146:INFO:Creating metrics dataframe
2023-05-20 16:24:20,453:INFO:Uploading results into container
2023-05-20 16:24:20,455:INFO:Uploading model into container now
2023-05-20 16:24:20,455:INFO:_master_model_container: 35
2023-05-20 16:24:20,455:INFO:_display_container: 3
2023-05-20 16:24:20,456:INFO:AdaBoostRegressor(random_state=5308)
2023-05-20 16:24:20,456:INFO:create_model() successfully completed......................................
2023-05-20 16:24:20,518:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:20,518:INFO:Creating metrics dataframe
2023-05-20 16:24:20,528:INFO:Initializing Gradient Boosting Regressor
2023-05-20 16:24:20,528:INFO:Total runtime is 0.8842489441235861 minutes
2023-05-20 16:24:20,531:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:20,531:INFO:Initializing create_model()
2023-05-20 16:24:20,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:20,531:INFO:Checking exceptions
2023-05-20 16:24:20,531:INFO:Importing libraries
2023-05-20 16:24:20,531:INFO:Copying training dataset
2023-05-20 16:24:20,546:INFO:Defining folds
2023-05-20 16:24:20,546:INFO:Declaring metric variables
2023-05-20 16:24:20,550:INFO:Importing untrained model
2023-05-20 16:24:20,556:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:24:20,564:INFO:Starting cross validation
2023-05-20 16:24:20,566:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:23,598:INFO:Calculating mean and std
2023-05-20 16:24:23,600:INFO:Creating metrics dataframe
2023-05-20 16:24:23,916:INFO:Uploading results into container
2023-05-20 16:24:23,917:INFO:Uploading model into container now
2023-05-20 16:24:23,918:INFO:_master_model_container: 36
2023-05-20 16:24:23,918:INFO:_display_container: 3
2023-05-20 16:24:23,918:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:24:23,918:INFO:create_model() successfully completed......................................
2023-05-20 16:24:23,981:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:23,981:INFO:Creating metrics dataframe
2023-05-20 16:24:23,991:INFO:Initializing Extreme Gradient Boosting
2023-05-20 16:24:23,991:INFO:Total runtime is 0.9419670899709067 minutes
2023-05-20 16:24:23,996:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:23,996:INFO:Initializing create_model()
2023-05-20 16:24:23,996:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:23,996:INFO:Checking exceptions
2023-05-20 16:24:23,996:INFO:Importing libraries
2023-05-20 16:24:23,996:INFO:Copying training dataset
2023-05-20 16:24:24,004:INFO:Defining folds
2023-05-20 16:24:24,004:INFO:Declaring metric variables
2023-05-20 16:24:24,007:INFO:Importing untrained model
2023-05-20 16:24:24,012:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 16:24:24,018:INFO:Starting cross validation
2023-05-20 16:24:24,019:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:27,012:INFO:Calculating mean and std
2023-05-20 16:24:27,013:INFO:Creating metrics dataframe
2023-05-20 16:24:27,328:INFO:Uploading results into container
2023-05-20 16:24:27,329:INFO:Uploading model into container now
2023-05-20 16:24:27,329:INFO:_master_model_container: 37
2023-05-20 16:24:27,329:INFO:_display_container: 3
2023-05-20 16:24:27,330:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5308, ...)
2023-05-20 16:24:27,330:INFO:create_model() successfully completed......................................
2023-05-20 16:24:27,395:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:27,396:INFO:Creating metrics dataframe
2023-05-20 16:24:27,407:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 16:24:27,407:INFO:Total runtime is 0.9988885203997295 minutes
2023-05-20 16:24:27,410:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:27,410:INFO:Initializing create_model()
2023-05-20 16:24:27,410:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:27,410:INFO:Checking exceptions
2023-05-20 16:24:27,410:INFO:Importing libraries
2023-05-20 16:24:27,411:INFO:Copying training dataset
2023-05-20 16:24:27,418:INFO:Defining folds
2023-05-20 16:24:27,418:INFO:Declaring metric variables
2023-05-20 16:24:27,421:INFO:Importing untrained model
2023-05-20 16:24:27,425:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:24:27,429:INFO:Starting cross validation
2023-05-20 16:24:27,429:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:30,429:INFO:Calculating mean and std
2023-05-20 16:24:30,431:INFO:Creating metrics dataframe
2023-05-20 16:24:30,747:INFO:Uploading results into container
2023-05-20 16:24:30,748:INFO:Uploading model into container now
2023-05-20 16:24:30,748:INFO:_master_model_container: 38
2023-05-20 16:24:30,748:INFO:_display_container: 3
2023-05-20 16:24:30,749:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:24:30,749:INFO:create_model() successfully completed......................................
2023-05-20 16:24:30,846:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:30,846:INFO:Creating metrics dataframe
2023-05-20 16:24:30,857:INFO:Initializing CatBoost Regressor
2023-05-20 16:24:30,858:INFO:Total runtime is 1.0563895821571352 minutes
2023-05-20 16:24:30,860:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:30,861:INFO:Initializing create_model()
2023-05-20 16:24:30,861:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:30,861:INFO:Checking exceptions
2023-05-20 16:24:30,861:INFO:Importing libraries
2023-05-20 16:24:30,861:INFO:Copying training dataset
2023-05-20 16:24:30,871:INFO:Defining folds
2023-05-20 16:24:30,871:INFO:Declaring metric variables
2023-05-20 16:24:30,874:INFO:Importing untrained model
2023-05-20 16:24:30,877:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:24:30,884:INFO:Starting cross validation
2023-05-20 16:24:30,886:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:33,961:INFO:Calculating mean and std
2023-05-20 16:24:33,963:INFO:Creating metrics dataframe
2023-05-20 16:24:34,276:INFO:Uploading results into container
2023-05-20 16:24:34,277:INFO:Uploading model into container now
2023-05-20 16:24:34,278:INFO:_master_model_container: 39
2023-05-20 16:24:34,278:INFO:_display_container: 3
2023-05-20 16:24:34,278:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF5C28280>
2023-05-20 16:24:34,278:INFO:create_model() successfully completed......................................
2023-05-20 16:24:34,338:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:34,339:INFO:Creating metrics dataframe
2023-05-20 16:24:34,351:INFO:Initializing Dummy Regressor
2023-05-20 16:24:34,351:INFO:Total runtime is 1.1146285017331443 minutes
2023-05-20 16:24:34,354:INFO:SubProcess create_model() called ==================================
2023-05-20 16:24:34,354:INFO:Initializing create_model()
2023-05-20 16:24:34,354:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BD82760E0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:34,354:INFO:Checking exceptions
2023-05-20 16:24:34,354:INFO:Importing libraries
2023-05-20 16:24:34,355:INFO:Copying training dataset
2023-05-20 16:24:34,364:INFO:Defining folds
2023-05-20 16:24:34,364:INFO:Declaring metric variables
2023-05-20 16:24:34,367:INFO:Importing untrained model
2023-05-20 16:24:34,370:INFO:Dummy Regressor Imported successfully
2023-05-20 16:24:34,376:INFO:Starting cross validation
2023-05-20 16:24:34,378:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:24:37,321:INFO:Calculating mean and std
2023-05-20 16:24:37,323:INFO:Creating metrics dataframe
2023-05-20 16:24:37,625:INFO:Uploading results into container
2023-05-20 16:24:37,626:INFO:Uploading model into container now
2023-05-20 16:24:37,627:INFO:_master_model_container: 40
2023-05-20 16:24:37,627:INFO:_display_container: 3
2023-05-20 16:24:37,627:INFO:DummyRegressor()
2023-05-20 16:24:37,627:INFO:create_model() successfully completed......................................
2023-05-20 16:24:37,691:INFO:SubProcess create_model() end ==================================
2023-05-20 16:24:37,691:INFO:Creating metrics dataframe
2023-05-20 16:24:37,709:INFO:Initializing create_model()
2023-05-20 16:24:37,709:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BF5C28280>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:24:37,709:INFO:Checking exceptions
2023-05-20 16:24:37,711:INFO:Importing libraries
2023-05-20 16:24:37,711:INFO:Copying training dataset
2023-05-20 16:24:37,719:INFO:Defining folds
2023-05-20 16:24:37,719:INFO:Declaring metric variables
2023-05-20 16:24:37,719:INFO:Importing untrained model
2023-05-20 16:24:37,719:INFO:Declaring custom model
2023-05-20 16:24:37,719:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:24:37,721:INFO:Cross validation set to False
2023-05-20 16:24:37,721:INFO:Fitting Model
2023-05-20 16:24:38,037:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF5C2AA70>
2023-05-20 16:24:38,037:INFO:create_model() successfully completed......................................
2023-05-20 16:24:38,133:INFO:_master_model_container: 40
2023-05-20 16:24:38,133:INFO:_display_container: 3
2023-05-20 16:24:38,133:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BF5C2AA70>
2023-05-20 16:24:38,133:INFO:compare_models() successfully completed......................................
2023-05-20 16:25:24,019:INFO:Initializing compare_models()
2023-05-20 16:25:24,019:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=auc, n_select=10, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'auc', 'n_select': 10, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:25:24,019:INFO:Checking exceptions
2023-05-20 16:25:31,067:INFO:Initializing compare_models()
2023-05-20 16:25:31,067:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=auc, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'auc', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:25:31,067:INFO:Checking exceptions
2023-05-20 16:26:18,768:INFO:Initializing compare_models()
2023-05-20 16:26:18,768:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=MAPE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAPE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:26:18,768:INFO:Checking exceptions
2023-05-20 16:26:18,772:INFO:Preparing display monitor
2023-05-20 16:26:18,837:INFO:Initializing Linear Regression
2023-05-20 16:26:18,837:INFO:Total runtime is 0.0 minutes
2023-05-20 16:26:18,841:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:18,842:INFO:Initializing create_model()
2023-05-20 16:26:18,842:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:18,842:INFO:Checking exceptions
2023-05-20 16:26:18,842:INFO:Importing libraries
2023-05-20 16:26:18,842:INFO:Copying training dataset
2023-05-20 16:26:18,858:INFO:Defining folds
2023-05-20 16:26:18,858:INFO:Declaring metric variables
2023-05-20 16:26:18,862:INFO:Importing untrained model
2023-05-20 16:26:18,866:INFO:Linear Regression Imported successfully
2023-05-20 16:26:18,872:INFO:Starting cross validation
2023-05-20 16:26:18,875:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:21,998:INFO:Calculating mean and std
2023-05-20 16:26:22,000:INFO:Creating metrics dataframe
2023-05-20 16:26:22,321:INFO:Uploading results into container
2023-05-20 16:26:22,323:INFO:Uploading model into container now
2023-05-20 16:26:22,323:INFO:_master_model_container: 41
2023-05-20 16:26:22,323:INFO:_display_container: 4
2023-05-20 16:26:22,323:INFO:LinearRegression(n_jobs=-1)
2023-05-20 16:26:22,324:INFO:create_model() successfully completed......................................
2023-05-20 16:26:22,436:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:22,437:INFO:Creating metrics dataframe
2023-05-20 16:26:22,444:INFO:Initializing Lasso Regression
2023-05-20 16:26:22,445:INFO:Total runtime is 0.06013989845911662 minutes
2023-05-20 16:26:22,447:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:22,448:INFO:Initializing create_model()
2023-05-20 16:26:22,448:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:22,449:INFO:Checking exceptions
2023-05-20 16:26:22,449:INFO:Importing libraries
2023-05-20 16:26:22,449:INFO:Copying training dataset
2023-05-20 16:26:22,460:INFO:Defining folds
2023-05-20 16:26:22,460:INFO:Declaring metric variables
2023-05-20 16:26:22,464:INFO:Importing untrained model
2023-05-20 16:26:22,468:INFO:Lasso Regression Imported successfully
2023-05-20 16:26:22,474:INFO:Starting cross validation
2023-05-20 16:26:22,476:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:25,578:INFO:Calculating mean and std
2023-05-20 16:26:25,580:INFO:Creating metrics dataframe
2023-05-20 16:26:25,902:INFO:Uploading results into container
2023-05-20 16:26:25,903:INFO:Uploading model into container now
2023-05-20 16:26:25,903:INFO:_master_model_container: 42
2023-05-20 16:26:25,903:INFO:_display_container: 4
2023-05-20 16:26:25,904:INFO:Lasso(random_state=5308)
2023-05-20 16:26:25,904:INFO:create_model() successfully completed......................................
2023-05-20 16:26:26,013:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:26,013:INFO:Creating metrics dataframe
2023-05-20 16:26:26,022:INFO:Initializing Ridge Regression
2023-05-20 16:26:26,022:INFO:Total runtime is 0.11975456476211548 minutes
2023-05-20 16:26:26,027:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:26,028:INFO:Initializing create_model()
2023-05-20 16:26:26,028:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:26,028:INFO:Checking exceptions
2023-05-20 16:26:26,028:INFO:Importing libraries
2023-05-20 16:26:26,028:INFO:Copying training dataset
2023-05-20 16:26:26,038:INFO:Defining folds
2023-05-20 16:26:26,038:INFO:Declaring metric variables
2023-05-20 16:26:26,042:INFO:Importing untrained model
2023-05-20 16:26:26,045:INFO:Ridge Regression Imported successfully
2023-05-20 16:26:26,052:INFO:Starting cross validation
2023-05-20 16:26:26,053:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:29,239:INFO:Calculating mean and std
2023-05-20 16:26:29,242:INFO:Creating metrics dataframe
2023-05-20 16:26:29,567:INFO:Uploading results into container
2023-05-20 16:26:29,567:INFO:Uploading model into container now
2023-05-20 16:26:29,568:INFO:_master_model_container: 43
2023-05-20 16:26:29,568:INFO:_display_container: 4
2023-05-20 16:26:29,568:INFO:Ridge(random_state=5308)
2023-05-20 16:26:29,568:INFO:create_model() successfully completed......................................
2023-05-20 16:26:29,675:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:29,675:INFO:Creating metrics dataframe
2023-05-20 16:26:29,684:INFO:Initializing Elastic Net
2023-05-20 16:26:29,684:INFO:Total runtime is 0.18079068660736083 minutes
2023-05-20 16:26:29,687:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:29,687:INFO:Initializing create_model()
2023-05-20 16:26:29,687:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:29,687:INFO:Checking exceptions
2023-05-20 16:26:29,687:INFO:Importing libraries
2023-05-20 16:26:29,688:INFO:Copying training dataset
2023-05-20 16:26:29,699:INFO:Defining folds
2023-05-20 16:26:29,699:INFO:Declaring metric variables
2023-05-20 16:26:29,702:INFO:Importing untrained model
2023-05-20 16:26:29,705:INFO:Elastic Net Imported successfully
2023-05-20 16:26:29,711:INFO:Starting cross validation
2023-05-20 16:26:29,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:32,874:INFO:Calculating mean and std
2023-05-20 16:26:32,875:INFO:Creating metrics dataframe
2023-05-20 16:26:33,209:INFO:Uploading results into container
2023-05-20 16:26:33,210:INFO:Uploading model into container now
2023-05-20 16:26:33,210:INFO:_master_model_container: 44
2023-05-20 16:26:33,210:INFO:_display_container: 4
2023-05-20 16:26:33,210:INFO:ElasticNet(random_state=5308)
2023-05-20 16:26:33,210:INFO:create_model() successfully completed......................................
2023-05-20 16:26:33,318:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:33,318:INFO:Creating metrics dataframe
2023-05-20 16:26:33,327:INFO:Initializing Least Angle Regression
2023-05-20 16:26:33,327:INFO:Total runtime is 0.2414983550707499 minutes
2023-05-20 16:26:33,330:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:33,330:INFO:Initializing create_model()
2023-05-20 16:26:33,331:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:33,331:INFO:Checking exceptions
2023-05-20 16:26:33,331:INFO:Importing libraries
2023-05-20 16:26:33,331:INFO:Copying training dataset
2023-05-20 16:26:33,342:INFO:Defining folds
2023-05-20 16:26:33,342:INFO:Declaring metric variables
2023-05-20 16:26:33,345:INFO:Importing untrained model
2023-05-20 16:26:33,350:INFO:Least Angle Regression Imported successfully
2023-05-20 16:26:33,356:INFO:Starting cross validation
2023-05-20 16:26:33,357:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:33,584:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,585:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,585:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,593:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,593:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,595:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,616:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,616:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,617:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,635:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 16:26:33,638:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 16:26:33,640:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,640:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,641:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,643:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,645:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,645:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:33,661:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,661:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:26:33,661:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:26:36,478:INFO:Calculating mean and std
2023-05-20 16:26:36,479:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 16:26:36,480:INFO:Creating metrics dataframe
2023-05-20 16:26:36,805:INFO:Uploading results into container
2023-05-20 16:26:36,806:INFO:Uploading model into container now
2023-05-20 16:26:36,807:INFO:_master_model_container: 45
2023-05-20 16:26:36,807:INFO:_display_container: 4
2023-05-20 16:26:36,808:INFO:Lars(random_state=5308)
2023-05-20 16:26:36,808:INFO:create_model() successfully completed......................................
2023-05-20 16:26:36,917:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:36,917:INFO:Creating metrics dataframe
2023-05-20 16:26:36,927:INFO:Initializing Lasso Least Angle Regression
2023-05-20 16:26:36,927:INFO:Total runtime is 0.30149883826573687 minutes
2023-05-20 16:26:36,930:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:36,930:INFO:Initializing create_model()
2023-05-20 16:26:36,930:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:36,930:INFO:Checking exceptions
2023-05-20 16:26:36,930:INFO:Importing libraries
2023-05-20 16:26:36,930:INFO:Copying training dataset
2023-05-20 16:26:36,943:INFO:Defining folds
2023-05-20 16:26:36,943:INFO:Declaring metric variables
2023-05-20 16:26:36,946:INFO:Importing untrained model
2023-05-20 16:26:36,950:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 16:26:36,956:INFO:Starting cross validation
2023-05-20 16:26:36,958:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:37,083:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,092:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,094:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,121:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,121:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,147:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,171:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,185:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:37,195:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:26:40,170:INFO:Calculating mean and std
2023-05-20 16:26:40,171:INFO:Creating metrics dataframe
2023-05-20 16:26:40,499:INFO:Uploading results into container
2023-05-20 16:26:40,500:INFO:Uploading model into container now
2023-05-20 16:26:40,500:INFO:_master_model_container: 46
2023-05-20 16:26:40,500:INFO:_display_container: 4
2023-05-20 16:26:40,501:INFO:LassoLars(random_state=5308)
2023-05-20 16:26:40,501:INFO:create_model() successfully completed......................................
2023-05-20 16:26:40,610:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:40,611:INFO:Creating metrics dataframe
2023-05-20 16:26:40,620:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 16:26:40,620:INFO:Total runtime is 0.3630565325419108 minutes
2023-05-20 16:26:40,623:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:40,624:INFO:Initializing create_model()
2023-05-20 16:26:40,624:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:40,624:INFO:Checking exceptions
2023-05-20 16:26:40,624:INFO:Importing libraries
2023-05-20 16:26:40,624:INFO:Copying training dataset
2023-05-20 16:26:40,635:INFO:Defining folds
2023-05-20 16:26:40,635:INFO:Declaring metric variables
2023-05-20 16:26:40,638:INFO:Importing untrained model
2023-05-20 16:26:40,643:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 16:26:40,649:INFO:Starting cross validation
2023-05-20 16:26:40,651:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:40,771:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,779:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,791:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,827:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,836:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,838:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,850:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,861:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:40,873:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:26:43,791:INFO:Calculating mean and std
2023-05-20 16:26:43,793:INFO:Creating metrics dataframe
2023-05-20 16:26:44,113:INFO:Uploading results into container
2023-05-20 16:26:44,114:INFO:Uploading model into container now
2023-05-20 16:26:44,115:INFO:_master_model_container: 47
2023-05-20 16:26:44,115:INFO:_display_container: 4
2023-05-20 16:26:44,116:INFO:OrthogonalMatchingPursuit()
2023-05-20 16:26:44,116:INFO:create_model() successfully completed......................................
2023-05-20 16:26:44,223:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:44,223:INFO:Creating metrics dataframe
2023-05-20 16:26:44,233:INFO:Initializing Bayesian Ridge
2023-05-20 16:26:44,234:INFO:Total runtime is 0.42328647772471106 minutes
2023-05-20 16:26:44,237:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:44,237:INFO:Initializing create_model()
2023-05-20 16:26:44,237:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:44,237:INFO:Checking exceptions
2023-05-20 16:26:44,237:INFO:Importing libraries
2023-05-20 16:26:44,237:INFO:Copying training dataset
2023-05-20 16:26:44,250:INFO:Defining folds
2023-05-20 16:26:44,250:INFO:Declaring metric variables
2023-05-20 16:26:44,253:INFO:Importing untrained model
2023-05-20 16:26:44,257:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:26:44,264:INFO:Starting cross validation
2023-05-20 16:26:44,266:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:47,507:INFO:Calculating mean and std
2023-05-20 16:26:47,509:INFO:Creating metrics dataframe
2023-05-20 16:26:47,834:INFO:Uploading results into container
2023-05-20 16:26:47,835:INFO:Uploading model into container now
2023-05-20 16:26:47,836:INFO:_master_model_container: 48
2023-05-20 16:26:47,836:INFO:_display_container: 4
2023-05-20 16:26:47,836:INFO:BayesianRidge()
2023-05-20 16:26:47,836:INFO:create_model() successfully completed......................................
2023-05-20 16:26:47,943:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:47,943:INFO:Creating metrics dataframe
2023-05-20 16:26:47,954:INFO:Initializing Passive Aggressive Regressor
2023-05-20 16:26:47,954:INFO:Total runtime is 0.4852851867675781 minutes
2023-05-20 16:26:47,958:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:47,958:INFO:Initializing create_model()
2023-05-20 16:26:47,958:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:47,958:INFO:Checking exceptions
2023-05-20 16:26:47,958:INFO:Importing libraries
2023-05-20 16:26:47,958:INFO:Copying training dataset
2023-05-20 16:26:47,969:INFO:Defining folds
2023-05-20 16:26:47,970:INFO:Declaring metric variables
2023-05-20 16:26:47,974:INFO:Importing untrained model
2023-05-20 16:26:47,978:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 16:26:47,990:INFO:Starting cross validation
2023-05-20 16:26:47,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:51,228:INFO:Calculating mean and std
2023-05-20 16:26:51,230:INFO:Creating metrics dataframe
2023-05-20 16:26:51,577:INFO:Uploading results into container
2023-05-20 16:26:51,579:INFO:Uploading model into container now
2023-05-20 16:26:51,579:INFO:_master_model_container: 49
2023-05-20 16:26:51,579:INFO:_display_container: 4
2023-05-20 16:26:51,579:INFO:PassiveAggressiveRegressor(random_state=5308)
2023-05-20 16:26:51,579:INFO:create_model() successfully completed......................................
2023-05-20 16:26:51,694:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:51,694:INFO:Creating metrics dataframe
2023-05-20 16:26:51,704:INFO:Initializing Huber Regressor
2023-05-20 16:26:51,704:INFO:Total runtime is 0.54779448111852 minutes
2023-05-20 16:26:51,708:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:51,709:INFO:Initializing create_model()
2023-05-20 16:26:51,709:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:51,709:INFO:Checking exceptions
2023-05-20 16:26:51,709:INFO:Importing libraries
2023-05-20 16:26:51,709:INFO:Copying training dataset
2023-05-20 16:26:51,719:INFO:Defining folds
2023-05-20 16:26:51,720:INFO:Declaring metric variables
2023-05-20 16:26:51,723:INFO:Importing untrained model
2023-05-20 16:26:51,727:INFO:Huber Regressor Imported successfully
2023-05-20 16:26:51,733:INFO:Starting cross validation
2023-05-20 16:26:51,735:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:54,844:INFO:Calculating mean and std
2023-05-20 16:26:54,846:INFO:Creating metrics dataframe
2023-05-20 16:26:55,179:INFO:Uploading results into container
2023-05-20 16:26:55,180:INFO:Uploading model into container now
2023-05-20 16:26:55,181:INFO:_master_model_container: 50
2023-05-20 16:26:55,181:INFO:_display_container: 4
2023-05-20 16:26:55,181:INFO:HuberRegressor()
2023-05-20 16:26:55,181:INFO:create_model() successfully completed......................................
2023-05-20 16:26:55,279:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:55,279:INFO:Creating metrics dataframe
2023-05-20 16:26:55,289:INFO:Initializing K Neighbors Regressor
2023-05-20 16:26:55,290:INFO:Total runtime is 0.6075504422187804 minutes
2023-05-20 16:26:55,293:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:55,294:INFO:Initializing create_model()
2023-05-20 16:26:55,294:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:55,294:INFO:Checking exceptions
2023-05-20 16:26:55,294:INFO:Importing libraries
2023-05-20 16:26:55,294:INFO:Copying training dataset
2023-05-20 16:26:55,305:INFO:Defining folds
2023-05-20 16:26:55,305:INFO:Declaring metric variables
2023-05-20 16:26:55,310:INFO:Importing untrained model
2023-05-20 16:26:55,313:INFO:K Neighbors Regressor Imported successfully
2023-05-20 16:26:55,319:INFO:Starting cross validation
2023-05-20 16:26:55,320:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:26:58,524:INFO:Calculating mean and std
2023-05-20 16:26:58,526:INFO:Creating metrics dataframe
2023-05-20 16:26:58,893:INFO:Uploading results into container
2023-05-20 16:26:58,894:INFO:Uploading model into container now
2023-05-20 16:26:58,895:INFO:_master_model_container: 51
2023-05-20 16:26:58,895:INFO:_display_container: 4
2023-05-20 16:26:58,895:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 16:26:58,896:INFO:create_model() successfully completed......................................
2023-05-20 16:26:59,030:INFO:SubProcess create_model() end ==================================
2023-05-20 16:26:59,030:INFO:Creating metrics dataframe
2023-05-20 16:26:59,043:INFO:Initializing Decision Tree Regressor
2023-05-20 16:26:59,043:INFO:Total runtime is 0.6701084613800048 minutes
2023-05-20 16:26:59,048:INFO:SubProcess create_model() called ==================================
2023-05-20 16:26:59,048:INFO:Initializing create_model()
2023-05-20 16:26:59,048:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:26:59,049:INFO:Checking exceptions
2023-05-20 16:26:59,049:INFO:Importing libraries
2023-05-20 16:26:59,049:INFO:Copying training dataset
2023-05-20 16:26:59,064:INFO:Defining folds
2023-05-20 16:26:59,064:INFO:Declaring metric variables
2023-05-20 16:26:59,068:INFO:Importing untrained model
2023-05-20 16:26:59,073:INFO:Decision Tree Regressor Imported successfully
2023-05-20 16:26:59,080:INFO:Starting cross validation
2023-05-20 16:26:59,082:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:02,367:INFO:Calculating mean and std
2023-05-20 16:27:02,369:INFO:Creating metrics dataframe
2023-05-20 16:27:02,683:INFO:Uploading results into container
2023-05-20 16:27:02,685:INFO:Uploading model into container now
2023-05-20 16:27:02,685:INFO:_master_model_container: 52
2023-05-20 16:27:02,686:INFO:_display_container: 4
2023-05-20 16:27:02,686:INFO:DecisionTreeRegressor(random_state=5308)
2023-05-20 16:27:02,686:INFO:create_model() successfully completed......................................
2023-05-20 16:27:02,795:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:02,795:INFO:Creating metrics dataframe
2023-05-20 16:27:02,806:INFO:Initializing Random Forest Regressor
2023-05-20 16:27:02,806:INFO:Total runtime is 0.7328187147776285 minutes
2023-05-20 16:27:02,810:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:02,810:INFO:Initializing create_model()
2023-05-20 16:27:02,810:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:02,810:INFO:Checking exceptions
2023-05-20 16:27:02,811:INFO:Importing libraries
2023-05-20 16:27:02,811:INFO:Copying training dataset
2023-05-20 16:27:02,822:INFO:Defining folds
2023-05-20 16:27:02,822:INFO:Declaring metric variables
2023-05-20 16:27:02,826:INFO:Importing untrained model
2023-05-20 16:27:02,830:INFO:Random Forest Regressor Imported successfully
2023-05-20 16:27:02,834:INFO:Starting cross validation
2023-05-20 16:27:02,836:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:06,187:INFO:Calculating mean and std
2023-05-20 16:27:06,189:INFO:Creating metrics dataframe
2023-05-20 16:27:06,497:INFO:Uploading results into container
2023-05-20 16:27:06,499:INFO:Uploading model into container now
2023-05-20 16:27:06,499:INFO:_master_model_container: 53
2023-05-20 16:27:06,499:INFO:_display_container: 4
2023-05-20 16:27:06,500:INFO:RandomForestRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:27:06,500:INFO:create_model() successfully completed......................................
2023-05-20 16:27:06,610:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:06,611:INFO:Creating metrics dataframe
2023-05-20 16:27:06,621:INFO:Initializing Extra Trees Regressor
2023-05-20 16:27:06,621:INFO:Total runtime is 0.7964110374450682 minutes
2023-05-20 16:27:06,624:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:06,625:INFO:Initializing create_model()
2023-05-20 16:27:06,625:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:06,625:INFO:Checking exceptions
2023-05-20 16:27:06,625:INFO:Importing libraries
2023-05-20 16:27:06,625:INFO:Copying training dataset
2023-05-20 16:27:06,633:INFO:Defining folds
2023-05-20 16:27:06,633:INFO:Declaring metric variables
2023-05-20 16:27:06,636:INFO:Importing untrained model
2023-05-20 16:27:06,641:INFO:Extra Trees Regressor Imported successfully
2023-05-20 16:27:06,646:INFO:Starting cross validation
2023-05-20 16:27:06,648:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:09,934:INFO:Calculating mean and std
2023-05-20 16:27:09,936:INFO:Creating metrics dataframe
2023-05-20 16:27:10,252:INFO:Uploading results into container
2023-05-20 16:27:10,254:INFO:Uploading model into container now
2023-05-20 16:27:10,254:INFO:_master_model_container: 54
2023-05-20 16:27:10,254:INFO:_display_container: 4
2023-05-20 16:27:10,254:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:27:10,255:INFO:create_model() successfully completed......................................
2023-05-20 16:27:10,360:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:10,360:INFO:Creating metrics dataframe
2023-05-20 16:27:10,370:INFO:Initializing AdaBoost Regressor
2023-05-20 16:27:10,370:INFO:Total runtime is 0.8588897029558816 minutes
2023-05-20 16:27:10,374:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:10,374:INFO:Initializing create_model()
2023-05-20 16:27:10,374:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:10,374:INFO:Checking exceptions
2023-05-20 16:27:10,374:INFO:Importing libraries
2023-05-20 16:27:10,374:INFO:Copying training dataset
2023-05-20 16:27:10,384:INFO:Defining folds
2023-05-20 16:27:10,384:INFO:Declaring metric variables
2023-05-20 16:27:10,386:INFO:Importing untrained model
2023-05-20 16:27:10,389:INFO:AdaBoost Regressor Imported successfully
2023-05-20 16:27:10,396:INFO:Starting cross validation
2023-05-20 16:27:10,397:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:13,417:INFO:Calculating mean and std
2023-05-20 16:27:13,418:INFO:Creating metrics dataframe
2023-05-20 16:27:13,725:INFO:Uploading results into container
2023-05-20 16:27:13,726:INFO:Uploading model into container now
2023-05-20 16:27:13,726:INFO:_master_model_container: 55
2023-05-20 16:27:13,726:INFO:_display_container: 4
2023-05-20 16:27:13,726:INFO:AdaBoostRegressor(random_state=5308)
2023-05-20 16:27:13,726:INFO:create_model() successfully completed......................................
2023-05-20 16:27:13,820:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:13,820:INFO:Creating metrics dataframe
2023-05-20 16:27:13,831:INFO:Initializing Gradient Boosting Regressor
2023-05-20 16:27:13,831:INFO:Total runtime is 0.9165662407875059 minutes
2023-05-20 16:27:13,834:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:13,834:INFO:Initializing create_model()
2023-05-20 16:27:13,834:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:13,834:INFO:Checking exceptions
2023-05-20 16:27:13,834:INFO:Importing libraries
2023-05-20 16:27:13,834:INFO:Copying training dataset
2023-05-20 16:27:13,846:INFO:Defining folds
2023-05-20 16:27:13,846:INFO:Declaring metric variables
2023-05-20 16:27:13,849:INFO:Importing untrained model
2023-05-20 16:27:13,852:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:27:13,858:INFO:Starting cross validation
2023-05-20 16:27:13,860:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:16,836:INFO:Calculating mean and std
2023-05-20 16:27:16,838:INFO:Creating metrics dataframe
2023-05-20 16:27:17,154:INFO:Uploading results into container
2023-05-20 16:27:17,155:INFO:Uploading model into container now
2023-05-20 16:27:17,156:INFO:_master_model_container: 56
2023-05-20 16:27:17,156:INFO:_display_container: 4
2023-05-20 16:27:17,156:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:27:17,157:INFO:create_model() successfully completed......................................
2023-05-20 16:27:17,252:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:17,252:INFO:Creating metrics dataframe
2023-05-20 16:27:17,262:INFO:Initializing Extreme Gradient Boosting
2023-05-20 16:27:17,262:INFO:Total runtime is 0.9737609148025511 minutes
2023-05-20 16:27:17,266:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:17,266:INFO:Initializing create_model()
2023-05-20 16:27:17,266:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:17,266:INFO:Checking exceptions
2023-05-20 16:27:17,266:INFO:Importing libraries
2023-05-20 16:27:17,266:INFO:Copying training dataset
2023-05-20 16:27:17,276:INFO:Defining folds
2023-05-20 16:27:17,276:INFO:Declaring metric variables
2023-05-20 16:27:17,280:INFO:Importing untrained model
2023-05-20 16:27:17,284:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 16:27:17,291:INFO:Starting cross validation
2023-05-20 16:27:17,292:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:20,262:INFO:Calculating mean and std
2023-05-20 16:27:20,264:INFO:Creating metrics dataframe
2023-05-20 16:27:20,580:INFO:Uploading results into container
2023-05-20 16:27:20,581:INFO:Uploading model into container now
2023-05-20 16:27:20,582:INFO:_master_model_container: 57
2023-05-20 16:27:20,583:INFO:_display_container: 4
2023-05-20 16:27:20,584:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5308, ...)
2023-05-20 16:27:20,585:INFO:create_model() successfully completed......................................
2023-05-20 16:27:20,677:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:20,677:INFO:Creating metrics dataframe
2023-05-20 16:27:20,689:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 16:27:20,689:INFO:Total runtime is 1.0308761994043985 minutes
2023-05-20 16:27:20,692:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:20,692:INFO:Initializing create_model()
2023-05-20 16:27:20,693:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:20,693:INFO:Checking exceptions
2023-05-20 16:27:20,693:INFO:Importing libraries
2023-05-20 16:27:20,693:INFO:Copying training dataset
2023-05-20 16:27:20,701:INFO:Defining folds
2023-05-20 16:27:20,701:INFO:Declaring metric variables
2023-05-20 16:27:20,705:INFO:Importing untrained model
2023-05-20 16:27:20,709:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:27:20,714:INFO:Starting cross validation
2023-05-20 16:27:20,716:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:23,657:INFO:Calculating mean and std
2023-05-20 16:27:23,659:INFO:Creating metrics dataframe
2023-05-20 16:27:23,976:INFO:Uploading results into container
2023-05-20 16:27:23,977:INFO:Uploading model into container now
2023-05-20 16:27:23,977:INFO:_master_model_container: 58
2023-05-20 16:27:23,977:INFO:_display_container: 4
2023-05-20 16:27:23,978:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:27:23,978:INFO:create_model() successfully completed......................................
2023-05-20 16:27:24,070:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:24,071:INFO:Creating metrics dataframe
2023-05-20 16:27:24,082:INFO:Initializing CatBoost Regressor
2023-05-20 16:27:24,082:INFO:Total runtime is 1.0874197284380593 minutes
2023-05-20 16:27:24,085:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:24,086:INFO:Initializing create_model()
2023-05-20 16:27:24,086:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:24,086:INFO:Checking exceptions
2023-05-20 16:27:24,086:INFO:Importing libraries
2023-05-20 16:27:24,086:INFO:Copying training dataset
2023-05-20 16:27:24,095:INFO:Defining folds
2023-05-20 16:27:24,095:INFO:Declaring metric variables
2023-05-20 16:27:24,099:INFO:Importing untrained model
2023-05-20 16:27:24,103:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:27:24,109:INFO:Starting cross validation
2023-05-20 16:27:24,110:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:27,110:INFO:Calculating mean and std
2023-05-20 16:27:27,112:INFO:Creating metrics dataframe
2023-05-20 16:27:27,426:INFO:Uploading results into container
2023-05-20 16:27:27,427:INFO:Uploading model into container now
2023-05-20 16:27:27,427:INFO:_master_model_container: 59
2023-05-20 16:27:27,427:INFO:_display_container: 4
2023-05-20 16:27:27,427:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFAF081F0>
2023-05-20 16:27:27,428:INFO:create_model() successfully completed......................................
2023-05-20 16:27:27,524:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:27,524:INFO:Creating metrics dataframe
2023-05-20 16:27:27,536:INFO:Initializing Dummy Regressor
2023-05-20 16:27:27,536:INFO:Total runtime is 1.1449831287066141 minutes
2023-05-20 16:27:27,539:INFO:SubProcess create_model() called ==================================
2023-05-20 16:27:27,539:INFO:Initializing create_model()
2023-05-20 16:27:27,539:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBED1D50>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:27,540:INFO:Checking exceptions
2023-05-20 16:27:27,540:INFO:Importing libraries
2023-05-20 16:27:27,540:INFO:Copying training dataset
2023-05-20 16:27:27,548:INFO:Defining folds
2023-05-20 16:27:27,548:INFO:Declaring metric variables
2023-05-20 16:27:27,551:INFO:Importing untrained model
2023-05-20 16:27:27,554:INFO:Dummy Regressor Imported successfully
2023-05-20 16:27:27,559:INFO:Starting cross validation
2023-05-20 16:27:27,561:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:27:30,516:INFO:Calculating mean and std
2023-05-20 16:27:30,518:INFO:Creating metrics dataframe
2023-05-20 16:27:30,821:INFO:Uploading results into container
2023-05-20 16:27:30,822:INFO:Uploading model into container now
2023-05-20 16:27:30,822:INFO:_master_model_container: 60
2023-05-20 16:27:30,822:INFO:_display_container: 4
2023-05-20 16:27:30,822:INFO:DummyRegressor()
2023-05-20 16:27:30,822:INFO:create_model() successfully completed......................................
2023-05-20 16:27:30,914:INFO:SubProcess create_model() end ==================================
2023-05-20 16:27:30,914:INFO:Creating metrics dataframe
2023-05-20 16:27:30,935:INFO:Initializing create_model()
2023-05-20 16:27:30,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BFAF081F0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:27:30,935:INFO:Checking exceptions
2023-05-20 16:27:30,936:INFO:Importing libraries
2023-05-20 16:27:30,936:INFO:Copying training dataset
2023-05-20 16:27:30,944:INFO:Defining folds
2023-05-20 16:27:30,945:INFO:Declaring metric variables
2023-05-20 16:27:30,945:INFO:Importing untrained model
2023-05-20 16:27:30,945:INFO:Declaring custom model
2023-05-20 16:27:30,945:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:27:30,946:INFO:Cross validation set to False
2023-05-20 16:27:30,946:INFO:Fitting Model
2023-05-20 16:27:31,266:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFAEFFEE0>
2023-05-20 16:27:31,266:INFO:create_model() successfully completed......................................
2023-05-20 16:27:31,391:INFO:_master_model_container: 60
2023-05-20 16:27:31,391:INFO:_display_container: 4
2023-05-20 16:27:31,391:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFAEFFEE0>
2023-05-20 16:27:31,391:INFO:compare_models() successfully completed......................................
2023-05-20 16:29:19,486:INFO:Initializing compare_models()
2023-05-20 16:29:19,486:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:29:19,486:INFO:Checking exceptions
2023-05-20 16:29:19,493:INFO:Preparing display monitor
2023-05-20 16:29:19,538:INFO:Initializing Linear Regression
2023-05-20 16:29:19,538:INFO:Total runtime is 0.0 minutes
2023-05-20 16:29:19,542:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:19,543:INFO:Initializing create_model()
2023-05-20 16:29:19,543:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:19,543:INFO:Checking exceptions
2023-05-20 16:29:19,543:INFO:Importing libraries
2023-05-20 16:29:19,543:INFO:Copying training dataset
2023-05-20 16:29:19,555:INFO:Defining folds
2023-05-20 16:29:19,555:INFO:Declaring metric variables
2023-05-20 16:29:19,559:INFO:Importing untrained model
2023-05-20 16:29:19,563:INFO:Linear Regression Imported successfully
2023-05-20 16:29:19,570:INFO:Starting cross validation
2023-05-20 16:29:19,572:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:22,860:INFO:Calculating mean and std
2023-05-20 16:29:22,862:INFO:Creating metrics dataframe
2023-05-20 16:29:23,167:INFO:Uploading results into container
2023-05-20 16:29:23,168:INFO:Uploading model into container now
2023-05-20 16:29:23,168:INFO:_master_model_container: 61
2023-05-20 16:29:23,169:INFO:_display_container: 5
2023-05-20 16:29:23,169:INFO:LinearRegression(n_jobs=-1)
2023-05-20 16:29:23,169:INFO:create_model() successfully completed......................................
2023-05-20 16:29:23,274:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:23,275:INFO:Creating metrics dataframe
2023-05-20 16:29:23,283:INFO:Initializing Lasso Regression
2023-05-20 16:29:23,283:INFO:Total runtime is 0.06241602102915446 minutes
2023-05-20 16:29:23,285:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:23,286:INFO:Initializing create_model()
2023-05-20 16:29:23,286:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:23,286:INFO:Checking exceptions
2023-05-20 16:29:23,286:INFO:Importing libraries
2023-05-20 16:29:23,286:INFO:Copying training dataset
2023-05-20 16:29:23,295:INFO:Defining folds
2023-05-20 16:29:23,296:INFO:Declaring metric variables
2023-05-20 16:29:23,300:INFO:Importing untrained model
2023-05-20 16:29:23,303:INFO:Lasso Regression Imported successfully
2023-05-20 16:29:23,308:INFO:Starting cross validation
2023-05-20 16:29:23,310:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:26,294:INFO:Calculating mean and std
2023-05-20 16:29:26,296:INFO:Creating metrics dataframe
2023-05-20 16:29:26,604:INFO:Uploading results into container
2023-05-20 16:29:26,605:INFO:Uploading model into container now
2023-05-20 16:29:26,606:INFO:_master_model_container: 62
2023-05-20 16:29:26,606:INFO:_display_container: 5
2023-05-20 16:29:26,606:INFO:Lasso(random_state=5308)
2023-05-20 16:29:26,606:INFO:create_model() successfully completed......................................
2023-05-20 16:29:26,703:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:26,703:INFO:Creating metrics dataframe
2023-05-20 16:29:26,711:INFO:Initializing Ridge Regression
2023-05-20 16:29:26,711:INFO:Total runtime is 0.11955613295237223 minutes
2023-05-20 16:29:26,715:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:26,715:INFO:Initializing create_model()
2023-05-20 16:29:26,715:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:26,715:INFO:Checking exceptions
2023-05-20 16:29:26,715:INFO:Importing libraries
2023-05-20 16:29:26,715:INFO:Copying training dataset
2023-05-20 16:29:26,725:INFO:Defining folds
2023-05-20 16:29:26,725:INFO:Declaring metric variables
2023-05-20 16:29:26,728:INFO:Importing untrained model
2023-05-20 16:29:26,731:INFO:Ridge Regression Imported successfully
2023-05-20 16:29:26,736:INFO:Starting cross validation
2023-05-20 16:29:26,737:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:29,722:INFO:Calculating mean and std
2023-05-20 16:29:29,724:INFO:Creating metrics dataframe
2023-05-20 16:29:30,031:INFO:Uploading results into container
2023-05-20 16:29:30,032:INFO:Uploading model into container now
2023-05-20 16:29:30,032:INFO:_master_model_container: 63
2023-05-20 16:29:30,032:INFO:_display_container: 5
2023-05-20 16:29:30,032:INFO:Ridge(random_state=5308)
2023-05-20 16:29:30,032:INFO:create_model() successfully completed......................................
2023-05-20 16:29:30,126:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:30,126:INFO:Creating metrics dataframe
2023-05-20 16:29:30,134:INFO:Initializing Elastic Net
2023-05-20 16:29:30,135:INFO:Total runtime is 0.17661840915679933 minutes
2023-05-20 16:29:30,137:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:30,137:INFO:Initializing create_model()
2023-05-20 16:29:30,137:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:30,138:INFO:Checking exceptions
2023-05-20 16:29:30,138:INFO:Importing libraries
2023-05-20 16:29:30,138:INFO:Copying training dataset
2023-05-20 16:29:30,147:INFO:Defining folds
2023-05-20 16:29:30,147:INFO:Declaring metric variables
2023-05-20 16:29:30,151:INFO:Importing untrained model
2023-05-20 16:29:30,154:INFO:Elastic Net Imported successfully
2023-05-20 16:29:30,159:INFO:Starting cross validation
2023-05-20 16:29:30,160:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:33,091:INFO:Calculating mean and std
2023-05-20 16:29:33,093:INFO:Creating metrics dataframe
2023-05-20 16:29:33,407:INFO:Uploading results into container
2023-05-20 16:29:33,407:INFO:Uploading model into container now
2023-05-20 16:29:33,407:INFO:_master_model_container: 64
2023-05-20 16:29:33,409:INFO:_display_container: 5
2023-05-20 16:29:33,409:INFO:ElasticNet(random_state=5308)
2023-05-20 16:29:33,409:INFO:create_model() successfully completed......................................
2023-05-20 16:29:33,503:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:33,503:INFO:Creating metrics dataframe
2023-05-20 16:29:33,511:INFO:Initializing Least Angle Regression
2023-05-20 16:29:33,512:INFO:Total runtime is 0.23290438254674278 minutes
2023-05-20 16:29:33,514:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:33,515:INFO:Initializing create_model()
2023-05-20 16:29:33,515:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:33,515:INFO:Checking exceptions
2023-05-20 16:29:33,515:INFO:Importing libraries
2023-05-20 16:29:33,515:INFO:Copying training dataset
2023-05-20 16:29:33,524:INFO:Defining folds
2023-05-20 16:29:33,525:INFO:Declaring metric variables
2023-05-20 16:29:33,527:INFO:Importing untrained model
2023-05-20 16:29:33,531:INFO:Least Angle Regression Imported successfully
2023-05-20 16:29:33,537:INFO:Starting cross validation
2023-05-20 16:29:33,538:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:33,728:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,729:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,729:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,744:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,745:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,745:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,750:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,750:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,751:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,778:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 16:29:33,780:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 16:29:33,783:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,783:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,784:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,802:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,802:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,803:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,808:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,809:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,809:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:33,815:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,815:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:29:33,816:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:29:36,578:INFO:Calculating mean and std
2023-05-20 16:29:36,579:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 16:29:36,581:INFO:Creating metrics dataframe
2023-05-20 16:29:36,913:INFO:Uploading results into container
2023-05-20 16:29:36,915:INFO:Uploading model into container now
2023-05-20 16:29:36,916:INFO:_master_model_container: 65
2023-05-20 16:29:36,916:INFO:_display_container: 5
2023-05-20 16:29:36,916:INFO:Lars(random_state=5308)
2023-05-20 16:29:36,916:INFO:create_model() successfully completed......................................
2023-05-20 16:29:37,021:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:37,021:INFO:Creating metrics dataframe
2023-05-20 16:29:37,030:INFO:Initializing Lasso Least Angle Regression
2023-05-20 16:29:37,030:INFO:Total runtime is 0.2915297865867615 minutes
2023-05-20 16:29:37,033:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:37,033:INFO:Initializing create_model()
2023-05-20 16:29:37,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:37,033:INFO:Checking exceptions
2023-05-20 16:29:37,033:INFO:Importing libraries
2023-05-20 16:29:37,034:INFO:Copying training dataset
2023-05-20 16:29:37,044:INFO:Defining folds
2023-05-20 16:29:37,044:INFO:Declaring metric variables
2023-05-20 16:29:37,047:INFO:Importing untrained model
2023-05-20 16:29:37,051:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 16:29:37,056:INFO:Starting cross validation
2023-05-20 16:29:37,058:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:37,181:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,191:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,213:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,224:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,226:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,235:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,252:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,257:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,265:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:37,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:29:40,139:INFO:Calculating mean and std
2023-05-20 16:29:40,142:INFO:Creating metrics dataframe
2023-05-20 16:29:40,466:INFO:Uploading results into container
2023-05-20 16:29:40,467:INFO:Uploading model into container now
2023-05-20 16:29:40,468:INFO:_master_model_container: 66
2023-05-20 16:29:40,468:INFO:_display_container: 5
2023-05-20 16:29:40,468:INFO:LassoLars(random_state=5308)
2023-05-20 16:29:40,468:INFO:create_model() successfully completed......................................
2023-05-20 16:29:40,565:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:40,565:INFO:Creating metrics dataframe
2023-05-20 16:29:40,575:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 16:29:40,575:INFO:Total runtime is 0.3506171782811483 minutes
2023-05-20 16:29:40,577:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:40,578:INFO:Initializing create_model()
2023-05-20 16:29:40,578:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:40,578:INFO:Checking exceptions
2023-05-20 16:29:40,578:INFO:Importing libraries
2023-05-20 16:29:40,578:INFO:Copying training dataset
2023-05-20 16:29:40,590:INFO:Defining folds
2023-05-20 16:29:40,591:INFO:Declaring metric variables
2023-05-20 16:29:40,593:INFO:Importing untrained model
2023-05-20 16:29:40,597:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 16:29:40,605:INFO:Starting cross validation
2023-05-20 16:29:40,607:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:40,725:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,730:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,742:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,752:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,756:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,764:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,781:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,791:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,800:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:40,808:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:29:43,608:INFO:Calculating mean and std
2023-05-20 16:29:43,610:INFO:Creating metrics dataframe
2023-05-20 16:29:43,915:INFO:Uploading results into container
2023-05-20 16:29:43,915:INFO:Uploading model into container now
2023-05-20 16:29:43,916:INFO:_master_model_container: 67
2023-05-20 16:29:43,916:INFO:_display_container: 5
2023-05-20 16:29:43,916:INFO:OrthogonalMatchingPursuit()
2023-05-20 16:29:43,916:INFO:create_model() successfully completed......................................
2023-05-20 16:29:44,027:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:44,027:INFO:Creating metrics dataframe
2023-05-20 16:29:44,036:INFO:Initializing Bayesian Ridge
2023-05-20 16:29:44,037:INFO:Total runtime is 0.40831908782323206 minutes
2023-05-20 16:29:44,040:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:44,041:INFO:Initializing create_model()
2023-05-20 16:29:44,041:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:44,041:INFO:Checking exceptions
2023-05-20 16:29:44,041:INFO:Importing libraries
2023-05-20 16:29:44,041:INFO:Copying training dataset
2023-05-20 16:29:44,051:INFO:Defining folds
2023-05-20 16:29:44,051:INFO:Declaring metric variables
2023-05-20 16:29:44,055:INFO:Importing untrained model
2023-05-20 16:29:44,058:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:29:44,063:INFO:Starting cross validation
2023-05-20 16:29:44,065:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:47,083:INFO:Calculating mean and std
2023-05-20 16:29:47,085:INFO:Creating metrics dataframe
2023-05-20 16:29:47,387:INFO:Uploading results into container
2023-05-20 16:29:47,387:INFO:Uploading model into container now
2023-05-20 16:29:47,388:INFO:_master_model_container: 68
2023-05-20 16:29:47,388:INFO:_display_container: 5
2023-05-20 16:29:47,388:INFO:BayesianRidge()
2023-05-20 16:29:47,388:INFO:create_model() successfully completed......................................
2023-05-20 16:29:47,484:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:47,484:INFO:Creating metrics dataframe
2023-05-20 16:29:47,493:INFO:Initializing Passive Aggressive Regressor
2023-05-20 16:29:47,493:INFO:Total runtime is 0.46591626803080244 minutes
2023-05-20 16:29:47,496:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:47,496:INFO:Initializing create_model()
2023-05-20 16:29:47,496:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:47,496:INFO:Checking exceptions
2023-05-20 16:29:47,496:INFO:Importing libraries
2023-05-20 16:29:47,497:INFO:Copying training dataset
2023-05-20 16:29:47,505:INFO:Defining folds
2023-05-20 16:29:47,505:INFO:Declaring metric variables
2023-05-20 16:29:47,508:INFO:Importing untrained model
2023-05-20 16:29:47,511:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 16:29:47,517:INFO:Starting cross validation
2023-05-20 16:29:47,518:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:50,563:INFO:Calculating mean and std
2023-05-20 16:29:50,564:INFO:Creating metrics dataframe
2023-05-20 16:29:50,875:INFO:Uploading results into container
2023-05-20 16:29:50,876:INFO:Uploading model into container now
2023-05-20 16:29:50,877:INFO:_master_model_container: 69
2023-05-20 16:29:50,877:INFO:_display_container: 5
2023-05-20 16:29:50,878:INFO:PassiveAggressiveRegressor(random_state=5308)
2023-05-20 16:29:50,878:INFO:create_model() successfully completed......................................
2023-05-20 16:29:50,976:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:50,976:INFO:Creating metrics dataframe
2023-05-20 16:29:50,986:INFO:Initializing Huber Regressor
2023-05-20 16:29:50,986:INFO:Total runtime is 0.5241358757019043 minutes
2023-05-20 16:29:50,989:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:50,989:INFO:Initializing create_model()
2023-05-20 16:29:50,989:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:50,989:INFO:Checking exceptions
2023-05-20 16:29:50,989:INFO:Importing libraries
2023-05-20 16:29:50,989:INFO:Copying training dataset
2023-05-20 16:29:50,999:INFO:Defining folds
2023-05-20 16:29:50,999:INFO:Declaring metric variables
2023-05-20 16:29:51,002:INFO:Importing untrained model
2023-05-20 16:29:51,005:INFO:Huber Regressor Imported successfully
2023-05-20 16:29:51,011:INFO:Starting cross validation
2023-05-20 16:29:51,013:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:54,018:INFO:Calculating mean and std
2023-05-20 16:29:54,020:INFO:Creating metrics dataframe
2023-05-20 16:29:54,332:INFO:Uploading results into container
2023-05-20 16:29:54,333:INFO:Uploading model into container now
2023-05-20 16:29:54,334:INFO:_master_model_container: 70
2023-05-20 16:29:54,334:INFO:_display_container: 5
2023-05-20 16:29:54,334:INFO:HuberRegressor()
2023-05-20 16:29:54,334:INFO:create_model() successfully completed......................................
2023-05-20 16:29:54,433:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:54,433:INFO:Creating metrics dataframe
2023-05-20 16:29:54,444:INFO:Initializing K Neighbors Regressor
2023-05-20 16:29:54,444:INFO:Total runtime is 0.5817766348520915 minutes
2023-05-20 16:29:54,448:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:54,449:INFO:Initializing create_model()
2023-05-20 16:29:54,449:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:54,449:INFO:Checking exceptions
2023-05-20 16:29:54,449:INFO:Importing libraries
2023-05-20 16:29:54,449:INFO:Copying training dataset
2023-05-20 16:29:54,459:INFO:Defining folds
2023-05-20 16:29:54,460:INFO:Declaring metric variables
2023-05-20 16:29:54,463:INFO:Importing untrained model
2023-05-20 16:29:54,467:INFO:K Neighbors Regressor Imported successfully
2023-05-20 16:29:54,478:INFO:Starting cross validation
2023-05-20 16:29:54,481:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:29:57,549:INFO:Calculating mean and std
2023-05-20 16:29:57,551:INFO:Creating metrics dataframe
2023-05-20 16:29:57,876:INFO:Uploading results into container
2023-05-20 16:29:57,877:INFO:Uploading model into container now
2023-05-20 16:29:57,877:INFO:_master_model_container: 71
2023-05-20 16:29:57,877:INFO:_display_container: 5
2023-05-20 16:29:57,878:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 16:29:57,878:INFO:create_model() successfully completed......................................
2023-05-20 16:29:57,977:INFO:SubProcess create_model() end ==================================
2023-05-20 16:29:57,977:INFO:Creating metrics dataframe
2023-05-20 16:29:57,988:INFO:Initializing Decision Tree Regressor
2023-05-20 16:29:57,988:INFO:Total runtime is 0.6408343195915223 minutes
2023-05-20 16:29:57,991:INFO:SubProcess create_model() called ==================================
2023-05-20 16:29:57,991:INFO:Initializing create_model()
2023-05-20 16:29:57,991:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:29:57,991:INFO:Checking exceptions
2023-05-20 16:29:57,991:INFO:Importing libraries
2023-05-20 16:29:57,992:INFO:Copying training dataset
2023-05-20 16:29:58,000:INFO:Defining folds
2023-05-20 16:29:58,000:INFO:Declaring metric variables
2023-05-20 16:29:58,003:INFO:Importing untrained model
2023-05-20 16:29:58,006:INFO:Decision Tree Regressor Imported successfully
2023-05-20 16:29:58,013:INFO:Starting cross validation
2023-05-20 16:29:58,014:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:01,033:INFO:Calculating mean and std
2023-05-20 16:30:01,036:INFO:Creating metrics dataframe
2023-05-20 16:30:01,350:INFO:Uploading results into container
2023-05-20 16:30:01,350:INFO:Uploading model into container now
2023-05-20 16:30:01,351:INFO:_master_model_container: 72
2023-05-20 16:30:01,351:INFO:_display_container: 5
2023-05-20 16:30:01,351:INFO:DecisionTreeRegressor(random_state=5308)
2023-05-20 16:30:01,351:INFO:create_model() successfully completed......................................
2023-05-20 16:30:01,464:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:01,464:INFO:Creating metrics dataframe
2023-05-20 16:30:01,475:INFO:Initializing Random Forest Regressor
2023-05-20 16:30:01,475:INFO:Total runtime is 0.6989586313565572 minutes
2023-05-20 16:30:01,478:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:01,478:INFO:Initializing create_model()
2023-05-20 16:30:01,478:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:01,478:INFO:Checking exceptions
2023-05-20 16:30:01,478:INFO:Importing libraries
2023-05-20 16:30:01,478:INFO:Copying training dataset
2023-05-20 16:30:01,489:INFO:Defining folds
2023-05-20 16:30:01,489:INFO:Declaring metric variables
2023-05-20 16:30:01,492:INFO:Importing untrained model
2023-05-20 16:30:01,496:INFO:Random Forest Regressor Imported successfully
2023-05-20 16:30:01,501:INFO:Starting cross validation
2023-05-20 16:30:01,503:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:04,641:INFO:Calculating mean and std
2023-05-20 16:30:04,643:INFO:Creating metrics dataframe
2023-05-20 16:30:04,958:INFO:Uploading results into container
2023-05-20 16:30:04,959:INFO:Uploading model into container now
2023-05-20 16:30:04,959:INFO:_master_model_container: 73
2023-05-20 16:30:04,959:INFO:_display_container: 5
2023-05-20 16:30:04,959:INFO:RandomForestRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:30:04,960:INFO:create_model() successfully completed......................................
2023-05-20 16:30:05,055:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:05,055:INFO:Creating metrics dataframe
2023-05-20 16:30:05,065:INFO:Initializing Extra Trees Regressor
2023-05-20 16:30:05,065:INFO:Total runtime is 0.7587939143180847 minutes
2023-05-20 16:30:05,068:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:05,068:INFO:Initializing create_model()
2023-05-20 16:30:05,068:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:05,068:INFO:Checking exceptions
2023-05-20 16:30:05,068:INFO:Importing libraries
2023-05-20 16:30:05,068:INFO:Copying training dataset
2023-05-20 16:30:05,078:INFO:Defining folds
2023-05-20 16:30:05,078:INFO:Declaring metric variables
2023-05-20 16:30:05,082:INFO:Importing untrained model
2023-05-20 16:30:05,085:INFO:Extra Trees Regressor Imported successfully
2023-05-20 16:30:05,090:INFO:Starting cross validation
2023-05-20 16:30:05,092:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:08,363:INFO:Calculating mean and std
2023-05-20 16:30:08,364:INFO:Creating metrics dataframe
2023-05-20 16:30:08,683:INFO:Uploading results into container
2023-05-20 16:30:08,684:INFO:Uploading model into container now
2023-05-20 16:30:08,684:INFO:_master_model_container: 74
2023-05-20 16:30:08,684:INFO:_display_container: 5
2023-05-20 16:30:08,685:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:30:08,685:INFO:create_model() successfully completed......................................
2023-05-20 16:30:08,786:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:08,786:INFO:Creating metrics dataframe
2023-05-20 16:30:08,797:INFO:Initializing AdaBoost Regressor
2023-05-20 16:30:08,797:INFO:Total runtime is 0.8209833423296611 minutes
2023-05-20 16:30:08,800:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:08,800:INFO:Initializing create_model()
2023-05-20 16:30:08,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:08,800:INFO:Checking exceptions
2023-05-20 16:30:08,801:INFO:Importing libraries
2023-05-20 16:30:08,801:INFO:Copying training dataset
2023-05-20 16:30:08,812:INFO:Defining folds
2023-05-20 16:30:08,812:INFO:Declaring metric variables
2023-05-20 16:30:08,814:INFO:Importing untrained model
2023-05-20 16:30:08,817:INFO:AdaBoost Regressor Imported successfully
2023-05-20 16:30:08,822:INFO:Starting cross validation
2023-05-20 16:30:08,824:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:11,917:INFO:Calculating mean and std
2023-05-20 16:30:11,919:INFO:Creating metrics dataframe
2023-05-20 16:30:12,223:INFO:Uploading results into container
2023-05-20 16:30:12,223:INFO:Uploading model into container now
2023-05-20 16:30:12,224:INFO:_master_model_container: 75
2023-05-20 16:30:12,224:INFO:_display_container: 5
2023-05-20 16:30:12,224:INFO:AdaBoostRegressor(random_state=5308)
2023-05-20 16:30:12,224:INFO:create_model() successfully completed......................................
2023-05-20 16:30:12,332:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:12,332:INFO:Creating metrics dataframe
2023-05-20 16:30:12,343:INFO:Initializing Gradient Boosting Regressor
2023-05-20 16:30:12,343:INFO:Total runtime is 0.8800926486651103 minutes
2023-05-20 16:30:12,348:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:12,348:INFO:Initializing create_model()
2023-05-20 16:30:12,348:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:12,349:INFO:Checking exceptions
2023-05-20 16:30:12,349:INFO:Importing libraries
2023-05-20 16:30:12,349:INFO:Copying training dataset
2023-05-20 16:30:12,360:INFO:Defining folds
2023-05-20 16:30:12,360:INFO:Declaring metric variables
2023-05-20 16:30:12,363:INFO:Importing untrained model
2023-05-20 16:30:12,367:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:30:12,372:INFO:Starting cross validation
2023-05-20 16:30:12,376:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:15,618:INFO:Calculating mean and std
2023-05-20 16:30:15,619:INFO:Creating metrics dataframe
2023-05-20 16:30:15,941:INFO:Uploading results into container
2023-05-20 16:30:15,942:INFO:Uploading model into container now
2023-05-20 16:30:15,942:INFO:_master_model_container: 76
2023-05-20 16:30:15,943:INFO:_display_container: 5
2023-05-20 16:30:15,943:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:30:15,943:INFO:create_model() successfully completed......................................
2023-05-20 16:30:16,034:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:16,034:INFO:Creating metrics dataframe
2023-05-20 16:30:16,043:INFO:Initializing Extreme Gradient Boosting
2023-05-20 16:30:16,045:INFO:Total runtime is 0.9417783657709757 minutes
2023-05-20 16:30:16,047:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:16,047:INFO:Initializing create_model()
2023-05-20 16:30:16,048:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:16,048:INFO:Checking exceptions
2023-05-20 16:30:16,048:INFO:Importing libraries
2023-05-20 16:30:16,048:INFO:Copying training dataset
2023-05-20 16:30:16,069:INFO:Defining folds
2023-05-20 16:30:16,069:INFO:Declaring metric variables
2023-05-20 16:30:16,077:INFO:Importing untrained model
2023-05-20 16:30:16,085:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 16:30:16,093:INFO:Starting cross validation
2023-05-20 16:30:16,096:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:19,282:INFO:Calculating mean and std
2023-05-20 16:30:19,284:INFO:Creating metrics dataframe
2023-05-20 16:30:19,596:INFO:Uploading results into container
2023-05-20 16:30:19,596:INFO:Uploading model into container now
2023-05-20 16:30:19,597:INFO:_master_model_container: 77
2023-05-20 16:30:19,597:INFO:_display_container: 5
2023-05-20 16:30:19,598:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5308, ...)
2023-05-20 16:30:19,598:INFO:create_model() successfully completed......................................
2023-05-20 16:30:19,691:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:19,691:INFO:Creating metrics dataframe
2023-05-20 16:30:19,702:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 16:30:19,702:INFO:Total runtime is 1.002740748723348 minutes
2023-05-20 16:30:19,705:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:19,706:INFO:Initializing create_model()
2023-05-20 16:30:19,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:19,706:INFO:Checking exceptions
2023-05-20 16:30:19,706:INFO:Importing libraries
2023-05-20 16:30:19,706:INFO:Copying training dataset
2023-05-20 16:30:19,716:INFO:Defining folds
2023-05-20 16:30:19,716:INFO:Declaring metric variables
2023-05-20 16:30:19,720:INFO:Importing untrained model
2023-05-20 16:30:19,723:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:30:19,728:INFO:Starting cross validation
2023-05-20 16:30:19,729:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:22,710:INFO:Calculating mean and std
2023-05-20 16:30:22,712:INFO:Creating metrics dataframe
2023-05-20 16:30:23,017:INFO:Uploading results into container
2023-05-20 16:30:23,018:INFO:Uploading model into container now
2023-05-20 16:30:23,019:INFO:_master_model_container: 78
2023-05-20 16:30:23,019:INFO:_display_container: 5
2023-05-20 16:30:23,019:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:30:23,020:INFO:create_model() successfully completed......................................
2023-05-20 16:30:23,116:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:23,117:INFO:Creating metrics dataframe
2023-05-20 16:30:23,126:INFO:Initializing CatBoost Regressor
2023-05-20 16:30:23,126:INFO:Total runtime is 1.0598084847132365 minutes
2023-05-20 16:30:23,129:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:23,130:INFO:Initializing create_model()
2023-05-20 16:30:23,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:23,130:INFO:Checking exceptions
2023-05-20 16:30:23,130:INFO:Importing libraries
2023-05-20 16:30:23,130:INFO:Copying training dataset
2023-05-20 16:30:23,140:INFO:Defining folds
2023-05-20 16:30:23,140:INFO:Declaring metric variables
2023-05-20 16:30:23,144:INFO:Importing untrained model
2023-05-20 16:30:23,147:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:30:23,153:INFO:Starting cross validation
2023-05-20 16:30:23,154:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:26,321:INFO:Calculating mean and std
2023-05-20 16:30:26,322:INFO:Creating metrics dataframe
2023-05-20 16:30:26,672:INFO:Uploading results into container
2023-05-20 16:30:26,673:INFO:Uploading model into container now
2023-05-20 16:30:26,674:INFO:_master_model_container: 79
2023-05-20 16:30:26,674:INFO:_display_container: 5
2023-05-20 16:30:26,674:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFBED2A40>
2023-05-20 16:30:26,674:INFO:create_model() successfully completed......................................
2023-05-20 16:30:26,780:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:26,780:INFO:Creating metrics dataframe
2023-05-20 16:30:26,792:INFO:Initializing Dummy Regressor
2023-05-20 16:30:26,792:INFO:Total runtime is 1.1209020296732586 minutes
2023-05-20 16:30:26,796:INFO:SubProcess create_model() called ==================================
2023-05-20 16:30:26,796:INFO:Initializing create_model()
2023-05-20 16:30:26,796:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BF61BE410>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:26,796:INFO:Checking exceptions
2023-05-20 16:30:26,796:INFO:Importing libraries
2023-05-20 16:30:26,796:INFO:Copying training dataset
2023-05-20 16:30:26,810:INFO:Defining folds
2023-05-20 16:30:26,810:INFO:Declaring metric variables
2023-05-20 16:30:26,815:INFO:Importing untrained model
2023-05-20 16:30:26,819:INFO:Dummy Regressor Imported successfully
2023-05-20 16:30:26,830:INFO:Starting cross validation
2023-05-20 16:30:26,832:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:30:29,907:INFO:Calculating mean and std
2023-05-20 16:30:29,909:INFO:Creating metrics dataframe
2023-05-20 16:30:30,220:INFO:Uploading results into container
2023-05-20 16:30:30,222:INFO:Uploading model into container now
2023-05-20 16:30:30,222:INFO:_master_model_container: 80
2023-05-20 16:30:30,222:INFO:_display_container: 5
2023-05-20 16:30:30,222:INFO:DummyRegressor()
2023-05-20 16:30:30,222:INFO:create_model() successfully completed......................................
2023-05-20 16:30:30,319:INFO:SubProcess create_model() end ==================================
2023-05-20 16:30:30,319:INFO:Creating metrics dataframe
2023-05-20 16:30:30,338:INFO:Initializing create_model()
2023-05-20 16:30:30,339:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BFBED2A40>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:30,339:INFO:Checking exceptions
2023-05-20 16:30:30,341:INFO:Importing libraries
2023-05-20 16:30:30,341:INFO:Copying training dataset
2023-05-20 16:30:30,350:INFO:Defining folds
2023-05-20 16:30:30,350:INFO:Declaring metric variables
2023-05-20 16:30:30,350:INFO:Importing untrained model
2023-05-20 16:30:30,350:INFO:Declaring custom model
2023-05-20 16:30:30,351:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:30:30,352:INFO:Cross validation set to False
2023-05-20 16:30:30,352:INFO:Fitting Model
2023-05-20 16:30:30,677:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFBF2CA60>
2023-05-20 16:30:30,678:INFO:create_model() successfully completed......................................
2023-05-20 16:30:30,783:INFO:Initializing create_model()
2023-05-20 16:30:30,783:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:30,783:INFO:Checking exceptions
2023-05-20 16:30:30,785:INFO:Importing libraries
2023-05-20 16:30:30,785:INFO:Copying training dataset
2023-05-20 16:30:30,794:INFO:Defining folds
2023-05-20 16:30:30,794:INFO:Declaring metric variables
2023-05-20 16:30:30,794:INFO:Importing untrained model
2023-05-20 16:30:30,794:INFO:Declaring custom model
2023-05-20 16:30:30,794:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:30:30,796:INFO:Cross validation set to False
2023-05-20 16:30:30,796:INFO:Fitting Model
2023-05-20 16:30:31,300:INFO:BayesianRidge()
2023-05-20 16:30:31,300:INFO:create_model() successfully completed......................................
2023-05-20 16:30:31,399:INFO:Initializing create_model()
2023-05-20 16:30:31,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=GradientBoostingRegressor(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:31,399:INFO:Checking exceptions
2023-05-20 16:30:31,401:INFO:Importing libraries
2023-05-20 16:30:31,401:INFO:Copying training dataset
2023-05-20 16:30:31,409:INFO:Defining folds
2023-05-20 16:30:31,410:INFO:Declaring metric variables
2023-05-20 16:30:31,410:INFO:Importing untrained model
2023-05-20 16:30:31,410:INFO:Declaring custom model
2023-05-20 16:30:31,410:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:30:31,412:INFO:Cross validation set to False
2023-05-20 16:30:31,412:INFO:Fitting Model
2023-05-20 16:30:32,444:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:30:32,444:INFO:create_model() successfully completed......................................
2023-05-20 16:30:32,544:INFO:Initializing create_model()
2023-05-20 16:30:32,544:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=Ridge(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:32,544:INFO:Checking exceptions
2023-05-20 16:30:32,546:INFO:Importing libraries
2023-05-20 16:30:32,546:INFO:Copying training dataset
2023-05-20 16:30:32,554:INFO:Defining folds
2023-05-20 16:30:32,554:INFO:Declaring metric variables
2023-05-20 16:30:32,554:INFO:Importing untrained model
2023-05-20 16:30:32,554:INFO:Declaring custom model
2023-05-20 16:30:32,555:INFO:Ridge Regression Imported successfully
2023-05-20 16:30:32,556:INFO:Cross validation set to False
2023-05-20 16:30:32,556:INFO:Fitting Model
2023-05-20 16:30:32,883:INFO:Ridge(random_state=5308)
2023-05-20 16:30:32,883:INFO:create_model() successfully completed......................................
2023-05-20 16:30:32,981:INFO:Initializing create_model()
2023-05-20 16:30:32,982:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=LGBMRegressor(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:30:32,982:INFO:Checking exceptions
2023-05-20 16:30:32,984:INFO:Importing libraries
2023-05-20 16:30:32,984:INFO:Copying training dataset
2023-05-20 16:30:32,994:INFO:Defining folds
2023-05-20 16:30:32,994:INFO:Declaring metric variables
2023-05-20 16:30:32,994:INFO:Importing untrained model
2023-05-20 16:30:32,994:INFO:Declaring custom model
2023-05-20 16:30:32,995:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:30:32,996:INFO:Cross validation set to False
2023-05-20 16:30:32,996:INFO:Fitting Model
2023-05-20 16:30:33,607:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:30:33,607:INFO:create_model() successfully completed......................................
2023-05-20 16:30:33,740:INFO:_master_model_container: 80
2023-05-20 16:30:33,740:INFO:_display_container: 5
2023-05-20 16:30:33,740:INFO:[<catboost.core.CatBoostRegressor object at 0x0000025BFBF2CA60>, BayesianRidge(), GradientBoostingRegressor(random_state=5308), Ridge(random_state=5308), LGBMRegressor(random_state=5308)]
2023-05-20 16:30:33,741:INFO:compare_models() successfully completed......................................
2023-05-20 16:32:15,954:INFO:Initializing predict_model()
2023-05-20 16:32:15,954:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BFBF2CA60>, probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025BF4908AF0>)
2023-05-20 16:32:15,954:INFO:Checking exceptions
2023-05-20 16:32:15,954:INFO:Preloading libraries
2023-05-20 16:34:41,986:INFO:Initializing compare_models()
2023-05-20 16:34:41,991:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 16:34:41,991:INFO:Checking exceptions
2023-05-20 16:34:41,997:INFO:Preparing display monitor
2023-05-20 16:34:42,045:INFO:Initializing Linear Regression
2023-05-20 16:34:42,045:INFO:Total runtime is 0.0 minutes
2023-05-20 16:34:42,047:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:42,049:INFO:Initializing create_model()
2023-05-20 16:34:42,049:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:42,049:INFO:Checking exceptions
2023-05-20 16:34:42,049:INFO:Importing libraries
2023-05-20 16:34:42,049:INFO:Copying training dataset
2023-05-20 16:34:42,058:INFO:Defining folds
2023-05-20 16:34:42,058:INFO:Declaring metric variables
2023-05-20 16:34:42,062:INFO:Importing untrained model
2023-05-20 16:34:42,066:INFO:Linear Regression Imported successfully
2023-05-20 16:34:42,073:INFO:Starting cross validation
2023-05-20 16:34:42,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:45,344:INFO:Calculating mean and std
2023-05-20 16:34:45,346:INFO:Creating metrics dataframe
2023-05-20 16:34:45,680:INFO:Uploading results into container
2023-05-20 16:34:45,681:INFO:Uploading model into container now
2023-05-20 16:34:45,681:INFO:_master_model_container: 81
2023-05-20 16:34:45,682:INFO:_display_container: 7
2023-05-20 16:34:45,682:INFO:LinearRegression(n_jobs=-1)
2023-05-20 16:34:45,682:INFO:create_model() successfully completed......................................
2023-05-20 16:34:45,789:INFO:SubProcess create_model() end ==================================
2023-05-20 16:34:45,789:INFO:Creating metrics dataframe
2023-05-20 16:34:45,796:INFO:Initializing Lasso Regression
2023-05-20 16:34:45,796:INFO:Total runtime is 0.0625218391418457 minutes
2023-05-20 16:34:45,799:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:45,800:INFO:Initializing create_model()
2023-05-20 16:34:45,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:45,800:INFO:Checking exceptions
2023-05-20 16:34:45,800:INFO:Importing libraries
2023-05-20 16:34:45,800:INFO:Copying training dataset
2023-05-20 16:34:45,809:INFO:Defining folds
2023-05-20 16:34:45,809:INFO:Declaring metric variables
2023-05-20 16:34:45,812:INFO:Importing untrained model
2023-05-20 16:34:45,816:INFO:Lasso Regression Imported successfully
2023-05-20 16:34:45,823:INFO:Starting cross validation
2023-05-20 16:34:45,824:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:48,897:INFO:Calculating mean and std
2023-05-20 16:34:48,898:INFO:Creating metrics dataframe
2023-05-20 16:34:49,211:INFO:Uploading results into container
2023-05-20 16:34:49,211:INFO:Uploading model into container now
2023-05-20 16:34:49,212:INFO:_master_model_container: 82
2023-05-20 16:34:49,212:INFO:_display_container: 7
2023-05-20 16:34:49,212:INFO:Lasso(random_state=5308)
2023-05-20 16:34:49,212:INFO:create_model() successfully completed......................................
2023-05-20 16:34:49,311:INFO:SubProcess create_model() end ==================================
2023-05-20 16:34:49,311:INFO:Creating metrics dataframe
2023-05-20 16:34:49,319:INFO:Initializing Ridge Regression
2023-05-20 16:34:49,320:INFO:Total runtime is 0.12122780879338582 minutes
2023-05-20 16:34:49,323:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:49,323:INFO:Initializing create_model()
2023-05-20 16:34:49,323:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:49,323:INFO:Checking exceptions
2023-05-20 16:34:49,323:INFO:Importing libraries
2023-05-20 16:34:49,323:INFO:Copying training dataset
2023-05-20 16:34:49,333:INFO:Defining folds
2023-05-20 16:34:49,333:INFO:Declaring metric variables
2023-05-20 16:34:49,337:INFO:Importing untrained model
2023-05-20 16:34:49,340:INFO:Ridge Regression Imported successfully
2023-05-20 16:34:49,347:INFO:Starting cross validation
2023-05-20 16:34:49,349:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:52,421:INFO:Calculating mean and std
2023-05-20 16:34:52,422:INFO:Creating metrics dataframe
2023-05-20 16:34:52,747:INFO:Uploading results into container
2023-05-20 16:34:52,748:INFO:Uploading model into container now
2023-05-20 16:34:52,748:INFO:_master_model_container: 83
2023-05-20 16:34:52,748:INFO:_display_container: 7
2023-05-20 16:34:52,749:INFO:Ridge(random_state=5308)
2023-05-20 16:34:52,749:INFO:create_model() successfully completed......................................
2023-05-20 16:34:52,862:INFO:SubProcess create_model() end ==================================
2023-05-20 16:34:52,862:INFO:Creating metrics dataframe
2023-05-20 16:34:52,870:INFO:Initializing Elastic Net
2023-05-20 16:34:52,870:INFO:Total runtime is 0.18042343457539878 minutes
2023-05-20 16:34:52,872:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:52,874:INFO:Initializing create_model()
2023-05-20 16:34:52,874:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:52,874:INFO:Checking exceptions
2023-05-20 16:34:52,874:INFO:Importing libraries
2023-05-20 16:34:52,874:INFO:Copying training dataset
2023-05-20 16:34:52,887:INFO:Defining folds
2023-05-20 16:34:52,887:INFO:Declaring metric variables
2023-05-20 16:34:52,890:INFO:Importing untrained model
2023-05-20 16:34:52,894:INFO:Elastic Net Imported successfully
2023-05-20 16:34:52,901:INFO:Starting cross validation
2023-05-20 16:34:52,904:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:55,911:INFO:Calculating mean and std
2023-05-20 16:34:55,913:INFO:Creating metrics dataframe
2023-05-20 16:34:56,257:INFO:Uploading results into container
2023-05-20 16:34:56,258:INFO:Uploading model into container now
2023-05-20 16:34:56,259:INFO:_master_model_container: 84
2023-05-20 16:34:56,259:INFO:_display_container: 7
2023-05-20 16:34:56,259:INFO:ElasticNet(random_state=5308)
2023-05-20 16:34:56,260:INFO:create_model() successfully completed......................................
2023-05-20 16:34:56,354:INFO:SubProcess create_model() end ==================================
2023-05-20 16:34:56,354:INFO:Creating metrics dataframe
2023-05-20 16:34:56,362:INFO:Initializing Least Angle Regression
2023-05-20 16:34:56,362:INFO:Total runtime is 0.2386142611503601 minutes
2023-05-20 16:34:56,365:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:56,365:INFO:Initializing create_model()
2023-05-20 16:34:56,365:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:56,365:INFO:Checking exceptions
2023-05-20 16:34:56,365:INFO:Importing libraries
2023-05-20 16:34:56,365:INFO:Copying training dataset
2023-05-20 16:34:56,375:INFO:Defining folds
2023-05-20 16:34:56,375:INFO:Declaring metric variables
2023-05-20 16:34:56,378:INFO:Importing untrained model
2023-05-20 16:34:56,381:INFO:Least Angle Regression Imported successfully
2023-05-20 16:34:56,386:INFO:Starting cross validation
2023-05-20 16:34:56,387:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:56,581:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,581:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,582:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,586:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,587:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,589:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,590:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,590:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,606:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,607:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,607:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,631:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 16:34:56,634:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 16:34:56,642:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,643:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,643:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,650:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,651:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,651:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:56,660:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,661:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 16:34:56,661:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 16:34:59,370:INFO:Calculating mean and std
2023-05-20 16:34:59,370:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 16:34:59,372:INFO:Creating metrics dataframe
2023-05-20 16:34:59,695:INFO:Uploading results into container
2023-05-20 16:34:59,695:INFO:Uploading model into container now
2023-05-20 16:34:59,696:INFO:_master_model_container: 85
2023-05-20 16:34:59,696:INFO:_display_container: 7
2023-05-20 16:34:59,696:INFO:Lars(random_state=5308)
2023-05-20 16:34:59,696:INFO:create_model() successfully completed......................................
2023-05-20 16:34:59,793:INFO:SubProcess create_model() end ==================================
2023-05-20 16:34:59,793:INFO:Creating metrics dataframe
2023-05-20 16:34:59,801:INFO:Initializing Lasso Least Angle Regression
2023-05-20 16:34:59,801:INFO:Total runtime is 0.29593255122502643 minutes
2023-05-20 16:34:59,804:INFO:SubProcess create_model() called ==================================
2023-05-20 16:34:59,804:INFO:Initializing create_model()
2023-05-20 16:34:59,805:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:34:59,805:INFO:Checking exceptions
2023-05-20 16:34:59,805:INFO:Importing libraries
2023-05-20 16:34:59,805:INFO:Copying training dataset
2023-05-20 16:34:59,814:INFO:Defining folds
2023-05-20 16:34:59,814:INFO:Declaring metric variables
2023-05-20 16:34:59,817:INFO:Importing untrained model
2023-05-20 16:34:59,820:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 16:34:59,826:INFO:Starting cross validation
2023-05-20 16:34:59,828:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:34:59,949:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:34:59,950:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:34:59,963:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:34:59,978:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:34:59,980:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:34:59,987:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:35:00,006:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:35:00,019:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:35:00,026:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:35:00,031:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 16:35:02,922:INFO:Calculating mean and std
2023-05-20 16:35:02,924:INFO:Creating metrics dataframe
2023-05-20 16:35:03,264:INFO:Uploading results into container
2023-05-20 16:35:03,264:INFO:Uploading model into container now
2023-05-20 16:35:03,265:INFO:_master_model_container: 86
2023-05-20 16:35:03,265:INFO:_display_container: 7
2023-05-20 16:35:03,265:INFO:LassoLars(random_state=5308)
2023-05-20 16:35:03,265:INFO:create_model() successfully completed......................................
2023-05-20 16:35:03,360:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:03,360:INFO:Creating metrics dataframe
2023-05-20 16:35:03,368:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 16:35:03,368:INFO:Total runtime is 0.3553882201512654 minutes
2023-05-20 16:35:03,370:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:03,371:INFO:Initializing create_model()
2023-05-20 16:35:03,371:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:03,371:INFO:Checking exceptions
2023-05-20 16:35:03,371:INFO:Importing libraries
2023-05-20 16:35:03,371:INFO:Copying training dataset
2023-05-20 16:35:03,381:INFO:Defining folds
2023-05-20 16:35:03,381:INFO:Declaring metric variables
2023-05-20 16:35:03,384:INFO:Importing untrained model
2023-05-20 16:35:03,387:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 16:35:03,392:INFO:Starting cross validation
2023-05-20 16:35:03,394:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:03,503:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,512:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,527:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,529:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,535:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,561:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,563:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,578:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,582:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:03,594:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 16:35:06,446:INFO:Calculating mean and std
2023-05-20 16:35:06,447:INFO:Creating metrics dataframe
2023-05-20 16:35:06,751:INFO:Uploading results into container
2023-05-20 16:35:06,752:INFO:Uploading model into container now
2023-05-20 16:35:06,752:INFO:_master_model_container: 87
2023-05-20 16:35:06,753:INFO:_display_container: 7
2023-05-20 16:35:06,753:INFO:OrthogonalMatchingPursuit()
2023-05-20 16:35:06,753:INFO:create_model() successfully completed......................................
2023-05-20 16:35:06,846:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:06,846:INFO:Creating metrics dataframe
2023-05-20 16:35:06,854:INFO:Initializing Bayesian Ridge
2023-05-20 16:35:06,854:INFO:Total runtime is 0.41348736286163323 minutes
2023-05-20 16:35:06,857:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:06,857:INFO:Initializing create_model()
2023-05-20 16:35:06,857:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:06,857:INFO:Checking exceptions
2023-05-20 16:35:06,857:INFO:Importing libraries
2023-05-20 16:35:06,857:INFO:Copying training dataset
2023-05-20 16:35:06,867:INFO:Defining folds
2023-05-20 16:35:06,867:INFO:Declaring metric variables
2023-05-20 16:35:06,870:INFO:Importing untrained model
2023-05-20 16:35:06,873:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:35:06,879:INFO:Starting cross validation
2023-05-20 16:35:06,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:09,896:INFO:Calculating mean and std
2023-05-20 16:35:09,898:INFO:Creating metrics dataframe
2023-05-20 16:35:10,204:INFO:Uploading results into container
2023-05-20 16:35:10,205:INFO:Uploading model into container now
2023-05-20 16:35:10,206:INFO:_master_model_container: 88
2023-05-20 16:35:10,206:INFO:_display_container: 7
2023-05-20 16:35:10,207:INFO:BayesianRidge()
2023-05-20 16:35:10,207:INFO:create_model() successfully completed......................................
2023-05-20 16:35:10,297:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:10,297:INFO:Creating metrics dataframe
2023-05-20 16:35:10,307:INFO:Initializing Passive Aggressive Regressor
2023-05-20 16:35:10,307:INFO:Total runtime is 0.47103667259216303 minutes
2023-05-20 16:35:10,310:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:10,310:INFO:Initializing create_model()
2023-05-20 16:35:10,310:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:10,310:INFO:Checking exceptions
2023-05-20 16:35:10,310:INFO:Importing libraries
2023-05-20 16:35:10,310:INFO:Copying training dataset
2023-05-20 16:35:10,320:INFO:Defining folds
2023-05-20 16:35:10,320:INFO:Declaring metric variables
2023-05-20 16:35:10,324:INFO:Importing untrained model
2023-05-20 16:35:10,327:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 16:35:10,332:INFO:Starting cross validation
2023-05-20 16:35:10,334:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:13,349:INFO:Calculating mean and std
2023-05-20 16:35:13,351:INFO:Creating metrics dataframe
2023-05-20 16:35:13,673:INFO:Uploading results into container
2023-05-20 16:35:13,674:INFO:Uploading model into container now
2023-05-20 16:35:13,674:INFO:_master_model_container: 89
2023-05-20 16:35:13,674:INFO:_display_container: 7
2023-05-20 16:35:13,674:INFO:PassiveAggressiveRegressor(random_state=5308)
2023-05-20 16:35:13,674:INFO:create_model() successfully completed......................................
2023-05-20 16:35:13,772:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:13,774:INFO:Creating metrics dataframe
2023-05-20 16:35:13,782:INFO:Initializing Huber Regressor
2023-05-20 16:35:13,783:INFO:Total runtime is 0.5289623339970906 minutes
2023-05-20 16:35:13,785:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:13,786:INFO:Initializing create_model()
2023-05-20 16:35:13,786:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:13,786:INFO:Checking exceptions
2023-05-20 16:35:13,786:INFO:Importing libraries
2023-05-20 16:35:13,786:INFO:Copying training dataset
2023-05-20 16:35:13,798:INFO:Defining folds
2023-05-20 16:35:13,798:INFO:Declaring metric variables
2023-05-20 16:35:13,800:INFO:Importing untrained model
2023-05-20 16:35:13,805:INFO:Huber Regressor Imported successfully
2023-05-20 16:35:13,811:INFO:Starting cross validation
2023-05-20 16:35:13,813:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:16,807:INFO:Calculating mean and std
2023-05-20 16:35:16,809:INFO:Creating metrics dataframe
2023-05-20 16:35:17,123:INFO:Uploading results into container
2023-05-20 16:35:17,123:INFO:Uploading model into container now
2023-05-20 16:35:17,124:INFO:_master_model_container: 90
2023-05-20 16:35:17,124:INFO:_display_container: 7
2023-05-20 16:35:17,124:INFO:HuberRegressor()
2023-05-20 16:35:17,124:INFO:create_model() successfully completed......................................
2023-05-20 16:35:17,221:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:17,221:INFO:Creating metrics dataframe
2023-05-20 16:35:17,231:INFO:Initializing K Neighbors Regressor
2023-05-20 16:35:17,231:INFO:Total runtime is 0.5864305933316548 minutes
2023-05-20 16:35:17,234:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:17,235:INFO:Initializing create_model()
2023-05-20 16:35:17,235:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:17,235:INFO:Checking exceptions
2023-05-20 16:35:17,235:INFO:Importing libraries
2023-05-20 16:35:17,235:INFO:Copying training dataset
2023-05-20 16:35:17,244:INFO:Defining folds
2023-05-20 16:35:17,244:INFO:Declaring metric variables
2023-05-20 16:35:17,247:INFO:Importing untrained model
2023-05-20 16:35:17,251:INFO:K Neighbors Regressor Imported successfully
2023-05-20 16:35:17,258:INFO:Starting cross validation
2023-05-20 16:35:17,260:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:20,274:INFO:Calculating mean and std
2023-05-20 16:35:20,276:INFO:Creating metrics dataframe
2023-05-20 16:35:20,608:INFO:Uploading results into container
2023-05-20 16:35:20,608:INFO:Uploading model into container now
2023-05-20 16:35:20,609:INFO:_master_model_container: 91
2023-05-20 16:35:20,609:INFO:_display_container: 7
2023-05-20 16:35:20,609:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 16:35:20,610:INFO:create_model() successfully completed......................................
2023-05-20 16:35:20,710:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:20,710:INFO:Creating metrics dataframe
2023-05-20 16:35:20,720:INFO:Initializing Decision Tree Regressor
2023-05-20 16:35:20,720:INFO:Total runtime is 0.6445794224739074 minutes
2023-05-20 16:35:20,723:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:20,723:INFO:Initializing create_model()
2023-05-20 16:35:20,723:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:20,723:INFO:Checking exceptions
2023-05-20 16:35:20,723:INFO:Importing libraries
2023-05-20 16:35:20,723:INFO:Copying training dataset
2023-05-20 16:35:20,734:INFO:Defining folds
2023-05-20 16:35:20,734:INFO:Declaring metric variables
2023-05-20 16:35:20,739:INFO:Importing untrained model
2023-05-20 16:35:20,742:INFO:Decision Tree Regressor Imported successfully
2023-05-20 16:35:20,748:INFO:Starting cross validation
2023-05-20 16:35:20,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:23,744:INFO:Calculating mean and std
2023-05-20 16:35:23,746:INFO:Creating metrics dataframe
2023-05-20 16:35:24,054:INFO:Uploading results into container
2023-05-20 16:35:24,056:INFO:Uploading model into container now
2023-05-20 16:35:24,056:INFO:_master_model_container: 92
2023-05-20 16:35:24,056:INFO:_display_container: 7
2023-05-20 16:35:24,056:INFO:DecisionTreeRegressor(random_state=5308)
2023-05-20 16:35:24,056:INFO:create_model() successfully completed......................................
2023-05-20 16:35:24,151:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:24,151:INFO:Creating metrics dataframe
2023-05-20 16:35:24,161:INFO:Initializing Random Forest Regressor
2023-05-20 16:35:24,161:INFO:Total runtime is 0.7019395589828491 minutes
2023-05-20 16:35:24,164:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:24,164:INFO:Initializing create_model()
2023-05-20 16:35:24,164:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:24,165:INFO:Checking exceptions
2023-05-20 16:35:24,165:INFO:Importing libraries
2023-05-20 16:35:24,165:INFO:Copying training dataset
2023-05-20 16:35:24,172:INFO:Defining folds
2023-05-20 16:35:24,172:INFO:Declaring metric variables
2023-05-20 16:35:24,177:INFO:Importing untrained model
2023-05-20 16:35:24,180:INFO:Random Forest Regressor Imported successfully
2023-05-20 16:35:24,186:INFO:Starting cross validation
2023-05-20 16:35:24,188:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:27,340:INFO:Calculating mean and std
2023-05-20 16:35:27,342:INFO:Creating metrics dataframe
2023-05-20 16:35:27,680:INFO:Uploading results into container
2023-05-20 16:35:27,681:INFO:Uploading model into container now
2023-05-20 16:35:27,681:INFO:_master_model_container: 93
2023-05-20 16:35:27,681:INFO:_display_container: 7
2023-05-20 16:35:27,682:INFO:RandomForestRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:35:27,682:INFO:create_model() successfully completed......................................
2023-05-20 16:35:27,787:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:27,788:INFO:Creating metrics dataframe
2023-05-20 16:35:27,799:INFO:Initializing Extra Trees Regressor
2023-05-20 16:35:27,800:INFO:Total runtime is 0.7625818808873495 minutes
2023-05-20 16:35:27,803:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:27,803:INFO:Initializing create_model()
2023-05-20 16:35:27,803:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:27,803:INFO:Checking exceptions
2023-05-20 16:35:27,803:INFO:Importing libraries
2023-05-20 16:35:27,804:INFO:Copying training dataset
2023-05-20 16:35:27,812:INFO:Defining folds
2023-05-20 16:35:27,813:INFO:Declaring metric variables
2023-05-20 16:35:27,815:INFO:Importing untrained model
2023-05-20 16:35:27,818:INFO:Extra Trees Regressor Imported successfully
2023-05-20 16:35:27,824:INFO:Starting cross validation
2023-05-20 16:35:27,825:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:31,057:INFO:Calculating mean and std
2023-05-20 16:35:31,060:INFO:Creating metrics dataframe
2023-05-20 16:35:31,365:INFO:Uploading results into container
2023-05-20 16:35:31,367:INFO:Uploading model into container now
2023-05-20 16:35:31,367:INFO:_master_model_container: 94
2023-05-20 16:35:31,369:INFO:_display_container: 7
2023-05-20 16:35:31,369:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5308)
2023-05-20 16:35:31,369:INFO:create_model() successfully completed......................................
2023-05-20 16:35:31,479:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:31,479:INFO:Creating metrics dataframe
2023-05-20 16:35:31,490:INFO:Initializing AdaBoost Regressor
2023-05-20 16:35:31,490:INFO:Total runtime is 0.8240870873133341 minutes
2023-05-20 16:35:31,494:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:31,494:INFO:Initializing create_model()
2023-05-20 16:35:31,494:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:31,496:INFO:Checking exceptions
2023-05-20 16:35:31,496:INFO:Importing libraries
2023-05-20 16:35:31,496:INFO:Copying training dataset
2023-05-20 16:35:31,507:INFO:Defining folds
2023-05-20 16:35:31,507:INFO:Declaring metric variables
2023-05-20 16:35:31,512:INFO:Importing untrained model
2023-05-20 16:35:31,515:INFO:AdaBoost Regressor Imported successfully
2023-05-20 16:35:31,523:INFO:Starting cross validation
2023-05-20 16:35:31,524:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:34,681:INFO:Calculating mean and std
2023-05-20 16:35:34,682:INFO:Creating metrics dataframe
2023-05-20 16:35:34,990:INFO:Uploading results into container
2023-05-20 16:35:34,990:INFO:Uploading model into container now
2023-05-20 16:35:34,991:INFO:_master_model_container: 95
2023-05-20 16:35:34,991:INFO:_display_container: 7
2023-05-20 16:35:34,991:INFO:AdaBoostRegressor(random_state=5308)
2023-05-20 16:35:34,991:INFO:create_model() successfully completed......................................
2023-05-20 16:35:35,082:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:35,082:INFO:Creating metrics dataframe
2023-05-20 16:35:35,098:INFO:Initializing Gradient Boosting Regressor
2023-05-20 16:35:35,098:INFO:Total runtime is 0.8842158198356628 minutes
2023-05-20 16:35:35,101:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:35,102:INFO:Initializing create_model()
2023-05-20 16:35:35,102:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:35,102:INFO:Checking exceptions
2023-05-20 16:35:35,102:INFO:Importing libraries
2023-05-20 16:35:35,102:INFO:Copying training dataset
2023-05-20 16:35:35,111:INFO:Defining folds
2023-05-20 16:35:35,111:INFO:Declaring metric variables
2023-05-20 16:35:35,115:INFO:Importing untrained model
2023-05-20 16:35:35,118:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:35:35,124:INFO:Starting cross validation
2023-05-20 16:35:35,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:38,136:INFO:Calculating mean and std
2023-05-20 16:35:38,138:INFO:Creating metrics dataframe
2023-05-20 16:35:38,463:INFO:Uploading results into container
2023-05-20 16:35:38,465:INFO:Uploading model into container now
2023-05-20 16:35:38,465:INFO:_master_model_container: 96
2023-05-20 16:35:38,465:INFO:_display_container: 7
2023-05-20 16:35:38,466:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:35:38,467:INFO:create_model() successfully completed......................................
2023-05-20 16:35:38,590:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:38,590:INFO:Creating metrics dataframe
2023-05-20 16:35:38,603:INFO:Initializing Extreme Gradient Boosting
2023-05-20 16:35:38,603:INFO:Total runtime is 0.9426320791244506 minutes
2023-05-20 16:35:38,606:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:38,606:INFO:Initializing create_model()
2023-05-20 16:35:38,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:38,606:INFO:Checking exceptions
2023-05-20 16:35:38,606:INFO:Importing libraries
2023-05-20 16:35:38,606:INFO:Copying training dataset
2023-05-20 16:35:38,620:INFO:Defining folds
2023-05-20 16:35:38,620:INFO:Declaring metric variables
2023-05-20 16:35:38,622:INFO:Importing untrained model
2023-05-20 16:35:38,628:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 16:35:38,634:INFO:Starting cross validation
2023-05-20 16:35:38,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:41,638:INFO:Calculating mean and std
2023-05-20 16:35:41,640:INFO:Creating metrics dataframe
2023-05-20 16:35:41,951:INFO:Uploading results into container
2023-05-20 16:35:41,952:INFO:Uploading model into container now
2023-05-20 16:35:41,952:INFO:_master_model_container: 97
2023-05-20 16:35:41,952:INFO:_display_container: 7
2023-05-20 16:35:41,954:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5308, ...)
2023-05-20 16:35:41,954:INFO:create_model() successfully completed......................................
2023-05-20 16:35:42,052:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:42,052:INFO:Creating metrics dataframe
2023-05-20 16:35:42,062:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 16:35:42,062:INFO:Total runtime is 1.0002905925114949 minutes
2023-05-20 16:35:42,066:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:42,066:INFO:Initializing create_model()
2023-05-20 16:35:42,066:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:42,066:INFO:Checking exceptions
2023-05-20 16:35:42,066:INFO:Importing libraries
2023-05-20 16:35:42,066:INFO:Copying training dataset
2023-05-20 16:35:42,074:INFO:Defining folds
2023-05-20 16:35:42,074:INFO:Declaring metric variables
2023-05-20 16:35:42,078:INFO:Importing untrained model
2023-05-20 16:35:42,081:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:35:42,087:INFO:Starting cross validation
2023-05-20 16:35:42,088:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:45,115:INFO:Calculating mean and std
2023-05-20 16:35:45,117:INFO:Creating metrics dataframe
2023-05-20 16:35:45,426:INFO:Uploading results into container
2023-05-20 16:35:45,426:INFO:Uploading model into container now
2023-05-20 16:35:45,427:INFO:_master_model_container: 98
2023-05-20 16:35:45,427:INFO:_display_container: 7
2023-05-20 16:35:45,427:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:35:45,427:INFO:create_model() successfully completed......................................
2023-05-20 16:35:45,521:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:45,521:INFO:Creating metrics dataframe
2023-05-20 16:35:45,532:INFO:Initializing CatBoost Regressor
2023-05-20 16:35:45,532:INFO:Total runtime is 1.058114763100942 minutes
2023-05-20 16:35:45,539:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:45,539:INFO:Initializing create_model()
2023-05-20 16:35:45,539:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:45,539:INFO:Checking exceptions
2023-05-20 16:35:45,539:INFO:Importing libraries
2023-05-20 16:35:45,539:INFO:Copying training dataset
2023-05-20 16:35:45,554:INFO:Defining folds
2023-05-20 16:35:45,554:INFO:Declaring metric variables
2023-05-20 16:35:45,557:INFO:Importing untrained model
2023-05-20 16:35:45,563:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:35:45,574:INFO:Starting cross validation
2023-05-20 16:35:45,576:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:48,552:INFO:Calculating mean and std
2023-05-20 16:35:48,554:INFO:Creating metrics dataframe
2023-05-20 16:35:48,867:INFO:Uploading results into container
2023-05-20 16:35:48,868:INFO:Uploading model into container now
2023-05-20 16:35:48,868:INFO:_master_model_container: 99
2023-05-20 16:35:48,868:INFO:_display_container: 7
2023-05-20 16:35:48,868:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFBF64B50>
2023-05-20 16:35:48,868:INFO:create_model() successfully completed......................................
2023-05-20 16:35:48,962:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:48,963:INFO:Creating metrics dataframe
2023-05-20 16:35:48,973:INFO:Initializing Dummy Regressor
2023-05-20 16:35:48,973:INFO:Total runtime is 1.115462609132131 minutes
2023-05-20 16:35:48,976:INFO:SubProcess create_model() called ==================================
2023-05-20 16:35:48,976:INFO:Initializing create_model()
2023-05-20 16:35:48,976:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025BFBF2E950>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:48,976:INFO:Checking exceptions
2023-05-20 16:35:48,977:INFO:Importing libraries
2023-05-20 16:35:48,977:INFO:Copying training dataset
2023-05-20 16:35:48,987:INFO:Defining folds
2023-05-20 16:35:48,987:INFO:Declaring metric variables
2023-05-20 16:35:48,990:INFO:Importing untrained model
2023-05-20 16:35:48,993:INFO:Dummy Regressor Imported successfully
2023-05-20 16:35:48,998:INFO:Starting cross validation
2023-05-20 16:35:48,999:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 16:35:52,021:INFO:Calculating mean and std
2023-05-20 16:35:52,023:INFO:Creating metrics dataframe
2023-05-20 16:35:52,336:INFO:Uploading results into container
2023-05-20 16:35:52,337:INFO:Uploading model into container now
2023-05-20 16:35:52,337:INFO:_master_model_container: 100
2023-05-20 16:35:52,337:INFO:_display_container: 7
2023-05-20 16:35:52,339:INFO:DummyRegressor()
2023-05-20 16:35:52,339:INFO:create_model() successfully completed......................................
2023-05-20 16:35:52,440:INFO:SubProcess create_model() end ==================================
2023-05-20 16:35:52,440:INFO:Creating metrics dataframe
2023-05-20 16:35:52,460:INFO:Initializing create_model()
2023-05-20 16:35:52,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=<catboost.core.CatBoostRegressor object at 0x0000025BFBF64B50>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:52,460:INFO:Checking exceptions
2023-05-20 16:35:52,462:INFO:Importing libraries
2023-05-20 16:35:52,462:INFO:Copying training dataset
2023-05-20 16:35:52,470:INFO:Defining folds
2023-05-20 16:35:52,471:INFO:Declaring metric variables
2023-05-20 16:35:52,471:INFO:Importing untrained model
2023-05-20 16:35:52,471:INFO:Declaring custom model
2023-05-20 16:35:52,471:INFO:CatBoost Regressor Imported successfully
2023-05-20 16:35:52,472:INFO:Cross validation set to False
2023-05-20 16:35:52,472:INFO:Fitting Model
2023-05-20 16:35:52,805:INFO:<catboost.core.CatBoostRegressor object at 0x0000025BFBF64460>
2023-05-20 16:35:52,805:INFO:create_model() successfully completed......................................
2023-05-20 16:35:52,923:INFO:Initializing create_model()
2023-05-20 16:35:52,923:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:52,923:INFO:Checking exceptions
2023-05-20 16:35:52,925:INFO:Importing libraries
2023-05-20 16:35:52,925:INFO:Copying training dataset
2023-05-20 16:35:52,934:INFO:Defining folds
2023-05-20 16:35:52,934:INFO:Declaring metric variables
2023-05-20 16:35:52,935:INFO:Importing untrained model
2023-05-20 16:35:52,935:INFO:Declaring custom model
2023-05-20 16:35:52,936:INFO:Bayesian Ridge Imported successfully
2023-05-20 16:35:52,937:INFO:Cross validation set to False
2023-05-20 16:35:52,937:INFO:Fitting Model
2023-05-20 16:35:53,289:INFO:BayesianRidge()
2023-05-20 16:35:53,289:INFO:create_model() successfully completed......................................
2023-05-20 16:35:53,403:INFO:Initializing create_model()
2023-05-20 16:35:53,403:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=GradientBoostingRegressor(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:53,403:INFO:Checking exceptions
2023-05-20 16:35:53,405:INFO:Importing libraries
2023-05-20 16:35:53,405:INFO:Copying training dataset
2023-05-20 16:35:53,416:INFO:Defining folds
2023-05-20 16:35:53,416:INFO:Declaring metric variables
2023-05-20 16:35:53,416:INFO:Importing untrained model
2023-05-20 16:35:53,416:INFO:Declaring custom model
2023-05-20 16:35:53,416:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 16:35:53,418:INFO:Cross validation set to False
2023-05-20 16:35:53,418:INFO:Fitting Model
2023-05-20 16:35:53,756:INFO:GradientBoostingRegressor(random_state=5308)
2023-05-20 16:35:53,756:INFO:create_model() successfully completed......................................
2023-05-20 16:35:53,867:INFO:Initializing create_model()
2023-05-20 16:35:53,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=Ridge(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:53,867:INFO:Checking exceptions
2023-05-20 16:35:53,869:INFO:Importing libraries
2023-05-20 16:35:53,869:INFO:Copying training dataset
2023-05-20 16:35:53,879:INFO:Defining folds
2023-05-20 16:35:53,879:INFO:Declaring metric variables
2023-05-20 16:35:53,879:INFO:Importing untrained model
2023-05-20 16:35:53,879:INFO:Declaring custom model
2023-05-20 16:35:53,879:INFO:Ridge Regression Imported successfully
2023-05-20 16:35:53,881:INFO:Cross validation set to False
2023-05-20 16:35:53,881:INFO:Fitting Model
2023-05-20 16:35:54,254:INFO:Ridge(random_state=5308)
2023-05-20 16:35:54,254:INFO:create_model() successfully completed......................................
2023-05-20 16:35:54,458:INFO:Initializing create_model()
2023-05-20 16:35:54,458:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000025BA497DC00>, estimator=LGBMRegressor(random_state=5308), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 16:35:54,458:INFO:Checking exceptions
2023-05-20 16:35:54,462:INFO:Importing libraries
2023-05-20 16:35:54,462:INFO:Copying training dataset
2023-05-20 16:35:54,477:INFO:Defining folds
2023-05-20 16:35:54,477:INFO:Declaring metric variables
2023-05-20 16:35:54,477:INFO:Importing untrained model
2023-05-20 16:35:54,477:INFO:Declaring custom model
2023-05-20 16:35:54,478:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 16:35:54,479:INFO:Cross validation set to False
2023-05-20 16:35:54,479:INFO:Fitting Model
2023-05-20 16:35:54,919:INFO:LGBMRegressor(random_state=5308)
2023-05-20 16:35:54,919:INFO:create_model() successfully completed......................................
2023-05-20 16:35:55,151:INFO:_master_model_container: 100
2023-05-20 16:35:55,151:INFO:_display_container: 7
2023-05-20 16:35:55,152:INFO:[<catboost.core.CatBoostRegressor object at 0x0000025BFBF64460>, BayesianRidge(), GradientBoostingRegressor(random_state=5308), Ridge(random_state=5308), LGBMRegressor(random_state=5308)]
2023-05-20 16:35:55,152:INFO:compare_models() successfully completed......................................
2023-05-20 18:58:24,068:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 18:58:24,150:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 19:33:05,318:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 19:33:05,398:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_5604\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 21:21:51,853:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 21:21:51,854:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 21:21:51,854:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 21:21:51,854:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 21:21:52,628:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-20 21:21:56,845:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_39108\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 21:21:56,909:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_39108\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 22:18:18,238:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 22:18:18,239:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 22:18:18,239:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 22:18:18,239:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-20 22:18:18,753:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-20 22:40:15,653:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:4: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(target, kde=True, fit=scipy.stats.norm)

2023-05-20 22:40:15,728:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:9: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)

2023-05-20 22:40:18,271:INFO:PyCaret RegressionExperiment
2023-05-20 22:40:18,271:INFO:Logging name: reg-default-name
2023-05-20 22:40:18,272:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 22:40:18,272:INFO:version 3.0.2
2023-05-20 22:40:18,272:INFO:Initializing setup()
2023-05-20 22:40:18,272:INFO:self.USI: af39
2023-05-20 22:40:18,272:INFO:self._variable_keys: {'y_train', 'fold_groups_param', 'fold_generator', 'n_jobs_param', 'memory', 'transform_target_param', 'USI', 'y_test', 'X', 'data', 'seed', 'logging_param', 'gpu_n_jobs_param', '_ml_usecase', 'exp_name_log', 'html_param', 'log_plots_param', 'exp_id', 'X_train', 'X_test', 'target_param', 'fold_shuffle_param', 'y', '_available_plots', 'idx', 'gpu_param', 'pipeline'}
2023-05-20 22:40:18,272:INFO:Checking environment
2023-05-20 22:40:18,272:INFO:python_version: 3.10.3
2023-05-20 22:40:18,272:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 22:40:18,272:INFO:machine: AMD64
2023-05-20 22:40:18,272:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 22:40:18,272:INFO:Memory: svmem(total=17083187200, available=3898236928, percent=77.2, used=13184950272, free=3898236928)
2023-05-20 22:40:18,272:INFO:Physical Core: 6
2023-05-20 22:40:18,272:INFO:Logical Core: 12
2023-05-20 22:40:18,272:INFO:Checking libraries
2023-05-20 22:40:18,272:INFO:System:
2023-05-20 22:40:18,272:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 22:40:18,272:INFO:executable: c:\Python310\python.exe
2023-05-20 22:40:18,272:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 22:40:18,272:INFO:PyCaret required dependencies:
2023-05-20 22:40:18,272:INFO:                 pip: 23.1.2
2023-05-20 22:40:18,273:INFO:          setuptools: 58.1.0
2023-05-20 22:40:18,273:INFO:             pycaret: 3.0.2
2023-05-20 22:40:18,273:INFO:             IPython: 8.5.0
2023-05-20 22:40:18,273:INFO:          ipywidgets: 8.0.6
2023-05-20 22:40:18,273:INFO:                tqdm: 4.65.0
2023-05-20 22:40:18,273:INFO:               numpy: 1.23.2
2023-05-20 22:40:18,273:INFO:              pandas: 1.5.2
2023-05-20 22:40:18,273:INFO:              jinja2: 3.1.2
2023-05-20 22:40:18,273:INFO:               scipy: 1.9.3
2023-05-20 22:40:18,273:INFO:              joblib: 1.2.0
2023-05-20 22:40:18,273:INFO:             sklearn: 1.1.3
2023-05-20 22:40:18,273:INFO:                pyod: 1.0.9
2023-05-20 22:40:18,273:INFO:            imblearn: 0.10.1
2023-05-20 22:40:18,273:INFO:   category_encoders: 2.6.1
2023-05-20 22:40:18,273:INFO:            lightgbm: 3.3.5
2023-05-20 22:40:18,273:INFO:               numba: 0.57.0
2023-05-20 22:40:18,273:INFO:            requests: 2.28.2
2023-05-20 22:40:18,273:INFO:          matplotlib: 3.5.3
2023-05-20 22:40:18,273:INFO:          scikitplot: 0.3.7
2023-05-20 22:40:18,273:INFO:         yellowbrick: 1.5
2023-05-20 22:40:18,274:INFO:              plotly: 5.13.1
2023-05-20 22:40:18,274:INFO:             kaleido: 0.2.1
2023-05-20 22:40:18,274:INFO:         statsmodels: 0.13.5
2023-05-20 22:40:18,274:INFO:              sktime: 0.17.0
2023-05-20 22:40:18,274:INFO:               tbats: 1.1.3
2023-05-20 22:40:18,274:INFO:            pmdarima: 2.0.3
2023-05-20 22:40:18,274:INFO:              psutil: 5.9.2
2023-05-20 22:40:18,274:INFO:PyCaret optional dependencies:
2023-05-20 22:40:18,286:INFO:                shap: Not installed
2023-05-20 22:40:18,287:INFO:           interpret: Not installed
2023-05-20 22:40:18,287:INFO:                umap: Not installed
2023-05-20 22:40:18,287:INFO:    pandas_profiling: Not installed
2023-05-20 22:40:18,287:INFO:  explainerdashboard: Not installed
2023-05-20 22:40:18,287:INFO:             autoviz: Not installed
2023-05-20 22:40:18,287:INFO:           fairlearn: Not installed
2023-05-20 22:40:18,287:INFO:             xgboost: 1.7.4
2023-05-20 22:40:18,287:INFO:            catboost: 1.1.1
2023-05-20 22:40:18,287:INFO:              kmodes: Not installed
2023-05-20 22:40:18,287:INFO:             mlxtend: Not installed
2023-05-20 22:40:18,287:INFO:       statsforecast: Not installed
2023-05-20 22:40:18,287:INFO:        tune_sklearn: Not installed
2023-05-20 22:40:18,287:INFO:                 ray: Not installed
2023-05-20 22:40:18,287:INFO:            hyperopt: Not installed
2023-05-20 22:40:18,287:INFO:              optuna: Not installed
2023-05-20 22:40:18,287:INFO:               skopt: Not installed
2023-05-20 22:40:18,287:INFO:              mlflow: Not installed
2023-05-20 22:40:18,287:INFO:              gradio: Not installed
2023-05-20 22:40:18,287:INFO:             fastapi: Not installed
2023-05-20 22:40:18,287:INFO:             uvicorn: Not installed
2023-05-20 22:40:18,287:INFO:              m2cgen: Not installed
2023-05-20 22:40:18,287:INFO:           evidently: Not installed
2023-05-20 22:40:18,287:INFO:               fugue: Not installed
2023-05-20 22:40:18,288:INFO:           streamlit: Not installed
2023-05-20 22:40:18,288:INFO:             prophet: Not installed
2023-05-20 22:40:18,288:INFO:None
2023-05-20 22:40:18,288:INFO:Set up data.
2023-05-20 22:40:18,374:INFO:Set up train/test split.
2023-05-20 22:40:18,384:INFO:Set up index.
2023-05-20 22:40:18,384:INFO:Set up folding strategy.
2023-05-20 22:40:18,385:INFO:Assigning column types.
2023-05-20 22:40:18,394:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 22:40:18,394:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,398:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,402:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,461:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,516:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,517:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:18,610:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:18,611:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,615:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,620:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,678:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,718:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,719:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:18,722:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:18,722:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 22:40:18,727:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,732:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,819:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,869:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,874:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:18,879:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:18,887:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,894:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:18,965:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,003:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,003:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,006:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,006:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 22:40:19,014:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,069:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,107:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,108:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,110:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,118:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,171:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,210:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,210:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,212:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,215:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 22:40:19,276:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,315:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,316:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,318:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,381:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,418:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,419:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,421:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,421:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 22:40:19,483:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,520:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,522:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,585:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 22:40:19,632:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,635:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,635:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 22:40:19,740:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,745:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,848:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:19,850:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:19,851:INFO:Preparing preprocessing pipeline...
2023-05-20 22:40:19,852:INFO:Set up simple imputation.
2023-05-20 22:40:19,853:INFO:Set up column name cleaning.
2023-05-20 22:40:19,908:INFO:Finished creating preprocessing pipeline.
2023-05-20 22:40:19,916:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 22:40:19,916:INFO:Creating final display dataframe.
2023-05-20 22:40:20,087:INFO:Setup _display_container:                     Description             Value
0                    Session id              2934
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 318)
4        Transformed data shape       (1460, 318)
5   Transformed train set shape       (1021, 318)
6    Transformed test set shape        (439, 318)
7              Numeric features               317
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              af39
2023-05-20 22:40:20,203:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:20,205:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:20,309:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 22:40:20,311:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 22:40:20,312:INFO:setup() successfully completed in 2.51s...............
2023-05-20 22:40:20,363:INFO:Initializing compare_models()
2023-05-20 22:40:20,363:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 22:40:20,363:INFO:Checking exceptions
2023-05-20 22:40:20,371:INFO:Preparing display monitor
2023-05-20 22:40:20,419:INFO:Initializing Linear Regression
2023-05-20 22:40:20,419:INFO:Total runtime is 0.0 minutes
2023-05-20 22:40:20,423:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:20,424:INFO:Initializing create_model()
2023-05-20 22:40:20,424:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:20,424:INFO:Checking exceptions
2023-05-20 22:40:20,424:INFO:Importing libraries
2023-05-20 22:40:20,424:INFO:Copying training dataset
2023-05-20 22:40:20,435:INFO:Defining folds
2023-05-20 22:40:20,436:INFO:Declaring metric variables
2023-05-20 22:40:20,438:INFO:Importing untrained model
2023-05-20 22:40:20,443:INFO:Linear Regression Imported successfully
2023-05-20 22:40:20,449:INFO:Starting cross validation
2023-05-20 22:40:20,458:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:29,034:INFO:Calculating mean and std
2023-05-20 22:40:29,036:INFO:Creating metrics dataframe
2023-05-20 22:40:29,362:INFO:Uploading results into container
2023-05-20 22:40:29,363:INFO:Uploading model into container now
2023-05-20 22:40:29,363:INFO:_master_model_container: 1
2023-05-20 22:40:29,364:INFO:_display_container: 2
2023-05-20 22:40:29,364:INFO:LinearRegression(n_jobs=-1)
2023-05-20 22:40:29,364:INFO:create_model() successfully completed......................................
2023-05-20 22:40:29,550:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:29,550:INFO:Creating metrics dataframe
2023-05-20 22:40:29,560:INFO:Initializing Lasso Regression
2023-05-20 22:40:29,560:INFO:Total runtime is 0.15233708222707112 minutes
2023-05-20 22:40:29,562:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:29,563:INFO:Initializing create_model()
2023-05-20 22:40:29,563:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:29,563:INFO:Checking exceptions
2023-05-20 22:40:29,563:INFO:Importing libraries
2023-05-20 22:40:29,563:INFO:Copying training dataset
2023-05-20 22:40:29,573:INFO:Defining folds
2023-05-20 22:40:29,573:INFO:Declaring metric variables
2023-05-20 22:40:29,577:INFO:Importing untrained model
2023-05-20 22:40:29,580:INFO:Lasso Regression Imported successfully
2023-05-20 22:40:29,586:INFO:Starting cross validation
2023-05-20 22:40:29,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:33,196:INFO:Calculating mean and std
2023-05-20 22:40:33,199:INFO:Creating metrics dataframe
2023-05-20 22:40:33,558:INFO:Uploading results into container
2023-05-20 22:40:33,559:INFO:Uploading model into container now
2023-05-20 22:40:33,560:INFO:_master_model_container: 2
2023-05-20 22:40:33,560:INFO:_display_container: 2
2023-05-20 22:40:33,560:INFO:Lasso(random_state=2934)
2023-05-20 22:40:33,560:INFO:create_model() successfully completed......................................
2023-05-20 22:40:33,642:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:33,642:INFO:Creating metrics dataframe
2023-05-20 22:40:33,651:INFO:Initializing Ridge Regression
2023-05-20 22:40:33,651:INFO:Total runtime is 0.22052190303802488 minutes
2023-05-20 22:40:33,655:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:33,655:INFO:Initializing create_model()
2023-05-20 22:40:33,655:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:33,655:INFO:Checking exceptions
2023-05-20 22:40:33,655:INFO:Importing libraries
2023-05-20 22:40:33,655:INFO:Copying training dataset
2023-05-20 22:40:33,666:INFO:Defining folds
2023-05-20 22:40:33,667:INFO:Declaring metric variables
2023-05-20 22:40:33,670:INFO:Importing untrained model
2023-05-20 22:40:33,674:INFO:Ridge Regression Imported successfully
2023-05-20 22:40:33,679:INFO:Starting cross validation
2023-05-20 22:40:33,682:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:36,880:INFO:Calculating mean and std
2023-05-20 22:40:36,882:INFO:Creating metrics dataframe
2023-05-20 22:40:37,208:INFO:Uploading results into container
2023-05-20 22:40:37,209:INFO:Uploading model into container now
2023-05-20 22:40:37,210:INFO:_master_model_container: 3
2023-05-20 22:40:37,210:INFO:_display_container: 2
2023-05-20 22:40:37,210:INFO:Ridge(random_state=2934)
2023-05-20 22:40:37,211:INFO:create_model() successfully completed......................................
2023-05-20 22:40:37,293:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:37,293:INFO:Creating metrics dataframe
2023-05-20 22:40:37,302:INFO:Initializing Elastic Net
2023-05-20 22:40:37,302:INFO:Total runtime is 0.2813725511233012 minutes
2023-05-20 22:40:37,306:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:37,307:INFO:Initializing create_model()
2023-05-20 22:40:37,307:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:37,307:INFO:Checking exceptions
2023-05-20 22:40:37,307:INFO:Importing libraries
2023-05-20 22:40:37,308:INFO:Copying training dataset
2023-05-20 22:40:37,334:INFO:Defining folds
2023-05-20 22:40:37,335:INFO:Declaring metric variables
2023-05-20 22:40:37,347:INFO:Importing untrained model
2023-05-20 22:40:37,357:INFO:Elastic Net Imported successfully
2023-05-20 22:40:37,373:INFO:Starting cross validation
2023-05-20 22:40:37,375:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:40,512:INFO:Calculating mean and std
2023-05-20 22:40:40,518:INFO:Creating metrics dataframe
2023-05-20 22:40:40,863:INFO:Uploading results into container
2023-05-20 22:40:40,863:INFO:Uploading model into container now
2023-05-20 22:40:40,864:INFO:_master_model_container: 4
2023-05-20 22:40:40,864:INFO:_display_container: 2
2023-05-20 22:40:40,864:INFO:ElasticNet(random_state=2934)
2023-05-20 22:40:40,865:INFO:create_model() successfully completed......................................
2023-05-20 22:40:40,947:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:40,947:INFO:Creating metrics dataframe
2023-05-20 22:40:40,958:INFO:Initializing Least Angle Regression
2023-05-20 22:40:40,958:INFO:Total runtime is 0.3423122962315877 minutes
2023-05-20 22:40:40,961:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:40,961:INFO:Initializing create_model()
2023-05-20 22:40:40,961:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:40,961:INFO:Checking exceptions
2023-05-20 22:40:40,961:INFO:Importing libraries
2023-05-20 22:40:40,961:INFO:Copying training dataset
2023-05-20 22:40:40,971:INFO:Defining folds
2023-05-20 22:40:40,972:INFO:Declaring metric variables
2023-05-20 22:40:40,975:INFO:Importing untrained model
2023-05-20 22:40:40,977:INFO:Least Angle Regression Imported successfully
2023-05-20 22:40:40,985:INFO:Starting cross validation
2023-05-20 22:40:40,987:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:41,152:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,152:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,162:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,176:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,181:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,181:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,201:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,208:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.510e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,208:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.873e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=9.372e-04, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,210:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.164e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,210:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.265e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,210:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.384e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,213:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,214:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=6.637e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,215:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=8.499e-04, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:41,218:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.412e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,221:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.176e-04, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,226:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.561e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,226:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.546e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,228:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.016e-04, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=9.639e-05, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,230:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.920e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,235:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.981e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,238:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.849e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,243:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.311e-04, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,245:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.731e-04, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,248:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.909e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,251:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=1.413e-04, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,257:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=2.442e-04, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,257:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=1.020e-03, with an active set of 136 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,260:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=9.466e-05, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,263:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=7.449e-05, with an active set of 149 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,272:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=9.459e-05, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,274:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=9.005e-05, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,285:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.457e-03, with an active set of 124 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,288:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.456e-03, with an active set of 127 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,288:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 190 iterations, i.e. alpha=1.761e-01, with an active set of 150 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,291:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 224 iterations, i.e. alpha=4.451e-04, with an active set of 189 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,292:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 203 iterations, i.e. alpha=6.277e-05, with an active set of 180 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,300:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 244 iterations, i.e. alpha=2.590e-04, with an active set of 199 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,301:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 210 iterations, i.e. alpha=1.722e-01, with an active set of 168 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,301:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 233 iterations, i.e. alpha=8.134e-03, with an active set of 191 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,341:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 22:40:41,341:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 22:40:41,343:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 22:40:41,344:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 22:40:41,349:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 22:40:41,364:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: invalid value encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 22:40:41,382:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 405 iterations, i.e. alpha=3.763e+34, with an active set of 178 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 22:40:41,437:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in add
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 22:40:41,439:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in subtract
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 22:40:41,440:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:776: RuntimeWarning: overflow encountered in subtract
  Cov -= gamma_ * corr_eq_dir

2023-05-20 22:40:41,441:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: invalid value encountered in subtract
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 22:40:41,455:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: invalid value encountered in add
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 22:40:41,462:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 22:40:41,463:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 22:40:41,464:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 22:40:41,726:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 22:40:41,730:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,730:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,731:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,731:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,731:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,732:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,735:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,735:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,736:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,736:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,736:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,737:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,753:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,753:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,754:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,754:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,755:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,755:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,761:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 22:40:41,767:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,767:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,768:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:41,793:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,793:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 22:40:41,794:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 22:40:44,680:INFO:Calculating mean and std
2023-05-20 22:40:44,695:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)

2023-05-20 22:40:44,698:INFO:Creating metrics dataframe
2023-05-20 22:40:45,025:INFO:Uploading results into container
2023-05-20 22:40:45,026:INFO:Uploading model into container now
2023-05-20 22:40:45,027:INFO:_master_model_container: 5
2023-05-20 22:40:45,027:INFO:_display_container: 2
2023-05-20 22:40:45,027:INFO:Lars(random_state=2934)
2023-05-20 22:40:45,027:INFO:create_model() successfully completed......................................
2023-05-20 22:40:45,102:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:45,102:INFO:Creating metrics dataframe
2023-05-20 22:40:45,111:INFO:Initializing Lasso Least Angle Regression
2023-05-20 22:40:45,111:INFO:Total runtime is 0.41153310537338256 minutes
2023-05-20 22:40:45,114:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:45,114:INFO:Initializing create_model()
2023-05-20 22:40:45,114:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:45,114:INFO:Checking exceptions
2023-05-20 22:40:45,114:INFO:Importing libraries
2023-05-20 22:40:45,115:INFO:Copying training dataset
2023-05-20 22:40:45,123:INFO:Defining folds
2023-05-20 22:40:45,124:INFO:Declaring metric variables
2023-05-20 22:40:45,127:INFO:Importing untrained model
2023-05-20 22:40:45,130:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 22:40:45,137:INFO:Starting cross validation
2023-05-20 22:40:45,139:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:45,264:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,275:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,276:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,296:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,301:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,315:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,327:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,336:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,344:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:45,358:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 22:40:48,292:INFO:Calculating mean and std
2023-05-20 22:40:48,293:INFO:Creating metrics dataframe
2023-05-20 22:40:48,652:INFO:Uploading results into container
2023-05-20 22:40:48,654:INFO:Uploading model into container now
2023-05-20 22:40:48,654:INFO:_master_model_container: 6
2023-05-20 22:40:48,654:INFO:_display_container: 2
2023-05-20 22:40:48,655:INFO:LassoLars(random_state=2934)
2023-05-20 22:40:48,655:INFO:create_model() successfully completed......................................
2023-05-20 22:40:48,732:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:48,732:INFO:Creating metrics dataframe
2023-05-20 22:40:48,743:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 22:40:48,743:INFO:Total runtime is 0.4720578749974569 minutes
2023-05-20 22:40:48,748:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:48,749:INFO:Initializing create_model()
2023-05-20 22:40:48,749:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:48,749:INFO:Checking exceptions
2023-05-20 22:40:48,749:INFO:Importing libraries
2023-05-20 22:40:48,749:INFO:Copying training dataset
2023-05-20 22:40:48,773:INFO:Defining folds
2023-05-20 22:40:48,773:INFO:Declaring metric variables
2023-05-20 22:40:48,783:INFO:Importing untrained model
2023-05-20 22:40:48,794:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 22:40:48,805:INFO:Starting cross validation
2023-05-20 22:40:48,809:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:48,970:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:48,972:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:48,980:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:48,984:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:49,003:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:49,018:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:49,032:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:49,036:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:49,040:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:40:52,002:INFO:Calculating mean and std
2023-05-20 22:40:52,004:INFO:Creating metrics dataframe
2023-05-20 22:40:52,371:INFO:Uploading results into container
2023-05-20 22:40:52,372:INFO:Uploading model into container now
2023-05-20 22:40:52,372:INFO:_master_model_container: 7
2023-05-20 22:40:52,372:INFO:_display_container: 2
2023-05-20 22:40:52,373:INFO:OrthogonalMatchingPursuit()
2023-05-20 22:40:52,373:INFO:create_model() successfully completed......................................
2023-05-20 22:40:52,494:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:52,494:INFO:Creating metrics dataframe
2023-05-20 22:40:52,504:INFO:Initializing Bayesian Ridge
2023-05-20 22:40:52,505:INFO:Total runtime is 0.5347663482030233 minutes
2023-05-20 22:40:52,510:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:52,510:INFO:Initializing create_model()
2023-05-20 22:40:52,511:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:52,511:INFO:Checking exceptions
2023-05-20 22:40:52,511:INFO:Importing libraries
2023-05-20 22:40:52,511:INFO:Copying training dataset
2023-05-20 22:40:52,534:INFO:Defining folds
2023-05-20 22:40:52,534:INFO:Declaring metric variables
2023-05-20 22:40:52,540:INFO:Importing untrained model
2023-05-20 22:40:52,545:INFO:Bayesian Ridge Imported successfully
2023-05-20 22:40:52,556:INFO:Starting cross validation
2023-05-20 22:40:52,560:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:55,827:INFO:Calculating mean and std
2023-05-20 22:40:55,829:INFO:Creating metrics dataframe
2023-05-20 22:40:56,150:INFO:Uploading results into container
2023-05-20 22:40:56,151:INFO:Uploading model into container now
2023-05-20 22:40:56,151:INFO:_master_model_container: 8
2023-05-20 22:40:56,152:INFO:_display_container: 2
2023-05-20 22:40:56,152:INFO:BayesianRidge()
2023-05-20 22:40:56,152:INFO:create_model() successfully completed......................................
2023-05-20 22:40:56,240:INFO:SubProcess create_model() end ==================================
2023-05-20 22:40:56,240:INFO:Creating metrics dataframe
2023-05-20 22:40:56,250:INFO:Initializing Passive Aggressive Regressor
2023-05-20 22:40:56,250:INFO:Total runtime is 0.5971725821495056 minutes
2023-05-20 22:40:56,254:INFO:SubProcess create_model() called ==================================
2023-05-20 22:40:56,255:INFO:Initializing create_model()
2023-05-20 22:40:56,255:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:40:56,255:INFO:Checking exceptions
2023-05-20 22:40:56,255:INFO:Importing libraries
2023-05-20 22:40:56,255:INFO:Copying training dataset
2023-05-20 22:40:56,267:INFO:Defining folds
2023-05-20 22:40:56,268:INFO:Declaring metric variables
2023-05-20 22:40:56,271:INFO:Importing untrained model
2023-05-20 22:40:56,276:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 22:40:56,281:INFO:Starting cross validation
2023-05-20 22:40:56,283:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:40:59,525:INFO:Calculating mean and std
2023-05-20 22:40:59,526:INFO:Creating metrics dataframe
2023-05-20 22:40:59,916:INFO:Uploading results into container
2023-05-20 22:40:59,917:INFO:Uploading model into container now
2023-05-20 22:40:59,917:INFO:_master_model_container: 9
2023-05-20 22:40:59,918:INFO:_display_container: 2
2023-05-20 22:40:59,918:INFO:PassiveAggressiveRegressor(random_state=2934)
2023-05-20 22:40:59,918:INFO:create_model() successfully completed......................................
2023-05-20 22:41:00,002:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:00,002:INFO:Creating metrics dataframe
2023-05-20 22:41:00,014:INFO:Initializing Huber Regressor
2023-05-20 22:41:00,014:INFO:Total runtime is 0.6599089463551839 minutes
2023-05-20 22:41:00,017:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:00,017:INFO:Initializing create_model()
2023-05-20 22:41:00,017:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:00,017:INFO:Checking exceptions
2023-05-20 22:41:00,017:INFO:Importing libraries
2023-05-20 22:41:00,017:INFO:Copying training dataset
2023-05-20 22:41:00,038:INFO:Defining folds
2023-05-20 22:41:00,038:INFO:Declaring metric variables
2023-05-20 22:41:00,045:INFO:Importing untrained model
2023-05-20 22:41:00,052:INFO:Huber Regressor Imported successfully
2023-05-20 22:41:00,066:INFO:Starting cross validation
2023-05-20 22:41:00,069:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:02,849:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,228:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,289:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,421:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,455:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,484:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,643:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,644:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,768:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:03,829:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 22:41:07,022:INFO:Calculating mean and std
2023-05-20 22:41:07,025:INFO:Creating metrics dataframe
2023-05-20 22:41:07,383:INFO:Uploading results into container
2023-05-20 22:41:07,385:INFO:Uploading model into container now
2023-05-20 22:41:07,385:INFO:_master_model_container: 10
2023-05-20 22:41:07,386:INFO:_display_container: 2
2023-05-20 22:41:07,386:INFO:HuberRegressor()
2023-05-20 22:41:07,386:INFO:create_model() successfully completed......................................
2023-05-20 22:41:07,466:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:07,466:INFO:Creating metrics dataframe
2023-05-20 22:41:07,477:INFO:Initializing K Neighbors Regressor
2023-05-20 22:41:07,477:INFO:Total runtime is 0.7842959086100261 minutes
2023-05-20 22:41:07,481:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:07,481:INFO:Initializing create_model()
2023-05-20 22:41:07,481:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:07,482:INFO:Checking exceptions
2023-05-20 22:41:07,482:INFO:Importing libraries
2023-05-20 22:41:07,482:INFO:Copying training dataset
2023-05-20 22:41:07,492:INFO:Defining folds
2023-05-20 22:41:07,492:INFO:Declaring metric variables
2023-05-20 22:41:07,496:INFO:Importing untrained model
2023-05-20 22:41:07,499:INFO:K Neighbors Regressor Imported successfully
2023-05-20 22:41:07,505:INFO:Starting cross validation
2023-05-20 22:41:07,507:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:10,611:INFO:Calculating mean and std
2023-05-20 22:41:10,613:INFO:Creating metrics dataframe
2023-05-20 22:41:10,947:INFO:Uploading results into container
2023-05-20 22:41:10,947:INFO:Uploading model into container now
2023-05-20 22:41:10,949:INFO:_master_model_container: 11
2023-05-20 22:41:10,950:INFO:_display_container: 2
2023-05-20 22:41:10,950:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 22:41:10,950:INFO:create_model() successfully completed......................................
2023-05-20 22:41:11,041:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:11,041:INFO:Creating metrics dataframe
2023-05-20 22:41:11,052:INFO:Initializing Decision Tree Regressor
2023-05-20 22:41:11,053:INFO:Total runtime is 0.8438894589742025 minutes
2023-05-20 22:41:11,056:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:11,056:INFO:Initializing create_model()
2023-05-20 22:41:11,056:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:11,056:INFO:Checking exceptions
2023-05-20 22:41:11,056:INFO:Importing libraries
2023-05-20 22:41:11,056:INFO:Copying training dataset
2023-05-20 22:41:11,067:INFO:Defining folds
2023-05-20 22:41:11,067:INFO:Declaring metric variables
2023-05-20 22:41:11,072:INFO:Importing untrained model
2023-05-20 22:41:11,076:INFO:Decision Tree Regressor Imported successfully
2023-05-20 22:41:11,082:INFO:Starting cross validation
2023-05-20 22:41:11,085:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:14,195:INFO:Calculating mean and std
2023-05-20 22:41:14,196:INFO:Creating metrics dataframe
2023-05-20 22:41:14,558:INFO:Uploading results into container
2023-05-20 22:41:14,560:INFO:Uploading model into container now
2023-05-20 22:41:14,560:INFO:_master_model_container: 12
2023-05-20 22:41:14,561:INFO:_display_container: 2
2023-05-20 22:41:14,561:INFO:DecisionTreeRegressor(random_state=2934)
2023-05-20 22:41:14,562:INFO:create_model() successfully completed......................................
2023-05-20 22:41:14,647:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:14,648:INFO:Creating metrics dataframe
2023-05-20 22:41:14,660:INFO:Initializing Random Forest Regressor
2023-05-20 22:41:14,660:INFO:Total runtime is 0.9040133277575175 minutes
2023-05-20 22:41:14,664:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:14,664:INFO:Initializing create_model()
2023-05-20 22:41:14,664:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:14,665:INFO:Checking exceptions
2023-05-20 22:41:14,665:INFO:Importing libraries
2023-05-20 22:41:14,665:INFO:Copying training dataset
2023-05-20 22:41:14,676:INFO:Defining folds
2023-05-20 22:41:14,676:INFO:Declaring metric variables
2023-05-20 22:41:14,679:INFO:Importing untrained model
2023-05-20 22:41:14,683:INFO:Random Forest Regressor Imported successfully
2023-05-20 22:41:14,688:INFO:Starting cross validation
2023-05-20 22:41:14,691:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:16,962:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.11s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 22:41:17,197:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.36s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 22:41:17,223:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.28s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 22:41:20,406:INFO:Calculating mean and std
2023-05-20 22:41:20,408:INFO:Creating metrics dataframe
2023-05-20 22:41:20,746:INFO:Uploading results into container
2023-05-20 22:41:20,747:INFO:Uploading model into container now
2023-05-20 22:41:20,748:INFO:_master_model_container: 13
2023-05-20 22:41:20,748:INFO:_display_container: 2
2023-05-20 22:41:20,749:INFO:RandomForestRegressor(n_jobs=-1, random_state=2934)
2023-05-20 22:41:20,749:INFO:create_model() successfully completed......................................
2023-05-20 22:41:20,838:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:20,838:INFO:Creating metrics dataframe
2023-05-20 22:41:20,849:INFO:Initializing Extra Trees Regressor
2023-05-20 22:41:20,849:INFO:Total runtime is 1.007163163026174 minutes
2023-05-20 22:41:20,853:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:20,853:INFO:Initializing create_model()
2023-05-20 22:41:20,853:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:20,853:INFO:Checking exceptions
2023-05-20 22:41:20,853:INFO:Importing libraries
2023-05-20 22:41:20,853:INFO:Copying training dataset
2023-05-20 22:41:20,864:INFO:Defining folds
2023-05-20 22:41:20,864:INFO:Declaring metric variables
2023-05-20 22:41:20,868:INFO:Importing untrained model
2023-05-20 22:41:20,871:INFO:Extra Trees Regressor Imported successfully
2023-05-20 22:41:20,876:INFO:Starting cross validation
2023-05-20 22:41:20,877:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:23,430:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.32s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 22:41:23,535:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.45s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 22:41:26,858:INFO:Calculating mean and std
2023-05-20 22:41:26,860:INFO:Creating metrics dataframe
2023-05-20 22:41:27,235:INFO:Uploading results into container
2023-05-20 22:41:27,236:INFO:Uploading model into container now
2023-05-20 22:41:27,236:INFO:_master_model_container: 14
2023-05-20 22:41:27,238:INFO:_display_container: 2
2023-05-20 22:41:27,238:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=2934)
2023-05-20 22:41:27,238:INFO:create_model() successfully completed......................................
2023-05-20 22:41:27,324:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:27,324:INFO:Creating metrics dataframe
2023-05-20 22:41:27,336:INFO:Initializing AdaBoost Regressor
2023-05-20 22:41:27,336:INFO:Total runtime is 1.115275462468465 minutes
2023-05-20 22:41:27,341:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:27,341:INFO:Initializing create_model()
2023-05-20 22:41:27,341:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:27,341:INFO:Checking exceptions
2023-05-20 22:41:27,341:INFO:Importing libraries
2023-05-20 22:41:27,341:INFO:Copying training dataset
2023-05-20 22:41:27,352:INFO:Defining folds
2023-05-20 22:41:27,352:INFO:Declaring metric variables
2023-05-20 22:41:27,355:INFO:Importing untrained model
2023-05-20 22:41:27,359:INFO:AdaBoost Regressor Imported successfully
2023-05-20 22:41:27,365:INFO:Starting cross validation
2023-05-20 22:41:27,367:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:32,274:INFO:Calculating mean and std
2023-05-20 22:41:32,276:INFO:Creating metrics dataframe
2023-05-20 22:41:32,624:INFO:Uploading results into container
2023-05-20 22:41:32,625:INFO:Uploading model into container now
2023-05-20 22:41:32,626:INFO:_master_model_container: 15
2023-05-20 22:41:32,626:INFO:_display_container: 2
2023-05-20 22:41:32,626:INFO:AdaBoostRegressor(random_state=2934)
2023-05-20 22:41:32,626:INFO:create_model() successfully completed......................................
2023-05-20 22:41:32,704:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:32,704:INFO:Creating metrics dataframe
2023-05-20 22:41:32,715:INFO:Initializing Gradient Boosting Regressor
2023-05-20 22:41:32,715:INFO:Total runtime is 1.2049273173014323 minutes
2023-05-20 22:41:32,718:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:32,719:INFO:Initializing create_model()
2023-05-20 22:41:32,719:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:32,719:INFO:Checking exceptions
2023-05-20 22:41:32,719:INFO:Importing libraries
2023-05-20 22:41:32,719:INFO:Copying training dataset
2023-05-20 22:41:32,729:INFO:Defining folds
2023-05-20 22:41:32,729:INFO:Declaring metric variables
2023-05-20 22:41:32,733:INFO:Importing untrained model
2023-05-20 22:41:32,737:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 22:41:32,744:INFO:Starting cross validation
2023-05-20 22:41:32,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:37,719:INFO:Calculating mean and std
2023-05-20 22:41:37,723:INFO:Creating metrics dataframe
2023-05-20 22:41:38,121:INFO:Uploading results into container
2023-05-20 22:41:38,122:INFO:Uploading model into container now
2023-05-20 22:41:38,123:INFO:_master_model_container: 16
2023-05-20 22:41:38,123:INFO:_display_container: 2
2023-05-20 22:41:38,124:INFO:GradientBoostingRegressor(random_state=2934)
2023-05-20 22:41:38,124:INFO:create_model() successfully completed......................................
2023-05-20 22:41:38,233:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:38,233:INFO:Creating metrics dataframe
2023-05-20 22:41:38,246:INFO:Initializing Extreme Gradient Boosting
2023-05-20 22:41:38,246:INFO:Total runtime is 1.2971163630485536 minutes
2023-05-20 22:41:38,251:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:38,252:INFO:Initializing create_model()
2023-05-20 22:41:38,252:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:38,252:INFO:Checking exceptions
2023-05-20 22:41:38,252:INFO:Importing libraries
2023-05-20 22:41:38,252:INFO:Copying training dataset
2023-05-20 22:41:38,266:INFO:Defining folds
2023-05-20 22:41:38,266:INFO:Declaring metric variables
2023-05-20 22:41:38,271:INFO:Importing untrained model
2023-05-20 22:41:38,275:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 22:41:38,282:INFO:Starting cross validation
2023-05-20 22:41:38,284:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:44,275:INFO:Calculating mean and std
2023-05-20 22:41:44,277:INFO:Creating metrics dataframe
2023-05-20 22:41:44,657:INFO:Uploading results into container
2023-05-20 22:41:44,658:INFO:Uploading model into container now
2023-05-20 22:41:44,659:INFO:_master_model_container: 17
2023-05-20 22:41:44,659:INFO:_display_container: 2
2023-05-20 22:41:44,660:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=2934, ...)
2023-05-20 22:41:44,660:INFO:create_model() successfully completed......................................
2023-05-20 22:41:44,776:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:44,776:INFO:Creating metrics dataframe
2023-05-20 22:41:44,791:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 22:41:44,791:INFO:Total runtime is 1.4061878283818565 minutes
2023-05-20 22:41:44,794:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:44,795:INFO:Initializing create_model()
2023-05-20 22:41:44,795:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:44,795:INFO:Checking exceptions
2023-05-20 22:41:44,795:INFO:Importing libraries
2023-05-20 22:41:44,795:INFO:Copying training dataset
2023-05-20 22:41:44,810:INFO:Defining folds
2023-05-20 22:41:44,810:INFO:Declaring metric variables
2023-05-20 22:41:44,814:INFO:Importing untrained model
2023-05-20 22:41:44,820:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 22:41:44,830:INFO:Starting cross validation
2023-05-20 22:41:44,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:41:49,214:INFO:Calculating mean and std
2023-05-20 22:41:49,216:INFO:Creating metrics dataframe
2023-05-20 22:41:49,611:INFO:Uploading results into container
2023-05-20 22:41:49,612:INFO:Uploading model into container now
2023-05-20 22:41:49,613:INFO:_master_model_container: 18
2023-05-20 22:41:49,613:INFO:_display_container: 2
2023-05-20 22:41:49,613:INFO:LGBMRegressor(random_state=2934)
2023-05-20 22:41:49,613:INFO:create_model() successfully completed......................................
2023-05-20 22:41:49,710:INFO:SubProcess create_model() end ==================================
2023-05-20 22:41:49,710:INFO:Creating metrics dataframe
2023-05-20 22:41:49,723:INFO:Initializing CatBoost Regressor
2023-05-20 22:41:49,723:INFO:Total runtime is 1.4883992592493696 minutes
2023-05-20 22:41:49,727:INFO:SubProcess create_model() called ==================================
2023-05-20 22:41:49,728:INFO:Initializing create_model()
2023-05-20 22:41:49,728:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:41:49,728:INFO:Checking exceptions
2023-05-20 22:41:49,728:INFO:Importing libraries
2023-05-20 22:41:49,728:INFO:Copying training dataset
2023-05-20 22:41:49,750:INFO:Defining folds
2023-05-20 22:41:49,751:INFO:Declaring metric variables
2023-05-20 22:41:49,759:INFO:Importing untrained model
2023-05-20 22:41:49,763:INFO:CatBoost Regressor Imported successfully
2023-05-20 22:41:49,775:INFO:Starting cross validation
2023-05-20 22:41:49,777:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:42:22,945:INFO:Calculating mean and std
2023-05-20 22:42:22,947:INFO:Creating metrics dataframe
2023-05-20 22:42:23,358:INFO:Uploading results into container
2023-05-20 22:42:23,359:INFO:Uploading model into container now
2023-05-20 22:42:23,359:INFO:_master_model_container: 19
2023-05-20 22:42:23,359:INFO:_display_container: 2
2023-05-20 22:42:23,359:INFO:<catboost.core.CatBoostRegressor object at 0x00000258A6CAED10>
2023-05-20 22:42:23,359:INFO:create_model() successfully completed......................................
2023-05-20 22:42:23,470:INFO:SubProcess create_model() end ==================================
2023-05-20 22:42:23,470:INFO:Creating metrics dataframe
2023-05-20 22:42:23,484:INFO:Initializing Dummy Regressor
2023-05-20 22:42:23,484:INFO:Total runtime is 2.051075820128123 minutes
2023-05-20 22:42:23,487:INFO:SubProcess create_model() called ==================================
2023-05-20 22:42:23,489:INFO:Initializing create_model()
2023-05-20 22:42:23,489:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258858C6110>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:23,489:INFO:Checking exceptions
2023-05-20 22:42:23,489:INFO:Importing libraries
2023-05-20 22:42:23,489:INFO:Copying training dataset
2023-05-20 22:42:23,500:INFO:Defining folds
2023-05-20 22:42:23,500:INFO:Declaring metric variables
2023-05-20 22:42:23,505:INFO:Importing untrained model
2023-05-20 22:42:23,509:INFO:Dummy Regressor Imported successfully
2023-05-20 22:42:23,515:INFO:Starting cross validation
2023-05-20 22:42:23,517:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 22:42:27,193:INFO:Calculating mean and std
2023-05-20 22:42:27,195:INFO:Creating metrics dataframe
2023-05-20 22:42:27,601:INFO:Uploading results into container
2023-05-20 22:42:27,601:INFO:Uploading model into container now
2023-05-20 22:42:27,602:INFO:_master_model_container: 20
2023-05-20 22:42:27,602:INFO:_display_container: 2
2023-05-20 22:42:27,602:INFO:DummyRegressor()
2023-05-20 22:42:27,602:INFO:create_model() successfully completed......................................
2023-05-20 22:42:27,702:INFO:SubProcess create_model() end ==================================
2023-05-20 22:42:27,702:INFO:Creating metrics dataframe
2023-05-20 22:42:27,727:INFO:Initializing create_model()
2023-05-20 22:42:27,727:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=<catboost.core.CatBoostRegressor object at 0x00000258A6CAED10>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:27,727:INFO:Checking exceptions
2023-05-20 22:42:27,729:INFO:Importing libraries
2023-05-20 22:42:27,729:INFO:Copying training dataset
2023-05-20 22:42:27,744:INFO:Defining folds
2023-05-20 22:42:27,744:INFO:Declaring metric variables
2023-05-20 22:42:27,744:INFO:Importing untrained model
2023-05-20 22:42:27,744:INFO:Declaring custom model
2023-05-20 22:42:27,745:INFO:CatBoost Regressor Imported successfully
2023-05-20 22:42:27,745:INFO:Cross validation set to False
2023-05-20 22:42:27,745:INFO:Fitting Model
2023-05-20 22:42:31,586:INFO:<catboost.core.CatBoostRegressor object at 0x0000025886206470>
2023-05-20 22:42:31,588:INFO:create_model() successfully completed......................................
2023-05-20 22:42:31,671:INFO:Initializing create_model()
2023-05-20 22:42:31,671:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=OrthogonalMatchingPursuit(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:31,671:INFO:Checking exceptions
2023-05-20 22:42:31,673:INFO:Importing libraries
2023-05-20 22:42:31,673:INFO:Copying training dataset
2023-05-20 22:42:31,683:INFO:Defining folds
2023-05-20 22:42:31,683:INFO:Declaring metric variables
2023-05-20 22:42:31,683:INFO:Importing untrained model
2023-05-20 22:42:31,683:INFO:Declaring custom model
2023-05-20 22:42:31,684:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 22:42:31,685:INFO:Cross validation set to False
2023-05-20 22:42:31,685:INFO:Fitting Model
2023-05-20 22:42:31,729:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 22:42:32,072:INFO:OrthogonalMatchingPursuit()
2023-05-20 22:42:32,072:INFO:create_model() successfully completed......................................
2023-05-20 22:42:32,158:INFO:Initializing create_model()
2023-05-20 22:42:32,159:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=GradientBoostingRegressor(random_state=2934), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:32,159:INFO:Checking exceptions
2023-05-20 22:42:32,160:INFO:Importing libraries
2023-05-20 22:42:32,160:INFO:Copying training dataset
2023-05-20 22:42:32,169:INFO:Defining folds
2023-05-20 22:42:32,169:INFO:Declaring metric variables
2023-05-20 22:42:32,169:INFO:Importing untrained model
2023-05-20 22:42:32,169:INFO:Declaring custom model
2023-05-20 22:42:32,170:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 22:42:32,171:INFO:Cross validation set to False
2023-05-20 22:42:32,171:INFO:Fitting Model
2023-05-20 22:42:33,258:INFO:GradientBoostingRegressor(random_state=2934)
2023-05-20 22:42:33,258:INFO:create_model() successfully completed......................................
2023-05-20 22:42:33,341:INFO:Initializing create_model()
2023-05-20 22:42:33,341:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:33,341:INFO:Checking exceptions
2023-05-20 22:42:33,343:INFO:Importing libraries
2023-05-20 22:42:33,343:INFO:Copying training dataset
2023-05-20 22:42:33,350:INFO:Defining folds
2023-05-20 22:42:33,350:INFO:Declaring metric variables
2023-05-20 22:42:33,351:INFO:Importing untrained model
2023-05-20 22:42:33,351:INFO:Declaring custom model
2023-05-20 22:42:33,351:INFO:Bayesian Ridge Imported successfully
2023-05-20 22:42:33,352:INFO:Cross validation set to False
2023-05-20 22:42:33,352:INFO:Fitting Model
2023-05-20 22:42:33,820:INFO:BayesianRidge()
2023-05-20 22:42:33,820:INFO:create_model() successfully completed......................................
2023-05-20 22:42:33,900:INFO:Initializing create_model()
2023-05-20 22:42:33,900:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A3ABD9F0>, estimator=LGBMRegressor(random_state=2934), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 22:42:33,900:INFO:Checking exceptions
2023-05-20 22:42:33,902:INFO:Importing libraries
2023-05-20 22:42:33,902:INFO:Copying training dataset
2023-05-20 22:42:33,910:INFO:Defining folds
2023-05-20 22:42:33,910:INFO:Declaring metric variables
2023-05-20 22:42:33,911:INFO:Importing untrained model
2023-05-20 22:42:33,911:INFO:Declaring custom model
2023-05-20 22:42:33,911:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 22:42:33,912:INFO:Cross validation set to False
2023-05-20 22:42:33,913:INFO:Fitting Model
2023-05-20 22:42:34,552:INFO:LGBMRegressor(random_state=2934)
2023-05-20 22:42:34,552:INFO:create_model() successfully completed......................................
2023-05-20 22:42:34,652:INFO:_master_model_container: 20
2023-05-20 22:42:34,652:INFO:_display_container: 2
2023-05-20 22:42:34,653:INFO:[<catboost.core.CatBoostRegressor object at 0x0000025886206470>, OrthogonalMatchingPursuit(), GradientBoostingRegressor(random_state=2934), BayesianRidge(), LGBMRegressor(random_state=2934)]
2023-05-20 22:42:34,653:INFO:compare_models() successfully completed......................................
2023-05-20 23:00:00,644:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\1931025223.py:1: RuntimeWarning: overflow encountered in exp
  np.exp(np.sqrt(np.mean(-results)))

2023-05-20 23:00:02,347:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\1931025223.py:1: RuntimeWarning: overflow encountered in exp
  np.exp(np.sqrt(np.mean(-results)))

2023-05-20 23:28:51,240:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:4: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:28:51,337:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:9: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:28:52,503:INFO:PyCaret RegressionExperiment
2023-05-20 23:28:52,503:INFO:Logging name: reg-default-name
2023-05-20 23:28:52,503:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 23:28:52,503:INFO:version 3.0.2
2023-05-20 23:28:52,504:INFO:Initializing setup()
2023-05-20 23:28:52,504:INFO:self.USI: 8e26
2023-05-20 23:28:52,504:INFO:self._variable_keys: {'y_train', 'fold_groups_param', 'fold_generator', 'n_jobs_param', 'memory', 'transform_target_param', 'USI', 'y_test', 'X', 'data', 'seed', 'logging_param', 'gpu_n_jobs_param', '_ml_usecase', 'exp_name_log', 'html_param', 'log_plots_param', 'exp_id', 'X_train', 'X_test', 'target_param', 'fold_shuffle_param', 'y', '_available_plots', 'idx', 'gpu_param', 'pipeline'}
2023-05-20 23:28:52,504:INFO:Checking environment
2023-05-20 23:28:52,504:INFO:python_version: 3.10.3
2023-05-20 23:28:52,504:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 23:28:52,504:INFO:machine: AMD64
2023-05-20 23:28:52,504:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 23:28:52,504:INFO:Memory: svmem(total=17083187200, available=3772846080, percent=77.9, used=13310341120, free=3772846080)
2023-05-20 23:28:52,504:INFO:Physical Core: 6
2023-05-20 23:28:52,504:INFO:Logical Core: 12
2023-05-20 23:28:52,504:INFO:Checking libraries
2023-05-20 23:28:52,504:INFO:System:
2023-05-20 23:28:52,504:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 23:28:52,504:INFO:executable: c:\Python310\python.exe
2023-05-20 23:28:52,504:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 23:28:52,504:INFO:PyCaret required dependencies:
2023-05-20 23:28:52,504:INFO:                 pip: 23.1.2
2023-05-20 23:28:52,504:INFO:          setuptools: 58.1.0
2023-05-20 23:28:52,504:INFO:             pycaret: 3.0.2
2023-05-20 23:28:52,504:INFO:             IPython: 8.5.0
2023-05-20 23:28:52,505:INFO:          ipywidgets: 8.0.6
2023-05-20 23:28:52,505:INFO:                tqdm: 4.65.0
2023-05-20 23:28:52,505:INFO:               numpy: 1.23.2
2023-05-20 23:28:52,505:INFO:              pandas: 1.5.2
2023-05-20 23:28:52,505:INFO:              jinja2: 3.1.2
2023-05-20 23:28:52,505:INFO:               scipy: 1.9.3
2023-05-20 23:28:52,505:INFO:              joblib: 1.2.0
2023-05-20 23:28:52,505:INFO:             sklearn: 1.1.3
2023-05-20 23:28:52,505:INFO:                pyod: 1.0.9
2023-05-20 23:28:52,505:INFO:            imblearn: 0.10.1
2023-05-20 23:28:52,505:INFO:   category_encoders: 2.6.1
2023-05-20 23:28:52,505:INFO:            lightgbm: 3.3.5
2023-05-20 23:28:52,505:INFO:               numba: 0.57.0
2023-05-20 23:28:52,505:INFO:            requests: 2.28.2
2023-05-20 23:28:52,505:INFO:          matplotlib: 3.5.3
2023-05-20 23:28:52,505:INFO:          scikitplot: 0.3.7
2023-05-20 23:28:52,505:INFO:         yellowbrick: 1.5
2023-05-20 23:28:52,505:INFO:              plotly: 5.13.1
2023-05-20 23:28:52,505:INFO:             kaleido: 0.2.1
2023-05-20 23:28:52,505:INFO:         statsmodels: 0.13.5
2023-05-20 23:28:52,505:INFO:              sktime: 0.17.0
2023-05-20 23:28:52,505:INFO:               tbats: 1.1.3
2023-05-20 23:28:52,505:INFO:            pmdarima: 2.0.3
2023-05-20 23:28:52,505:INFO:              psutil: 5.9.2
2023-05-20 23:28:52,505:INFO:PyCaret optional dependencies:
2023-05-20 23:28:52,506:INFO:                shap: Not installed
2023-05-20 23:28:52,506:INFO:           interpret: Not installed
2023-05-20 23:28:52,506:INFO:                umap: Not installed
2023-05-20 23:28:52,506:INFO:    pandas_profiling: Not installed
2023-05-20 23:28:52,506:INFO:  explainerdashboard: Not installed
2023-05-20 23:28:52,506:INFO:             autoviz: Not installed
2023-05-20 23:28:52,506:INFO:           fairlearn: Not installed
2023-05-20 23:28:52,506:INFO:             xgboost: 1.7.4
2023-05-20 23:28:52,506:INFO:            catboost: 1.1.1
2023-05-20 23:28:52,506:INFO:              kmodes: Not installed
2023-05-20 23:28:52,506:INFO:             mlxtend: Not installed
2023-05-20 23:28:52,506:INFO:       statsforecast: Not installed
2023-05-20 23:28:52,506:INFO:        tune_sklearn: Not installed
2023-05-20 23:28:52,506:INFO:                 ray: Not installed
2023-05-20 23:28:52,506:INFO:            hyperopt: Not installed
2023-05-20 23:28:52,506:INFO:              optuna: Not installed
2023-05-20 23:28:52,506:INFO:               skopt: Not installed
2023-05-20 23:28:52,506:INFO:              mlflow: Not installed
2023-05-20 23:28:52,507:INFO:              gradio: Not installed
2023-05-20 23:28:52,507:INFO:             fastapi: Not installed
2023-05-20 23:28:52,507:INFO:             uvicorn: Not installed
2023-05-20 23:28:52,507:INFO:              m2cgen: Not installed
2023-05-20 23:28:52,507:INFO:           evidently: Not installed
2023-05-20 23:28:52,507:INFO:               fugue: Not installed
2023-05-20 23:28:52,507:INFO:           streamlit: Not installed
2023-05-20 23:28:52,507:INFO:             prophet: Not installed
2023-05-20 23:28:52,507:INFO:None
2023-05-20 23:28:52,507:INFO:Set up data.
2023-05-20 23:28:52,630:INFO:Set up train/test split.
2023-05-20 23:28:52,653:INFO:Set up index.
2023-05-20 23:28:52,654:INFO:Set up folding strategy.
2023-05-20 23:28:52,654:INFO:Assigning column types.
2023-05-20 23:28:52,670:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 23:28:52,671:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,678:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,683:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,747:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,794:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,794:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:52,797:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:52,797:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,802:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,806:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,930:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,930:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:52,933:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:52,933:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 23:28:52,937:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:28:52,942:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,011:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,060:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,060:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,062:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,066:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,071:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,127:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,169:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,170:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,172:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,173:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 23:28:53,180:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,235:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,274:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,275:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,278:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,287:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,343:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,380:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,381:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,383:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,384:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 23:28:53,447:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,486:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,487:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,489:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,559:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,603:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,603:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,606:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,607:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 23:28:53,674:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,717:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,719:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,791:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:28:53,834:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:53,837:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:53,837:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 23:28:54,155:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:54,158:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:54,272:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:54,275:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:54,277:INFO:Preparing preprocessing pipeline...
2023-05-20 23:28:54,277:INFO:Set up simple imputation.
2023-05-20 23:28:54,278:INFO:Set up column name cleaning.
2023-05-20 23:28:54,336:INFO:Finished creating preprocessing pipeline.
2023-05-20 23:28:54,343:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 23:28:54,343:INFO:Creating final display dataframe.
2023-05-20 23:28:54,554:INFO:Setup _display_container:                     Description             Value
0                    Session id              3396
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 319)
4        Transformed data shape       (1460, 319)
5   Transformed train set shape       (1021, 319)
6    Transformed test set shape        (439, 319)
7              Numeric features               318
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              8e26
2023-05-20 23:28:54,691:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:54,694:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:54,829:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:28:54,831:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:28:54,832:INFO:setup() successfully completed in 2.7s...............
2023-05-20 23:28:55,226:INFO:Initializing compare_models()
2023-05-20 23:28:55,226:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 23:28:55,226:INFO:Checking exceptions
2023-05-20 23:28:55,235:INFO:Preparing display monitor
2023-05-20 23:28:55,288:INFO:Initializing Linear Regression
2023-05-20 23:28:55,289:INFO:Total runtime is 1.6641616821289062e-05 minutes
2023-05-20 23:28:55,292:INFO:SubProcess create_model() called ==================================
2023-05-20 23:28:55,292:INFO:Initializing create_model()
2023-05-20 23:28:55,292:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:28:55,292:INFO:Checking exceptions
2023-05-20 23:28:55,292:INFO:Importing libraries
2023-05-20 23:28:55,292:INFO:Copying training dataset
2023-05-20 23:28:55,305:INFO:Defining folds
2023-05-20 23:28:55,305:INFO:Declaring metric variables
2023-05-20 23:28:55,308:INFO:Importing untrained model
2023-05-20 23:28:55,312:INFO:Linear Regression Imported successfully
2023-05-20 23:28:55,318:INFO:Starting cross validation
2023-05-20 23:28:55,320:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:04,038:INFO:Calculating mean and std
2023-05-20 23:29:04,041:INFO:Creating metrics dataframe
2023-05-20 23:29:04,427:INFO:Uploading results into container
2023-05-20 23:29:04,427:INFO:Uploading model into container now
2023-05-20 23:29:04,428:INFO:_master_model_container: 1
2023-05-20 23:29:04,428:INFO:_display_container: 2
2023-05-20 23:29:04,428:INFO:LinearRegression(n_jobs=-1)
2023-05-20 23:29:04,429:INFO:create_model() successfully completed......................................
2023-05-20 23:29:04,531:INFO:SubProcess create_model() end ==================================
2023-05-20 23:29:04,531:INFO:Creating metrics dataframe
2023-05-20 23:29:04,539:INFO:Initializing Lasso Regression
2023-05-20 23:29:04,539:INFO:Total runtime is 0.15418185393015546 minutes
2023-05-20 23:29:04,543:INFO:SubProcess create_model() called ==================================
2023-05-20 23:29:04,543:INFO:Initializing create_model()
2023-05-20 23:29:04,544:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:04,544:INFO:Checking exceptions
2023-05-20 23:29:04,544:INFO:Importing libraries
2023-05-20 23:29:04,544:INFO:Copying training dataset
2023-05-20 23:29:04,554:INFO:Defining folds
2023-05-20 23:29:04,554:INFO:Declaring metric variables
2023-05-20 23:29:04,558:INFO:Importing untrained model
2023-05-20 23:29:04,561:INFO:Lasso Regression Imported successfully
2023-05-20 23:29:04,567:INFO:Starting cross validation
2023-05-20 23:29:04,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:08,658:INFO:Calculating mean and std
2023-05-20 23:29:08,660:INFO:Creating metrics dataframe
2023-05-20 23:29:09,079:INFO:Uploading results into container
2023-05-20 23:29:09,080:INFO:Uploading model into container now
2023-05-20 23:29:09,080:INFO:_master_model_container: 2
2023-05-20 23:29:09,080:INFO:_display_container: 2
2023-05-20 23:29:09,081:INFO:Lasso(random_state=3396)
2023-05-20 23:29:09,081:INFO:create_model() successfully completed......................................
2023-05-20 23:29:09,184:INFO:SubProcess create_model() end ==================================
2023-05-20 23:29:09,184:INFO:Creating metrics dataframe
2023-05-20 23:29:09,194:INFO:Initializing Ridge Regression
2023-05-20 23:29:09,194:INFO:Total runtime is 0.2317650596300761 minutes
2023-05-20 23:29:09,196:INFO:SubProcess create_model() called ==================================
2023-05-20 23:29:09,197:INFO:Initializing create_model()
2023-05-20 23:29:09,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:09,197:INFO:Checking exceptions
2023-05-20 23:29:09,197:INFO:Importing libraries
2023-05-20 23:29:09,197:INFO:Copying training dataset
2023-05-20 23:29:09,210:INFO:Defining folds
2023-05-20 23:29:09,210:INFO:Declaring metric variables
2023-05-20 23:29:09,212:INFO:Importing untrained model
2023-05-20 23:29:09,216:INFO:Ridge Regression Imported successfully
2023-05-20 23:29:09,223:INFO:Starting cross validation
2023-05-20 23:29:09,225:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:13,003:INFO:Calculating mean and std
2023-05-20 23:29:13,005:INFO:Creating metrics dataframe
2023-05-20 23:29:13,402:INFO:Uploading results into container
2023-05-20 23:29:13,402:INFO:Uploading model into container now
2023-05-20 23:29:13,403:INFO:_master_model_container: 3
2023-05-20 23:29:13,403:INFO:_display_container: 2
2023-05-20 23:29:13,403:INFO:Ridge(random_state=3396)
2023-05-20 23:29:13,403:INFO:create_model() successfully completed......................................
2023-05-20 23:29:13,518:INFO:SubProcess create_model() end ==================================
2023-05-20 23:29:13,518:INFO:Creating metrics dataframe
2023-05-20 23:29:13,527:INFO:Initializing Elastic Net
2023-05-20 23:29:13,527:INFO:Total runtime is 0.3039913853009542 minutes
2023-05-20 23:29:13,530:INFO:SubProcess create_model() called ==================================
2023-05-20 23:29:13,530:INFO:Initializing create_model()
2023-05-20 23:29:13,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:13,531:INFO:Checking exceptions
2023-05-20 23:29:13,531:INFO:Importing libraries
2023-05-20 23:29:13,531:INFO:Copying training dataset
2023-05-20 23:29:13,544:INFO:Defining folds
2023-05-20 23:29:13,544:INFO:Declaring metric variables
2023-05-20 23:29:13,548:INFO:Importing untrained model
2023-05-20 23:29:13,552:INFO:Elastic Net Imported successfully
2023-05-20 23:29:13,560:INFO:Starting cross validation
2023-05-20 23:29:13,561:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:17,248:INFO:Calculating mean and std
2023-05-20 23:29:17,251:INFO:Creating metrics dataframe
2023-05-20 23:29:17,638:INFO:Uploading results into container
2023-05-20 23:29:17,638:INFO:Uploading model into container now
2023-05-20 23:29:17,640:INFO:_master_model_container: 4
2023-05-20 23:29:17,640:INFO:_display_container: 2
2023-05-20 23:29:17,641:INFO:ElasticNet(random_state=3396)
2023-05-20 23:29:17,641:INFO:create_model() successfully completed......................................
2023-05-20 23:29:17,747:INFO:SubProcess create_model() end ==================================
2023-05-20 23:29:17,747:INFO:Creating metrics dataframe
2023-05-20 23:29:17,758:INFO:Initializing Least Angle Regression
2023-05-20 23:29:17,758:INFO:Total runtime is 0.3744988242785136 minutes
2023-05-20 23:29:17,761:INFO:SubProcess create_model() called ==================================
2023-05-20 23:29:17,761:INFO:Initializing create_model()
2023-05-20 23:29:17,761:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:17,761:INFO:Checking exceptions
2023-05-20 23:29:17,761:INFO:Importing libraries
2023-05-20 23:29:17,762:INFO:Copying training dataset
2023-05-20 23:29:17,774:INFO:Defining folds
2023-05-20 23:29:17,774:INFO:Declaring metric variables
2023-05-20 23:29:17,778:INFO:Importing untrained model
2023-05-20 23:29:17,781:INFO:Least Angle Regression Imported successfully
2023-05-20 23:29:17,788:INFO:Starting cross validation
2023-05-20 23:29:17,790:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:17,906:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,917:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,943:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=7.810e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,943:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,952:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.395e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,964:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=2.089e-04, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,968:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.576e-04, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,968:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,975:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.510e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,977:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.536e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,979:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=8.266e-04, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(


2023-05-20 23:29:17,980:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.571e-04, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,984:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:17,988:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=2.405e-04, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,988:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=4.361e-04, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,990:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.124e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,991:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=9.863e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,993:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=5.085e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,995:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=5.075e-04, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:17,995:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:18,000:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.578e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,002:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=8.882e-04, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,004:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:18,005:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.160e-03, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,010:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.022e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,016:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.576e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,029:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.395e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,034:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:18,036:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:29:18,043:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 189 iterations, i.e. alpha=8.648e-04, with an active set of 161 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,045:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.965e+00, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,055:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.629e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,055:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=2.075e+00, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,063:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=1.671e-03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,068:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 230 iterations, i.e. alpha=1.010e-03, with an active set of 198 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,071:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 196 iterations, i.e. alpha=2.203e+00, with an active set of 164 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,078:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=1.856e-03, with an active set of 163 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,079:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.445e-03, with an active set of 89 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,085:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 23:29:18,085:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=1.451e-03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,086:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 23:29:18,086:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:29:18,088:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.446e-03, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,088:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

s, i.e. alpha=1.869e-03, with an active set of 181 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,090:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 206 iterations, i.e. alpha=1.854e-03, with an active set of 185 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,094:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=1.431e-03, with an active set of 132 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,095:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:29:18,102:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 223 iterations, i.e. alpha=1.836e-03, with an active set of 200 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,104:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 268 iterations, i.e. alpha=1.303e-02, with an active set of 228 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,112:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 168 iterations, i.e. alpha=1.916e-03, with an active set of 145 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,113:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 171 iterations, i.e. alpha=1.912e-03, with an active set of 148 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:29:18,122:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: invalid value encountered in add
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:29:18,134:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-20 23:29:18,135:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-20 23:29:18,135:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-20 23:29:18,136:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-20 23:29:18,137:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 23:29:18,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:29:18,309:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:29:18,489:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,490:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,490:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,520:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:18,521:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,521:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,521:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,525:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:18,526:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,526:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,527:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,537:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:18,540:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:18,541:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,542:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,542:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,551:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,552:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,553:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,556:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,557:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:18,557:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:18,578:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:18,582:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:21,807:INFO:Calculating mean and std
2023-05-20 23:29:21,808:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-20 23:29:21,810:INFO:Creating metrics dataframe
2023-05-20 23:29:22,193:INFO:Uploading results into container
2023-05-20 23:29:22,194:INFO:Uploading model into container now
2023-05-20 23:29:22,195:INFO:_master_model_container: 5
2023-05-20 23:29:22,195:INFO:_display_container: 2
2023-05-20 23:29:22,195:INFO:Lars(random_state=3396)
2023-05-20 23:29:22,196:INFO:create_model() successfully completed......................................
2023-05-20 23:29:22,290:WARNING:create_model() for Lars(random_state=3396) raised an exception or returned all 0.0, trying without fit_kwargs:
2023-05-20 23:29:22,298:WARNING:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

2023-05-20 23:29:22,298:INFO:Initializing create_model()
2023-05-20 23:29:22,298:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:22,298:INFO:Checking exceptions
2023-05-20 23:29:22,298:INFO:Importing libraries
2023-05-20 23:29:22,298:INFO:Copying training dataset
2023-05-20 23:29:22,310:INFO:Defining folds
2023-05-20 23:29:22,310:INFO:Declaring metric variables
2023-05-20 23:29:22,313:INFO:Importing untrained model
2023-05-20 23:29:22,318:INFO:Least Angle Regression Imported successfully
2023-05-20 23:29:22,324:INFO:Starting cross validation
2023-05-20 23:29:22,326:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:22,545:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:22,549:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:22,564:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,564:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,565:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:22,570:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:22,571:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,571:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,572:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:22,573:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:22,588:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,588:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,588:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:22,610:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,610:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,610:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:22,616:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:29:22,618:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,618:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,618:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:22,620:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:29:22,637:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,637:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:29:22,637:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:29:25,903:INFO:Calculating mean and std
2023-05-20 23:29:25,903:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-20 23:29:25,905:INFO:Creating metrics dataframe
2023-05-20 23:29:26,297:INFO:Uploading results into container
2023-05-20 23:29:26,298:INFO:Uploading model into container now
2023-05-20 23:29:26,299:INFO:_master_model_container: 6
2023-05-20 23:29:26,300:INFO:_display_container: 2
2023-05-20 23:29:26,300:INFO:Lars(random_state=3396)
2023-05-20 23:29:26,300:INFO:create_model() successfully completed......................................
2023-05-20 23:29:26,401:ERROR:create_model() for Lars(random_state=3396) raised an exception or returned all 0.0:
2023-05-20 23:29:26,401:ERROR:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 810, in compare_models
    assert (
AssertionError

2023-05-20 23:29:26,401:INFO:Initializing Lasso Least Angle Regression
2023-05-20 23:29:26,401:INFO:Total runtime is 0.5185571551322937 minutes
2023-05-20 23:29:26,404:INFO:SubProcess create_model() called ==================================
2023-05-20 23:29:26,404:INFO:Initializing create_model()
2023-05-20 23:29:26,405:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BD50A530>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:29:26,405:INFO:Checking exceptions
2023-05-20 23:29:26,405:INFO:Importing libraries
2023-05-20 23:29:26,405:INFO:Copying training dataset
2023-05-20 23:29:26,416:INFO:Defining folds
2023-05-20 23:29:26,416:INFO:Declaring metric variables
2023-05-20 23:29:26,419:INFO:Importing untrained model
2023-05-20 23:29:26,422:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 23:29:26,429:INFO:Starting cross validation
2023-05-20 23:29:26,431:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:29:26,544:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,560:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,577:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,588:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,592:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,608:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,629:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,637:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,651:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:29:26,661:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:02,652:INFO:Initializing compare_models()
2023-05-20 23:31:02,653:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 23:31:02,653:INFO:Checking exceptions
2023-05-20 23:31:02,658:INFO:Preparing display monitor
2023-05-20 23:31:02,710:INFO:Initializing Linear Regression
2023-05-20 23:31:02,711:INFO:Total runtime is 1.6657511393229165e-05 minutes
2023-05-20 23:31:02,716:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:02,717:INFO:Initializing create_model()
2023-05-20 23:31:02,717:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:02,717:INFO:Checking exceptions
2023-05-20 23:31:02,717:INFO:Importing libraries
2023-05-20 23:31:02,717:INFO:Copying training dataset
2023-05-20 23:31:02,733:INFO:Defining folds
2023-05-20 23:31:02,733:INFO:Declaring metric variables
2023-05-20 23:31:02,737:INFO:Importing untrained model
2023-05-20 23:31:02,743:INFO:Linear Regression Imported successfully
2023-05-20 23:31:02,750:INFO:Starting cross validation
2023-05-20 23:31:02,753:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:11,338:INFO:Calculating mean and std
2023-05-20 23:31:11,341:INFO:Creating metrics dataframe
2023-05-20 23:31:11,731:INFO:Uploading results into container
2023-05-20 23:31:11,732:INFO:Uploading model into container now
2023-05-20 23:31:11,732:INFO:_master_model_container: 7
2023-05-20 23:31:11,732:INFO:_display_container: 2
2023-05-20 23:31:11,732:INFO:LinearRegression(n_jobs=-1)
2023-05-20 23:31:11,732:INFO:create_model() successfully completed......................................
2023-05-20 23:31:11,881:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:11,882:INFO:Creating metrics dataframe
2023-05-20 23:31:11,890:INFO:Initializing Lasso Regression
2023-05-20 23:31:11,891:INFO:Total runtime is 0.1530171831448873 minutes
2023-05-20 23:31:11,893:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:11,894:INFO:Initializing create_model()
2023-05-20 23:31:11,894:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:11,894:INFO:Checking exceptions
2023-05-20 23:31:11,894:INFO:Importing libraries
2023-05-20 23:31:11,894:INFO:Copying training dataset
2023-05-20 23:31:11,904:INFO:Defining folds
2023-05-20 23:31:11,904:INFO:Declaring metric variables
2023-05-20 23:31:11,908:INFO:Importing untrained model
2023-05-20 23:31:11,911:INFO:Lasso Regression Imported successfully
2023-05-20 23:31:11,917:INFO:Starting cross validation
2023-05-20 23:31:11,918:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:15,822:INFO:Calculating mean and std
2023-05-20 23:31:15,824:INFO:Creating metrics dataframe
2023-05-20 23:31:16,201:INFO:Uploading results into container
2023-05-20 23:31:16,202:INFO:Uploading model into container now
2023-05-20 23:31:16,203:INFO:_master_model_container: 8
2023-05-20 23:31:16,203:INFO:_display_container: 2
2023-05-20 23:31:16,203:INFO:Lasso(random_state=3396)
2023-05-20 23:31:16,203:INFO:create_model() successfully completed......................................
2023-05-20 23:31:16,342:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:16,342:INFO:Creating metrics dataframe
2023-05-20 23:31:16,352:INFO:Initializing Ridge Regression
2023-05-20 23:31:16,352:INFO:Total runtime is 0.22736637194951376 minutes
2023-05-20 23:31:16,356:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:16,356:INFO:Initializing create_model()
2023-05-20 23:31:16,356:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:16,356:INFO:Checking exceptions
2023-05-20 23:31:16,356:INFO:Importing libraries
2023-05-20 23:31:16,356:INFO:Copying training dataset
2023-05-20 23:31:16,366:INFO:Defining folds
2023-05-20 23:31:16,366:INFO:Declaring metric variables
2023-05-20 23:31:16,369:INFO:Importing untrained model
2023-05-20 23:31:16,375:INFO:Ridge Regression Imported successfully
2023-05-20 23:31:16,380:INFO:Starting cross validation
2023-05-20 23:31:16,382:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:19,935:INFO:Calculating mean and std
2023-05-20 23:31:19,939:INFO:Creating metrics dataframe
2023-05-20 23:31:20,325:INFO:Uploading results into container
2023-05-20 23:31:20,326:INFO:Uploading model into container now
2023-05-20 23:31:20,326:INFO:_master_model_container: 9
2023-05-20 23:31:20,327:INFO:_display_container: 2
2023-05-20 23:31:20,327:INFO:Ridge(random_state=3396)
2023-05-20 23:31:20,327:INFO:create_model() successfully completed......................................
2023-05-20 23:31:20,484:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:20,484:INFO:Creating metrics dataframe
2023-05-20 23:31:20,493:INFO:Initializing Elastic Net
2023-05-20 23:31:20,493:INFO:Total runtime is 0.2963931759198507 minutes
2023-05-20 23:31:20,497:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:20,498:INFO:Initializing create_model()
2023-05-20 23:31:20,498:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:20,498:INFO:Checking exceptions
2023-05-20 23:31:20,498:INFO:Importing libraries
2023-05-20 23:31:20,498:INFO:Copying training dataset
2023-05-20 23:31:20,510:INFO:Defining folds
2023-05-20 23:31:20,510:INFO:Declaring metric variables
2023-05-20 23:31:20,514:INFO:Importing untrained model
2023-05-20 23:31:20,518:INFO:Elastic Net Imported successfully
2023-05-20 23:31:20,526:INFO:Starting cross validation
2023-05-20 23:31:20,527:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:24,127:INFO:Calculating mean and std
2023-05-20 23:31:24,129:INFO:Creating metrics dataframe
2023-05-20 23:31:24,508:INFO:Uploading results into container
2023-05-20 23:31:24,510:INFO:Uploading model into container now
2023-05-20 23:31:24,510:INFO:_master_model_container: 10
2023-05-20 23:31:24,510:INFO:_display_container: 2
2023-05-20 23:31:24,510:INFO:ElasticNet(random_state=3396)
2023-05-20 23:31:24,510:INFO:create_model() successfully completed......................................
2023-05-20 23:31:24,644:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:24,644:INFO:Creating metrics dataframe
2023-05-20 23:31:24,654:INFO:Initializing Least Angle Regression
2023-05-20 23:31:24,654:INFO:Total runtime is 0.36574271122614543 minutes
2023-05-20 23:31:24,658:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:24,658:INFO:Initializing create_model()
2023-05-20 23:31:24,658:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:24,660:INFO:Checking exceptions
2023-05-20 23:31:24,660:INFO:Importing libraries
2023-05-20 23:31:24,660:INFO:Copying training dataset
2023-05-20 23:31:24,672:INFO:Defining folds
2023-05-20 23:31:24,672:INFO:Declaring metric variables
2023-05-20 23:31:24,675:INFO:Importing untrained model
2023-05-20 23:31:24,678:INFO:Least Angle Regression Imported successfully
2023-05-20 23:31:24,684:INFO:Starting cross validation
2023-05-20 23:31:24,686:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:24,901:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:24,905:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:24,912:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,912:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,913:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:24,927:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:24,932:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:24,935:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,937:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,937:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:24,947:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,949:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,949:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:24,967:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,968:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,968:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:24,977:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:24,978:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,979:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,979:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:24,980:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:24,995:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,996:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:24,996:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:28,294:INFO:Calculating mean and std
2023-05-20 23:31:28,295:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-20 23:31:28,296:INFO:Creating metrics dataframe
2023-05-20 23:31:28,671:INFO:Uploading results into container
2023-05-20 23:31:28,673:INFO:Uploading model into container now
2023-05-20 23:31:28,673:INFO:_master_model_container: 11
2023-05-20 23:31:28,673:INFO:_display_container: 2
2023-05-20 23:31:28,673:INFO:Lars(random_state=3396)
2023-05-20 23:31:28,673:INFO:create_model() successfully completed......................................
2023-05-20 23:31:28,805:WARNING:create_model() for Lars(random_state=3396) raised an exception or returned all 0.0, trying without fit_kwargs:
2023-05-20 23:31:28,806:WARNING:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

2023-05-20 23:31:28,806:INFO:Initializing create_model()
2023-05-20 23:31:28,806:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:28,806:INFO:Checking exceptions
2023-05-20 23:31:28,806:INFO:Importing libraries
2023-05-20 23:31:28,806:INFO:Copying training dataset
2023-05-20 23:31:28,816:INFO:Defining folds
2023-05-20 23:31:28,817:INFO:Declaring metric variables
2023-05-20 23:31:28,820:INFO:Importing untrained model
2023-05-20 23:31:28,825:INFO:Least Angle Regression Imported successfully
2023-05-20 23:31:28,830:INFO:Starting cross validation
2023-05-20 23:31:28,832:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:29,052:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:29,055:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,055:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,055:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:29,056:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:29,056:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:29,056:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:29,067:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,068:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,068:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:29,075:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,076:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,076:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:29,096:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,097:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,097:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:29,102:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:31:29,106:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:31:29,106:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,106:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,107:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:29,127:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,128:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:31:29,128:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:31:32,332:INFO:Calculating mean and std
2023-05-20 23:31:32,332:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-20 23:31:32,334:INFO:Creating metrics dataframe
2023-05-20 23:31:32,729:INFO:Uploading results into container
2023-05-20 23:31:32,730:INFO:Uploading model into container now
2023-05-20 23:31:32,730:INFO:_master_model_container: 12
2023-05-20 23:31:32,731:INFO:_display_container: 2
2023-05-20 23:31:32,731:INFO:Lars(random_state=3396)
2023-05-20 23:31:32,731:INFO:create_model() successfully completed......................................
2023-05-20 23:31:32,867:ERROR:create_model() for Lars(random_state=3396) raised an exception or returned all 0.0:
2023-05-20 23:31:32,867:ERROR:Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 794, in compare_models
    assert (
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 810, in compare_models
    assert (
AssertionError

2023-05-20 23:31:32,867:INFO:Initializing Lasso Least Angle Regression
2023-05-20 23:31:32,867:INFO:Total runtime is 0.5026258111000061 minutes
2023-05-20 23:31:32,871:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:32,871:INFO:Initializing create_model()
2023-05-20 23:31:32,871:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:32,871:INFO:Checking exceptions
2023-05-20 23:31:32,871:INFO:Importing libraries
2023-05-20 23:31:32,871:INFO:Copying training dataset
2023-05-20 23:31:32,881:INFO:Defining folds
2023-05-20 23:31:32,881:INFO:Declaring metric variables
2023-05-20 23:31:32,885:INFO:Importing untrained model
2023-05-20 23:31:32,888:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 23:31:32,894:INFO:Starting cross validation
2023-05-20 23:31:32,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:33,023:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,035:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,046:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,065:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,066:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,072:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,096:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,105:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,113:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:33,120:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:31:36,504:INFO:Calculating mean and std
2023-05-20 23:31:36,506:INFO:Creating metrics dataframe
2023-05-20 23:31:36,889:INFO:Uploading results into container
2023-05-20 23:31:36,890:INFO:Uploading model into container now
2023-05-20 23:31:36,890:INFO:_master_model_container: 13
2023-05-20 23:31:36,891:INFO:_display_container: 2
2023-05-20 23:31:36,891:INFO:LassoLars(random_state=3396)
2023-05-20 23:31:36,891:INFO:create_model() successfully completed......................................
2023-05-20 23:31:37,031:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:37,031:INFO:Creating metrics dataframe
2023-05-20 23:31:37,040:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 23:31:37,040:INFO:Total runtime is 0.5721734245618185 minutes
2023-05-20 23:31:37,043:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:37,043:INFO:Initializing create_model()
2023-05-20 23:31:37,043:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:37,043:INFO:Checking exceptions
2023-05-20 23:31:37,043:INFO:Importing libraries
2023-05-20 23:31:37,043:INFO:Copying training dataset
2023-05-20 23:31:37,054:INFO:Defining folds
2023-05-20 23:31:37,055:INFO:Declaring metric variables
2023-05-20 23:31:37,057:INFO:Importing untrained model
2023-05-20 23:31:37,061:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 23:31:37,067:INFO:Starting cross validation
2023-05-20 23:31:37,069:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:37,177:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,187:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,208:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,208:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,231:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,243:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,251:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,266:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:37,275:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:31:40,605:INFO:Calculating mean and std
2023-05-20 23:31:40,607:INFO:Creating metrics dataframe
2023-05-20 23:31:40,986:INFO:Uploading results into container
2023-05-20 23:31:40,987:INFO:Uploading model into container now
2023-05-20 23:31:40,988:INFO:_master_model_container: 14
2023-05-20 23:31:40,988:INFO:_display_container: 2
2023-05-20 23:31:40,989:INFO:OrthogonalMatchingPursuit()
2023-05-20 23:31:40,989:INFO:create_model() successfully completed......................................
2023-05-20 23:31:41,119:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:41,119:INFO:Creating metrics dataframe
2023-05-20 23:31:41,128:INFO:Initializing Bayesian Ridge
2023-05-20 23:31:41,129:INFO:Total runtime is 0.6403157234191895 minutes
2023-05-20 23:31:41,132:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:41,132:INFO:Initializing create_model()
2023-05-20 23:31:41,132:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:41,132:INFO:Checking exceptions
2023-05-20 23:31:41,132:INFO:Importing libraries
2023-05-20 23:31:41,132:INFO:Copying training dataset
2023-05-20 23:31:41,145:INFO:Defining folds
2023-05-20 23:31:41,145:INFO:Declaring metric variables
2023-05-20 23:31:41,150:INFO:Importing untrained model
2023-05-20 23:31:41,155:INFO:Bayesian Ridge Imported successfully
2023-05-20 23:31:41,161:INFO:Starting cross validation
2023-05-20 23:31:41,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:44,803:INFO:Calculating mean and std
2023-05-20 23:31:44,805:INFO:Creating metrics dataframe
2023-05-20 23:31:45,182:INFO:Uploading results into container
2023-05-20 23:31:45,183:INFO:Uploading model into container now
2023-05-20 23:31:45,183:INFO:_master_model_container: 15
2023-05-20 23:31:45,184:INFO:_display_container: 2
2023-05-20 23:31:45,184:INFO:BayesianRidge()
2023-05-20 23:31:45,184:INFO:create_model() successfully completed......................................
2023-05-20 23:31:45,310:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:45,310:INFO:Creating metrics dataframe
2023-05-20 23:31:45,319:INFO:Initializing Passive Aggressive Regressor
2023-05-20 23:31:45,319:INFO:Total runtime is 0.7101500590642293 minutes
2023-05-20 23:31:45,324:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:45,324:INFO:Initializing create_model()
2023-05-20 23:31:45,324:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:45,324:INFO:Checking exceptions
2023-05-20 23:31:45,324:INFO:Importing libraries
2023-05-20 23:31:45,324:INFO:Copying training dataset
2023-05-20 23:31:45,334:INFO:Defining folds
2023-05-20 23:31:45,335:INFO:Declaring metric variables
2023-05-20 23:31:45,339:INFO:Importing untrained model
2023-05-20 23:31:45,345:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 23:31:45,353:INFO:Starting cross validation
2023-05-20 23:31:45,355:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:48,991:INFO:Calculating mean and std
2023-05-20 23:31:48,993:INFO:Creating metrics dataframe
2023-05-20 23:31:49,369:INFO:Uploading results into container
2023-05-20 23:31:49,370:INFO:Uploading model into container now
2023-05-20 23:31:49,371:INFO:_master_model_container: 16
2023-05-20 23:31:49,371:INFO:_display_container: 2
2023-05-20 23:31:49,371:INFO:PassiveAggressiveRegressor(random_state=3396)
2023-05-20 23:31:49,371:INFO:create_model() successfully completed......................................
2023-05-20 23:31:49,504:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:49,505:INFO:Creating metrics dataframe
2023-05-20 23:31:49,514:INFO:Initializing Huber Regressor
2023-05-20 23:31:49,514:INFO:Total runtime is 0.7800644993782043 minutes
2023-05-20 23:31:49,518:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:49,518:INFO:Initializing create_model()
2023-05-20 23:31:49,518:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:49,518:INFO:Checking exceptions
2023-05-20 23:31:49,518:INFO:Importing libraries
2023-05-20 23:31:49,518:INFO:Copying training dataset
2023-05-20 23:31:49,528:INFO:Defining folds
2023-05-20 23:31:49,529:INFO:Declaring metric variables
2023-05-20 23:31:49,531:INFO:Importing untrained model
2023-05-20 23:31:49,534:INFO:Huber Regressor Imported successfully
2023-05-20 23:31:49,540:INFO:Starting cross validation
2023-05-20 23:31:49,542:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:31:52,171:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,539:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,644:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,646:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,682:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,690:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,712:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,731:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,741:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:52,750:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:31:55,930:INFO:Calculating mean and std
2023-05-20 23:31:55,932:INFO:Creating metrics dataframe
2023-05-20 23:31:56,315:INFO:Uploading results into container
2023-05-20 23:31:56,316:INFO:Uploading model into container now
2023-05-20 23:31:56,317:INFO:_master_model_container: 17
2023-05-20 23:31:56,317:INFO:_display_container: 2
2023-05-20 23:31:56,317:INFO:HuberRegressor()
2023-05-20 23:31:56,317:INFO:create_model() successfully completed......................................
2023-05-20 23:31:56,456:INFO:SubProcess create_model() end ==================================
2023-05-20 23:31:56,456:INFO:Creating metrics dataframe
2023-05-20 23:31:56,466:INFO:Initializing K Neighbors Regressor
2023-05-20 23:31:56,467:INFO:Total runtime is 0.8959478457768758 minutes
2023-05-20 23:31:56,469:INFO:SubProcess create_model() called ==================================
2023-05-20 23:31:56,470:INFO:Initializing create_model()
2023-05-20 23:31:56,470:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:31:56,470:INFO:Checking exceptions
2023-05-20 23:31:56,470:INFO:Importing libraries
2023-05-20 23:31:56,470:INFO:Copying training dataset
2023-05-20 23:31:56,484:INFO:Defining folds
2023-05-20 23:31:56,484:INFO:Declaring metric variables
2023-05-20 23:31:56,488:INFO:Importing untrained model
2023-05-20 23:31:56,495:INFO:K Neighbors Regressor Imported successfully
2023-05-20 23:31:56,505:INFO:Starting cross validation
2023-05-20 23:31:56,507:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:00,309:INFO:Calculating mean and std
2023-05-20 23:32:00,311:INFO:Creating metrics dataframe
2023-05-20 23:32:00,715:INFO:Uploading results into container
2023-05-20 23:32:00,716:INFO:Uploading model into container now
2023-05-20 23:32:00,716:INFO:_master_model_container: 18
2023-05-20 23:32:00,716:INFO:_display_container: 2
2023-05-20 23:32:00,717:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 23:32:00,717:INFO:create_model() successfully completed......................................
2023-05-20 23:32:00,865:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:00,865:INFO:Creating metrics dataframe
2023-05-20 23:32:00,877:INFO:Initializing Decision Tree Regressor
2023-05-20 23:32:00,878:INFO:Total runtime is 0.9694671670595805 minutes
2023-05-20 23:32:00,883:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:00,883:INFO:Initializing create_model()
2023-05-20 23:32:00,883:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:00,883:INFO:Checking exceptions
2023-05-20 23:32:00,883:INFO:Importing libraries
2023-05-20 23:32:00,883:INFO:Copying training dataset
2023-05-20 23:32:00,896:INFO:Defining folds
2023-05-20 23:32:00,897:INFO:Declaring metric variables
2023-05-20 23:32:00,902:INFO:Importing untrained model
2023-05-20 23:32:00,907:INFO:Decision Tree Regressor Imported successfully
2023-05-20 23:32:00,914:INFO:Starting cross validation
2023-05-20 23:32:00,916:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:04,713:INFO:Calculating mean and std
2023-05-20 23:32:04,715:INFO:Creating metrics dataframe
2023-05-20 23:32:05,116:INFO:Uploading results into container
2023-05-20 23:32:05,116:INFO:Uploading model into container now
2023-05-20 23:32:05,117:INFO:_master_model_container: 19
2023-05-20 23:32:05,117:INFO:_display_container: 2
2023-05-20 23:32:05,117:INFO:DecisionTreeRegressor(random_state=3396)
2023-05-20 23:32:05,117:INFO:create_model() successfully completed......................................
2023-05-20 23:32:05,256:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:05,256:INFO:Creating metrics dataframe
2023-05-20 23:32:05,266:INFO:Initializing Random Forest Regressor
2023-05-20 23:32:05,266:INFO:Total runtime is 1.0426093975702921 minutes
2023-05-20 23:32:05,270:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:05,270:INFO:Initializing create_model()
2023-05-20 23:32:05,270:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:05,270:INFO:Checking exceptions
2023-05-20 23:32:05,270:INFO:Importing libraries
2023-05-20 23:32:05,270:INFO:Copying training dataset
2023-05-20 23:32:05,280:INFO:Defining folds
2023-05-20 23:32:05,280:INFO:Declaring metric variables
2023-05-20 23:32:05,285:INFO:Importing untrained model
2023-05-20 23:32:05,288:INFO:Random Forest Regressor Imported successfully
2023-05-20 23:32:05,296:INFO:Starting cross validation
2023-05-20 23:32:05,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:07,640:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.10s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:07,819:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:07,893:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:08,066:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:08,102:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:08,168:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:08,244:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:12,056:INFO:Calculating mean and std
2023-05-20 23:32:12,058:INFO:Creating metrics dataframe
2023-05-20 23:32:12,478:INFO:Uploading results into container
2023-05-20 23:32:12,479:INFO:Uploading model into container now
2023-05-20 23:32:12,481:INFO:_master_model_container: 20
2023-05-20 23:32:12,481:INFO:_display_container: 2
2023-05-20 23:32:12,481:INFO:RandomForestRegressor(n_jobs=-1, random_state=3396)
2023-05-20 23:32:12,481:INFO:create_model() successfully completed......................................
2023-05-20 23:32:12,637:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:12,637:INFO:Creating metrics dataframe
2023-05-20 23:32:12,649:INFO:Initializing Extra Trees Regressor
2023-05-20 23:32:12,649:INFO:Total runtime is 1.1656507015228272 minutes
2023-05-20 23:32:12,653:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:12,653:INFO:Initializing create_model()
2023-05-20 23:32:12,653:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:12,654:INFO:Checking exceptions
2023-05-20 23:32:12,654:INFO:Importing libraries
2023-05-20 23:32:12,654:INFO:Copying training dataset
2023-05-20 23:32:12,666:INFO:Defining folds
2023-05-20 23:32:12,666:INFO:Declaring metric variables
2023-05-20 23:32:12,669:INFO:Importing untrained model
2023-05-20 23:32:12,674:INFO:Extra Trees Regressor Imported successfully
2023-05-20 23:32:12,681:INFO:Starting cross validation
2023-05-20 23:32:12,683:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:14,746:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:15,428:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:32:19,620:INFO:Calculating mean and std
2023-05-20 23:32:19,623:INFO:Creating metrics dataframe
2023-05-20 23:32:20,033:INFO:Uploading results into container
2023-05-20 23:32:20,034:INFO:Uploading model into container now
2023-05-20 23:32:20,035:INFO:_master_model_container: 21
2023-05-20 23:32:20,036:INFO:_display_container: 2
2023-05-20 23:32:20,036:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3396)
2023-05-20 23:32:20,036:INFO:create_model() successfully completed......................................
2023-05-20 23:32:20,181:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:20,181:INFO:Creating metrics dataframe
2023-05-20 23:32:20,192:INFO:Initializing AdaBoost Regressor
2023-05-20 23:32:20,192:INFO:Total runtime is 1.291370224952698 minutes
2023-05-20 23:32:20,195:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:20,195:INFO:Initializing create_model()
2023-05-20 23:32:20,195:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:20,195:INFO:Checking exceptions
2023-05-20 23:32:20,195:INFO:Importing libraries
2023-05-20 23:32:20,196:INFO:Copying training dataset
2023-05-20 23:32:20,205:INFO:Defining folds
2023-05-20 23:32:20,205:INFO:Declaring metric variables
2023-05-20 23:32:20,209:INFO:Importing untrained model
2023-05-20 23:32:20,212:INFO:AdaBoost Regressor Imported successfully
2023-05-20 23:32:20,217:INFO:Starting cross validation
2023-05-20 23:32:20,219:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:24,981:INFO:Calculating mean and std
2023-05-20 23:32:24,983:INFO:Creating metrics dataframe
2023-05-20 23:32:25,391:INFO:Uploading results into container
2023-05-20 23:32:25,392:INFO:Uploading model into container now
2023-05-20 23:32:25,393:INFO:_master_model_container: 22
2023-05-20 23:32:25,393:INFO:_display_container: 2
2023-05-20 23:32:25,393:INFO:AdaBoostRegressor(random_state=3396)
2023-05-20 23:32:25,393:INFO:create_model() successfully completed......................................
2023-05-20 23:32:25,528:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:25,528:INFO:Creating metrics dataframe
2023-05-20 23:32:25,539:INFO:Initializing Gradient Boosting Regressor
2023-05-20 23:32:25,539:INFO:Total runtime is 1.3804883122444154 minutes
2023-05-20 23:32:25,542:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:25,543:INFO:Initializing create_model()
2023-05-20 23:32:25,543:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:25,543:INFO:Checking exceptions
2023-05-20 23:32:25,543:INFO:Importing libraries
2023-05-20 23:32:25,543:INFO:Copying training dataset
2023-05-20 23:32:25,553:INFO:Defining folds
2023-05-20 23:32:25,553:INFO:Declaring metric variables
2023-05-20 23:32:25,557:INFO:Importing untrained model
2023-05-20 23:32:25,560:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 23:32:25,565:INFO:Starting cross validation
2023-05-20 23:32:25,567:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:31,022:INFO:Calculating mean and std
2023-05-20 23:32:31,023:INFO:Creating metrics dataframe
2023-05-20 23:32:31,444:INFO:Uploading results into container
2023-05-20 23:32:31,445:INFO:Uploading model into container now
2023-05-20 23:32:31,446:INFO:_master_model_container: 23
2023-05-20 23:32:31,446:INFO:_display_container: 2
2023-05-20 23:32:31,447:INFO:GradientBoostingRegressor(random_state=3396)
2023-05-20 23:32:31,447:INFO:create_model() successfully completed......................................
2023-05-20 23:32:31,591:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:31,591:INFO:Creating metrics dataframe
2023-05-20 23:32:31,602:INFO:Initializing Extreme Gradient Boosting
2023-05-20 23:32:31,602:INFO:Total runtime is 1.481538705031077 minutes
2023-05-20 23:32:31,605:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:31,606:INFO:Initializing create_model()
2023-05-20 23:32:31,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:31,606:INFO:Checking exceptions
2023-05-20 23:32:31,606:INFO:Importing libraries
2023-05-20 23:32:31,606:INFO:Copying training dataset
2023-05-20 23:32:31,621:INFO:Defining folds
2023-05-20 23:32:31,621:INFO:Declaring metric variables
2023-05-20 23:32:31,625:INFO:Importing untrained model
2023-05-20 23:32:31,628:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 23:32:31,635:INFO:Starting cross validation
2023-05-20 23:32:31,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:37,600:INFO:Calculating mean and std
2023-05-20 23:32:37,601:INFO:Creating metrics dataframe
2023-05-20 23:32:38,028:INFO:Uploading results into container
2023-05-20 23:32:38,028:INFO:Uploading model into container now
2023-05-20 23:32:38,030:INFO:_master_model_container: 24
2023-05-20 23:32:38,031:INFO:_display_container: 2
2023-05-20 23:32:38,032:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3396, ...)
2023-05-20 23:32:38,032:INFO:create_model() successfully completed......................................
2023-05-20 23:32:38,161:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:38,161:INFO:Creating metrics dataframe
2023-05-20 23:32:38,172:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 23:32:38,172:INFO:Total runtime is 1.591037925084432 minutes
2023-05-20 23:32:38,175:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:38,176:INFO:Initializing create_model()
2023-05-20 23:32:38,176:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:38,176:INFO:Checking exceptions
2023-05-20 23:32:38,176:INFO:Importing libraries
2023-05-20 23:32:38,176:INFO:Copying training dataset
2023-05-20 23:32:38,187:INFO:Defining folds
2023-05-20 23:32:38,187:INFO:Declaring metric variables
2023-05-20 23:32:38,190:INFO:Importing untrained model
2023-05-20 23:32:38,193:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 23:32:38,199:INFO:Starting cross validation
2023-05-20 23:32:38,201:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:32:42,765:INFO:Calculating mean and std
2023-05-20 23:32:42,767:INFO:Creating metrics dataframe
2023-05-20 23:32:43,183:INFO:Uploading results into container
2023-05-20 23:32:43,184:INFO:Uploading model into container now
2023-05-20 23:32:43,185:INFO:_master_model_container: 25
2023-05-20 23:32:43,185:INFO:_display_container: 2
2023-05-20 23:32:43,186:INFO:LGBMRegressor(random_state=3396)
2023-05-20 23:32:43,186:INFO:create_model() successfully completed......................................
2023-05-20 23:32:43,318:INFO:SubProcess create_model() end ==================================
2023-05-20 23:32:43,318:INFO:Creating metrics dataframe
2023-05-20 23:32:43,330:INFO:Initializing CatBoost Regressor
2023-05-20 23:32:43,330:INFO:Total runtime is 1.676999580860138 minutes
2023-05-20 23:32:43,333:INFO:SubProcess create_model() called ==================================
2023-05-20 23:32:43,334:INFO:Initializing create_model()
2023-05-20 23:32:43,334:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:32:43,334:INFO:Checking exceptions
2023-05-20 23:32:43,334:INFO:Importing libraries
2023-05-20 23:32:43,334:INFO:Copying training dataset
2023-05-20 23:32:43,343:INFO:Defining folds
2023-05-20 23:32:43,343:INFO:Declaring metric variables
2023-05-20 23:32:43,347:INFO:Importing untrained model
2023-05-20 23:32:43,350:INFO:CatBoost Regressor Imported successfully
2023-05-20 23:32:43,356:INFO:Starting cross validation
2023-05-20 23:32:43,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:33:10,842:INFO:Calculating mean and std
2023-05-20 23:33:10,844:INFO:Creating metrics dataframe
2023-05-20 23:33:11,266:INFO:Uploading results into container
2023-05-20 23:33:11,267:INFO:Uploading model into container now
2023-05-20 23:33:11,268:INFO:_master_model_container: 26
2023-05-20 23:33:11,268:INFO:_display_container: 2
2023-05-20 23:33:11,268:INFO:<catboost.core.CatBoostRegressor object at 0x00000258ACD08280>
2023-05-20 23:33:11,268:INFO:create_model() successfully completed......................................
2023-05-20 23:33:11,397:INFO:SubProcess create_model() end ==================================
2023-05-20 23:33:11,398:INFO:Creating metrics dataframe
2023-05-20 23:33:11,410:INFO:Initializing Dummy Regressor
2023-05-20 23:33:11,410:INFO:Total runtime is 2.1450009822845457 minutes
2023-05-20 23:33:11,414:INFO:SubProcess create_model() called ==================================
2023-05-20 23:33:11,414:INFO:Initializing create_model()
2023-05-20 23:33:11,414:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258ACD93E20>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:11,414:INFO:Checking exceptions
2023-05-20 23:33:11,414:INFO:Importing libraries
2023-05-20 23:33:11,414:INFO:Copying training dataset
2023-05-20 23:33:11,425:INFO:Defining folds
2023-05-20 23:33:11,425:INFO:Declaring metric variables
2023-05-20 23:33:11,428:INFO:Importing untrained model
2023-05-20 23:33:11,432:INFO:Dummy Regressor Imported successfully
2023-05-20 23:33:11,437:INFO:Starting cross validation
2023-05-20 23:33:11,438:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:33:15,290:INFO:Calculating mean and std
2023-05-20 23:33:15,292:INFO:Creating metrics dataframe
2023-05-20 23:33:15,723:INFO:Uploading results into container
2023-05-20 23:33:15,724:INFO:Uploading model into container now
2023-05-20 23:33:15,725:INFO:_master_model_container: 27
2023-05-20 23:33:15,725:INFO:_display_container: 2
2023-05-20 23:33:15,725:INFO:DummyRegressor()
2023-05-20 23:33:15,725:INFO:create_model() successfully completed......................................
2023-05-20 23:33:15,852:INFO:SubProcess create_model() end ==================================
2023-05-20 23:33:15,852:INFO:Creating metrics dataframe
2023-05-20 23:33:15,871:INFO:Initializing create_model()
2023-05-20 23:33:15,871:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=<catboost.core.CatBoostRegressor object at 0x00000258ACD08280>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:15,871:INFO:Checking exceptions
2023-05-20 23:33:15,874:INFO:Importing libraries
2023-05-20 23:33:15,874:INFO:Copying training dataset
2023-05-20 23:33:15,883:INFO:Defining folds
2023-05-20 23:33:15,883:INFO:Declaring metric variables
2023-05-20 23:33:15,883:INFO:Importing untrained model
2023-05-20 23:33:15,883:INFO:Declaring custom model
2023-05-20 23:33:15,884:INFO:CatBoost Regressor Imported successfully
2023-05-20 23:33:15,885:INFO:Cross validation set to False
2023-05-20 23:33:15,885:INFO:Fitting Model
2023-05-20 23:33:19,745:INFO:<catboost.core.CatBoostRegressor object at 0x00000258ACDDC0D0>
2023-05-20 23:33:19,745:INFO:create_model() successfully completed......................................
2023-05-20 23:33:19,885:INFO:Initializing create_model()
2023-05-20 23:33:19,885:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:19,885:INFO:Checking exceptions
2023-05-20 23:33:19,887:INFO:Importing libraries
2023-05-20 23:33:19,888:INFO:Copying training dataset
2023-05-20 23:33:19,902:INFO:Defining folds
2023-05-20 23:33:19,902:INFO:Declaring metric variables
2023-05-20 23:33:19,903:INFO:Importing untrained model
2023-05-20 23:33:19,903:INFO:Declaring custom model
2023-05-20 23:33:19,903:INFO:Bayesian Ridge Imported successfully
2023-05-20 23:33:19,905:INFO:Cross validation set to False
2023-05-20 23:33:19,905:INFO:Fitting Model
2023-05-20 23:33:20,426:INFO:BayesianRidge()
2023-05-20 23:33:20,426:INFO:create_model() successfully completed......................................
2023-05-20 23:33:20,562:INFO:Initializing create_model()
2023-05-20 23:33:20,563:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=GradientBoostingRegressor(random_state=3396), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:20,563:INFO:Checking exceptions
2023-05-20 23:33:20,565:INFO:Importing libraries
2023-05-20 23:33:20,565:INFO:Copying training dataset
2023-05-20 23:33:20,572:INFO:Defining folds
2023-05-20 23:33:20,572:INFO:Declaring metric variables
2023-05-20 23:33:20,573:INFO:Importing untrained model
2023-05-20 23:33:20,573:INFO:Declaring custom model
2023-05-20 23:33:20,573:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 23:33:20,575:INFO:Cross validation set to False
2023-05-20 23:33:20,575:INFO:Fitting Model
2023-05-20 23:33:21,755:INFO:GradientBoostingRegressor(random_state=3396)
2023-05-20 23:33:21,755:INFO:create_model() successfully completed......................................
2023-05-20 23:33:21,898:INFO:Initializing create_model()
2023-05-20 23:33:21,898:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=OrthogonalMatchingPursuit(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:21,899:INFO:Checking exceptions
2023-05-20 23:33:21,901:INFO:Importing libraries
2023-05-20 23:33:21,901:INFO:Copying training dataset
2023-05-20 23:33:21,908:INFO:Defining folds
2023-05-20 23:33:21,908:INFO:Declaring metric variables
2023-05-20 23:33:21,908:INFO:Importing untrained model
2023-05-20 23:33:21,909:INFO:Declaring custom model
2023-05-20 23:33:21,909:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 23:33:21,910:INFO:Cross validation set to False
2023-05-20 23:33:21,910:INFO:Fitting Model
2023-05-20 23:33:21,953:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning:

The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)




2023-05-20 23:33:22,341:INFO:OrthogonalMatchingPursuit()
2023-05-20 23:33:22,341:INFO:create_model() successfully completed......................................
2023-05-20 23:33:22,473:INFO:Initializing create_model()
2023-05-20 23:33:22,473:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258A2C25240>, estimator=LGBMRegressor(random_state=3396), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:33:22,473:INFO:Checking exceptions
2023-05-20 23:33:22,475:INFO:Importing libraries
2023-05-20 23:33:22,475:INFO:Copying training dataset
2023-05-20 23:33:22,487:INFO:Defining folds
2023-05-20 23:33:22,487:INFO:Declaring metric variables
2023-05-20 23:33:22,487:INFO:Importing untrained model
2023-05-20 23:33:22,487:INFO:Declaring custom model
2023-05-20 23:33:22,488:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 23:33:22,489:INFO:Cross validation set to False
2023-05-20 23:33:22,489:INFO:Fitting Model
2023-05-20 23:33:23,184:INFO:LGBMRegressor(random_state=3396)
2023-05-20 23:33:23,184:INFO:create_model() successfully completed......................................
2023-05-20 23:33:23,337:INFO:_master_model_container: 27
2023-05-20 23:33:23,337:INFO:_display_container: 2
2023-05-20 23:33:23,338:INFO:[<catboost.core.CatBoostRegressor object at 0x00000258ACDDC0D0>, BayesianRidge(), GradientBoostingRegressor(random_state=3396), OrthogonalMatchingPursuit(), LGBMRegressor(random_state=3396)]
2023-05-20 23:33:23,338:INFO:compare_models() successfully completed......................................
2023-05-20 23:53:17,768:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\95960041.py:6: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:53:29,115:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\1793585164.py:6: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:54:56,716:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:4: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:54:56,786:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32896\2221667683.py:9: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-20 23:54:58,218:INFO:PyCaret RegressionExperiment
2023-05-20 23:54:58,219:INFO:Logging name: reg-default-name
2023-05-20 23:54:58,219:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-20 23:54:58,219:INFO:version 3.0.2
2023-05-20 23:54:58,219:INFO:Initializing setup()
2023-05-20 23:54:58,219:INFO:self.USI: 4d6e
2023-05-20 23:54:58,219:INFO:self._variable_keys: {'y_train', 'fold_groups_param', 'fold_generator', 'n_jobs_param', 'memory', 'transform_target_param', 'USI', 'y_test', 'X', 'data', 'seed', 'logging_param', 'gpu_n_jobs_param', '_ml_usecase', 'exp_name_log', 'html_param', 'log_plots_param', 'exp_id', 'X_train', 'X_test', 'target_param', 'fold_shuffle_param', 'y', '_available_plots', 'idx', 'gpu_param', 'pipeline'}
2023-05-20 23:54:58,219:INFO:Checking environment
2023-05-20 23:54:58,219:INFO:python_version: 3.10.3
2023-05-20 23:54:58,219:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-20 23:54:58,219:INFO:machine: AMD64
2023-05-20 23:54:58,219:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-20 23:54:58,219:INFO:Memory: svmem(total=17083187200, available=3534782464, percent=79.3, used=13548404736, free=3534782464)
2023-05-20 23:54:58,219:INFO:Physical Core: 6
2023-05-20 23:54:58,219:INFO:Logical Core: 12
2023-05-20 23:54:58,219:INFO:Checking libraries
2023-05-20 23:54:58,219:INFO:System:
2023-05-20 23:54:58,219:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-20 23:54:58,219:INFO:executable: c:\Python310\python.exe
2023-05-20 23:54:58,220:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-20 23:54:58,220:INFO:PyCaret required dependencies:
2023-05-20 23:54:58,220:INFO:                 pip: 23.1.2
2023-05-20 23:54:58,220:INFO:          setuptools: 58.1.0
2023-05-20 23:54:58,220:INFO:             pycaret: 3.0.2
2023-05-20 23:54:58,220:INFO:             IPython: 8.5.0
2023-05-20 23:54:58,220:INFO:          ipywidgets: 8.0.6
2023-05-20 23:54:58,220:INFO:                tqdm: 4.65.0
2023-05-20 23:54:58,220:INFO:               numpy: 1.23.2
2023-05-20 23:54:58,220:INFO:              pandas: 1.5.2
2023-05-20 23:54:58,220:INFO:              jinja2: 3.1.2
2023-05-20 23:54:58,220:INFO:               scipy: 1.9.3
2023-05-20 23:54:58,220:INFO:              joblib: 1.2.0
2023-05-20 23:54:58,220:INFO:             sklearn: 1.1.3
2023-05-20 23:54:58,220:INFO:                pyod: 1.0.9
2023-05-20 23:54:58,220:INFO:            imblearn: 0.10.1
2023-05-20 23:54:58,220:INFO:   category_encoders: 2.6.1
2023-05-20 23:54:58,220:INFO:            lightgbm: 3.3.5
2023-05-20 23:54:58,220:INFO:               numba: 0.57.0
2023-05-20 23:54:58,220:INFO:            requests: 2.28.2
2023-05-20 23:54:58,220:INFO:          matplotlib: 3.5.3
2023-05-20 23:54:58,220:INFO:          scikitplot: 0.3.7
2023-05-20 23:54:58,220:INFO:         yellowbrick: 1.5
2023-05-20 23:54:58,220:INFO:              plotly: 5.13.1
2023-05-20 23:54:58,220:INFO:             kaleido: 0.2.1
2023-05-20 23:54:58,220:INFO:         statsmodels: 0.13.5
2023-05-20 23:54:58,220:INFO:              sktime: 0.17.0
2023-05-20 23:54:58,220:INFO:               tbats: 1.1.3
2023-05-20 23:54:58,220:INFO:            pmdarima: 2.0.3
2023-05-20 23:54:58,220:INFO:              psutil: 5.9.2
2023-05-20 23:54:58,222:INFO:PyCaret optional dependencies:
2023-05-20 23:54:58,222:INFO:                shap: Not installed
2023-05-20 23:54:58,222:INFO:           interpret: Not installed
2023-05-20 23:54:58,222:INFO:                umap: Not installed
2023-05-20 23:54:58,222:INFO:    pandas_profiling: Not installed
2023-05-20 23:54:58,222:INFO:  explainerdashboard: Not installed
2023-05-20 23:54:58,222:INFO:             autoviz: Not installed
2023-05-20 23:54:58,222:INFO:           fairlearn: Not installed
2023-05-20 23:54:58,222:INFO:             xgboost: 1.7.4
2023-05-20 23:54:58,222:INFO:            catboost: 1.1.1
2023-05-20 23:54:58,222:INFO:              kmodes: Not installed
2023-05-20 23:54:58,222:INFO:             mlxtend: Not installed
2023-05-20 23:54:58,222:INFO:       statsforecast: Not installed
2023-05-20 23:54:58,222:INFO:        tune_sklearn: Not installed
2023-05-20 23:54:58,222:INFO:                 ray: Not installed
2023-05-20 23:54:58,222:INFO:            hyperopt: Not installed
2023-05-20 23:54:58,222:INFO:              optuna: Not installed
2023-05-20 23:54:58,222:INFO:               skopt: Not installed
2023-05-20 23:54:58,222:INFO:              mlflow: Not installed
2023-05-20 23:54:58,222:INFO:              gradio: Not installed
2023-05-20 23:54:58,222:INFO:             fastapi: Not installed
2023-05-20 23:54:58,222:INFO:             uvicorn: Not installed
2023-05-20 23:54:58,222:INFO:              m2cgen: Not installed
2023-05-20 23:54:58,223:INFO:           evidently: Not installed
2023-05-20 23:54:58,223:INFO:               fugue: Not installed
2023-05-20 23:54:58,223:INFO:           streamlit: Not installed
2023-05-20 23:54:58,223:INFO:             prophet: Not installed
2023-05-20 23:54:58,223:INFO:None
2023-05-20 23:54:58,223:INFO:Set up data.
2023-05-20 23:54:58,320:INFO:Set up train/test split.
2023-05-20 23:54:58,337:INFO:Set up index.
2023-05-20 23:54:58,338:INFO:Set up folding strategy.
2023-05-20 23:54:58,339:INFO:Assigning column types.
2023-05-20 23:54:58,348:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-20 23:54:58,349:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,353:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,357:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,420:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,461:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,461:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:58,463:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:58,464:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,468:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,472:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,530:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,574:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,574:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:58,576:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:58,580:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-20 23:54:58,585:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,589:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,647:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,688:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,688:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:58,692:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:58,701:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,707:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,774:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,826:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,827:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:58,832:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:58,833:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-20 23:54:58,847:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,924:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,966:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:58,966:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:58,969:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:58,978:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,039:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,077:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,077:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,079:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,081:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-20 23:54:59,141:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,184:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,184:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,187:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,249:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,287:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,288:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,290:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,291:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-20 23:54:59,349:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,390:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,394:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,467:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-20 23:54:59,512:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,514:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,514:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-20 23:54:59,615:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,617:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,719:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:54:59,722:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:54:59,723:INFO:Preparing preprocessing pipeline...
2023-05-20 23:54:59,723:INFO:Set up simple imputation.
2023-05-20 23:54:59,725:INFO:Set up column name cleaning.
2023-05-20 23:54:59,794:INFO:Finished creating preprocessing pipeline.
2023-05-20 23:54:59,801:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-20 23:54:59,801:INFO:Creating final display dataframe.
2023-05-20 23:54:59,986:INFO:Setup _display_container:                     Description             Value
0                    Session id              3832
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 319)
4        Transformed data shape       (1460, 319)
5   Transformed train set shape       (1021, 319)
6    Transformed test set shape        (439, 319)
7              Numeric features               318
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              4d6e
2023-05-20 23:55:00,109:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:55:00,111:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:55:00,219:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-20 23:55:00,221:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-20 23:55:00,222:INFO:setup() successfully completed in 2.59s...............
2023-05-20 23:55:00,685:INFO:Initializing compare_models()
2023-05-20 23:55:00,685:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-20 23:55:00,685:INFO:Checking exceptions
2023-05-20 23:55:00,691:INFO:Preparing display monitor
2023-05-20 23:55:00,755:INFO:Initializing Linear Regression
2023-05-20 23:55:00,756:INFO:Total runtime is 0.0 minutes
2023-05-20 23:55:00,761:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:00,761:INFO:Initializing create_model()
2023-05-20 23:55:00,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:00,762:INFO:Checking exceptions
2023-05-20 23:55:00,762:INFO:Importing libraries
2023-05-20 23:55:00,762:INFO:Copying training dataset
2023-05-20 23:55:00,776:INFO:Defining folds
2023-05-20 23:55:00,776:INFO:Declaring metric variables
2023-05-20 23:55:00,782:INFO:Importing untrained model
2023-05-20 23:55:00,785:INFO:Linear Regression Imported successfully
2023-05-20 23:55:00,795:INFO:Starting cross validation
2023-05-20 23:55:00,798:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:06,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:06,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:06,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:06,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:06,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:11,291:INFO:Calculating mean and std
2023-05-20 23:55:11,293:INFO:Creating metrics dataframe
2023-05-20 23:55:11,773:INFO:Uploading results into container
2023-05-20 23:55:11,775:INFO:Uploading model into container now
2023-05-20 23:55:11,776:INFO:_master_model_container: 1
2023-05-20 23:55:11,776:INFO:_display_container: 2
2023-05-20 23:55:11,776:INFO:LinearRegression(n_jobs=-1)
2023-05-20 23:55:11,776:INFO:create_model() successfully completed......................................
2023-05-20 23:55:12,372:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:12,372:INFO:Creating metrics dataframe
2023-05-20 23:55:12,382:INFO:Initializing Lasso Regression
2023-05-20 23:55:12,382:INFO:Total runtime is 0.19378343025843303 minutes
2023-05-20 23:55:12,385:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:12,386:INFO:Initializing create_model()
2023-05-20 23:55:12,386:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:12,386:INFO:Checking exceptions
2023-05-20 23:55:12,387:INFO:Importing libraries
2023-05-20 23:55:12,387:INFO:Copying training dataset
2023-05-20 23:55:12,400:INFO:Defining folds
2023-05-20 23:55:12,400:INFO:Declaring metric variables
2023-05-20 23:55:12,406:INFO:Importing untrained model
2023-05-20 23:55:12,410:INFO:Lasso Regression Imported successfully
2023-05-20 23:55:12,419:INFO:Starting cross validation
2023-05-20 23:55:12,421:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:12,470:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,476:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,484:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,495:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,508:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,519:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,533:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:12,546:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:14,906:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:14,918:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:16,776:INFO:Calculating mean and std
2023-05-20 23:55:16,778:INFO:Creating metrics dataframe
2023-05-20 23:55:17,208:INFO:Uploading results into container
2023-05-20 23:55:17,209:INFO:Uploading model into container now
2023-05-20 23:55:17,209:INFO:_master_model_container: 2
2023-05-20 23:55:17,210:INFO:_display_container: 2
2023-05-20 23:55:17,210:INFO:Lasso(random_state=3832)
2023-05-20 23:55:17,210:INFO:create_model() successfully completed......................................
2023-05-20 23:55:17,462:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:17,463:INFO:Creating metrics dataframe
2023-05-20 23:55:17,472:INFO:Initializing Ridge Regression
2023-05-20 23:55:17,473:INFO:Total runtime is 0.2786415576934814 minutes
2023-05-20 23:55:17,476:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:17,476:INFO:Initializing create_model()
2023-05-20 23:55:17,476:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:17,476:INFO:Checking exceptions
2023-05-20 23:55:17,476:INFO:Importing libraries
2023-05-20 23:55:17,476:INFO:Copying training dataset
2023-05-20 23:55:17,490:INFO:Defining folds
2023-05-20 23:55:17,490:INFO:Declaring metric variables
2023-05-20 23:55:17,499:INFO:Importing untrained model
2023-05-20 23:55:17,508:INFO:Ridge Regression Imported successfully
2023-05-20 23:55:17,518:INFO:Starting cross validation
2023-05-20 23:55:17,522:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:17,549:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,555:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,561:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,569:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,580:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,588:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,602:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,614:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,626:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:17,639:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:21,577:INFO:Calculating mean and std
2023-05-20 23:55:21,578:INFO:Creating metrics dataframe
2023-05-20 23:55:22,049:INFO:Uploading results into container
2023-05-20 23:55:22,050:INFO:Uploading model into container now
2023-05-20 23:55:22,051:INFO:_master_model_container: 3
2023-05-20 23:55:22,051:INFO:_display_container: 2
2023-05-20 23:55:22,051:INFO:Ridge(random_state=3832)
2023-05-20 23:55:22,051:INFO:create_model() successfully completed......................................
2023-05-20 23:55:22,253:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:22,253:INFO:Creating metrics dataframe
2023-05-20 23:55:22,262:INFO:Initializing Elastic Net
2023-05-20 23:55:22,262:INFO:Total runtime is 0.3584495306015014 minutes
2023-05-20 23:55:22,266:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:22,266:INFO:Initializing create_model()
2023-05-20 23:55:22,267:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:22,267:INFO:Checking exceptions
2023-05-20 23:55:22,267:INFO:Importing libraries
2023-05-20 23:55:22,267:INFO:Copying training dataset
2023-05-20 23:55:22,277:INFO:Defining folds
2023-05-20 23:55:22,277:INFO:Declaring metric variables
2023-05-20 23:55:22,282:INFO:Importing untrained model
2023-05-20 23:55:22,285:INFO:Elastic Net Imported successfully
2023-05-20 23:55:22,291:INFO:Starting cross validation
2023-05-20 23:55:22,293:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:22,320:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,326:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,332:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,339:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,347:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,358:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,372:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,385:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,397:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:22,413:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:26,556:INFO:Calculating mean and std
2023-05-20 23:55:26,559:INFO:Creating metrics dataframe
2023-05-20 23:55:27,077:INFO:Uploading results into container
2023-05-20 23:55:27,078:INFO:Uploading model into container now
2023-05-20 23:55:27,079:INFO:_master_model_container: 4
2023-05-20 23:55:27,079:INFO:_display_container: 2
2023-05-20 23:55:27,080:INFO:ElasticNet(random_state=3832)
2023-05-20 23:55:27,080:INFO:create_model() successfully completed......................................
2023-05-20 23:55:27,419:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:27,420:INFO:Creating metrics dataframe
2023-05-20 23:55:27,434:INFO:Initializing Least Angle Regression
2023-05-20 23:55:27,435:INFO:Total runtime is 0.4446662584940592 minutes
2023-05-20 23:55:27,438:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:27,439:INFO:Initializing create_model()
2023-05-20 23:55:27,439:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:27,439:INFO:Checking exceptions
2023-05-20 23:55:27,439:INFO:Importing libraries
2023-05-20 23:55:27,439:INFO:Copying training dataset
2023-05-20 23:55:27,449:INFO:Defining folds
2023-05-20 23:55:27,449:INFO:Declaring metric variables
2023-05-20 23:55:27,453:INFO:Importing untrained model
2023-05-20 23:55:27,459:INFO:Least Angle Regression Imported successfully
2023-05-20 23:55:27,467:INFO:Starting cross validation
2023-05-20 23:55:27,469:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:27,501:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,509:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,516:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,526:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,538:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,556:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,573:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,589:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,602:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,616:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:27,629:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,639:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,640:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,662:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=7.745e-04, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,663:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,674:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.528e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,676:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=3.701e-04, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,677:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,682:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,683:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.999e-04, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,694:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.622e-04, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,700:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.553e-04, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,702:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.448e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,702:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,709:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.295e-03, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,709:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=7.011e-04, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,712:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=2.088e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,718:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.309e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,719:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.219e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,722:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.636e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,723:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,730:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.751e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,772:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:27,795:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.497e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,810:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 225 iterations, i.e. alpha=1.424e-03, with an active set of 201 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,828:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-20 23:55:27,839:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=3.693e-04, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,846:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.616e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,851:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.616e-03, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,857:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 249 iterations, i.e. alpha=9.680e-04, with an active set of 212 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,870:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.669e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,956:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 204 iterations, i.e. alpha=1.865e-03, with an active set of 181 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:27,997:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 264 iterations, i.e. alpha=3.183e-03, with an active set of 226 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-20 23:55:28,027:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-20 23:55:28,278:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,278:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,279:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,281:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,282:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,282:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,316:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,317:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,318:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,324:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,326:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,326:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,326:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,327:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,328:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,339:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,340:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,340:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,348:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-20 23:55:28,352:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-20 23:55:28,357:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,358:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,358:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,407:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,408:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,408:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:28,432:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,432:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-20 23:55:28,433:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-20 23:55:32,757:INFO:Calculating mean and std
2023-05-20 23:55:32,758:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-20 23:55:32,759:INFO:Creating metrics dataframe
2023-05-20 23:55:33,300:INFO:Uploading results into container
2023-05-20 23:55:33,302:INFO:Uploading model into container now
2023-05-20 23:55:33,302:INFO:_master_model_container: 5
2023-05-20 23:55:33,302:INFO:_display_container: 2
2023-05-20 23:55:33,302:INFO:Lars(random_state=3832)
2023-05-20 23:55:33,303:INFO:create_model() successfully completed......................................
2023-05-20 23:55:33,520:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:33,521:INFO:Creating metrics dataframe
2023-05-20 23:55:33,532:INFO:Initializing Lasso Least Angle Regression
2023-05-20 23:55:33,532:INFO:Total runtime is 0.5462869127591451 minutes
2023-05-20 23:55:33,535:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:33,536:INFO:Initializing create_model()
2023-05-20 23:55:33,536:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:33,536:INFO:Checking exceptions
2023-05-20 23:55:33,536:INFO:Importing libraries
2023-05-20 23:55:33,536:INFO:Copying training dataset
2023-05-20 23:55:33,550:INFO:Defining folds
2023-05-20 23:55:33,550:INFO:Declaring metric variables
2023-05-20 23:55:33,555:INFO:Importing untrained model
2023-05-20 23:55:33,558:INFO:Lasso Least Angle Regression Imported successfully
2023-05-20 23:55:33,565:INFO:Starting cross validation
2023-05-20 23:55:33,567:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:33,592:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,599:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,605:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,616:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,625:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,635:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,647:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,658:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,671:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,685:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:33,691:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,693:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,694:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,753:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,758:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,774:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,818:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:33,834:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-20 23:55:37,950:INFO:Calculating mean and std
2023-05-20 23:55:37,953:INFO:Creating metrics dataframe
2023-05-20 23:55:38,405:INFO:Uploading results into container
2023-05-20 23:55:38,406:INFO:Uploading model into container now
2023-05-20 23:55:38,406:INFO:_master_model_container: 6
2023-05-20 23:55:38,407:INFO:_display_container: 2
2023-05-20 23:55:38,407:INFO:LassoLars(random_state=3832)
2023-05-20 23:55:38,407:INFO:create_model() successfully completed......................................
2023-05-20 23:55:38,615:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:38,615:INFO:Creating metrics dataframe
2023-05-20 23:55:38,625:INFO:Initializing Orthogonal Matching Pursuit
2023-05-20 23:55:38,625:INFO:Total runtime is 0.6311738689740499 minutes
2023-05-20 23:55:38,629:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:38,629:INFO:Initializing create_model()
2023-05-20 23:55:38,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:38,629:INFO:Checking exceptions
2023-05-20 23:55:38,629:INFO:Importing libraries
2023-05-20 23:55:38,630:INFO:Copying training dataset
2023-05-20 23:55:38,642:INFO:Defining folds
2023-05-20 23:55:38,644:INFO:Declaring metric variables
2023-05-20 23:55:38,650:INFO:Importing untrained model
2023-05-20 23:55:38,654:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 23:55:38,661:INFO:Starting cross validation
2023-05-20 23:55:38,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:38,688:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,694:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,700:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,707:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,716:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,727:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,737:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,749:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,762:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,765:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,775:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:38,793:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,798:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,809:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,819:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,845:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,847:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,850:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,880:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:38,895:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-20 23:55:42,920:INFO:Calculating mean and std
2023-05-20 23:55:42,923:INFO:Creating metrics dataframe
2023-05-20 23:55:43,394:INFO:Uploading results into container
2023-05-20 23:55:43,396:INFO:Uploading model into container now
2023-05-20 23:55:43,396:INFO:_master_model_container: 7
2023-05-20 23:55:43,396:INFO:_display_container: 2
2023-05-20 23:55:43,397:INFO:OrthogonalMatchingPursuit()
2023-05-20 23:55:43,397:INFO:create_model() successfully completed......................................
2023-05-20 23:55:43,596:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:43,597:INFO:Creating metrics dataframe
2023-05-20 23:55:43,606:INFO:Initializing Bayesian Ridge
2023-05-20 23:55:43,606:INFO:Total runtime is 0.7141907334327698 minutes
2023-05-20 23:55:43,609:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:43,609:INFO:Initializing create_model()
2023-05-20 23:55:43,609:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:43,609:INFO:Checking exceptions
2023-05-20 23:55:43,609:INFO:Importing libraries
2023-05-20 23:55:43,609:INFO:Copying training dataset
2023-05-20 23:55:43,621:INFO:Defining folds
2023-05-20 23:55:43,621:INFO:Declaring metric variables
2023-05-20 23:55:43,624:INFO:Importing untrained model
2023-05-20 23:55:43,630:INFO:Bayesian Ridge Imported successfully
2023-05-20 23:55:43,636:INFO:Starting cross validation
2023-05-20 23:55:43,638:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:43,669:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,675:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,683:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,694:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,702:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,712:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,724:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,738:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,752:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:43,765:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:47,924:INFO:Calculating mean and std
2023-05-20 23:55:47,926:INFO:Creating metrics dataframe
2023-05-20 23:55:48,378:INFO:Uploading results into container
2023-05-20 23:55:48,380:INFO:Uploading model into container now
2023-05-20 23:55:48,380:INFO:_master_model_container: 8
2023-05-20 23:55:48,380:INFO:_display_container: 2
2023-05-20 23:55:48,381:INFO:BayesianRidge()
2023-05-20 23:55:48,381:INFO:create_model() successfully completed......................................
2023-05-20 23:55:48,649:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:48,649:INFO:Creating metrics dataframe
2023-05-20 23:55:48,662:INFO:Initializing Passive Aggressive Regressor
2023-05-20 23:55:48,662:INFO:Total runtime is 0.7984504143397014 minutes
2023-05-20 23:55:48,668:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:48,668:INFO:Initializing create_model()
2023-05-20 23:55:48,669:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:48,669:INFO:Checking exceptions
2023-05-20 23:55:48,669:INFO:Importing libraries
2023-05-20 23:55:48,669:INFO:Copying training dataset
2023-05-20 23:55:48,681:INFO:Defining folds
2023-05-20 23:55:48,681:INFO:Declaring metric variables
2023-05-20 23:55:48,684:INFO:Importing untrained model
2023-05-20 23:55:48,690:INFO:Passive Aggressive Regressor Imported successfully
2023-05-20 23:55:48,698:INFO:Starting cross validation
2023-05-20 23:55:48,700:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:48,725:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,732:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,739:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,752:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,762:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,777:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,790:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,805:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,816:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:48,830:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:52,956:INFO:Calculating mean and std
2023-05-20 23:55:52,958:INFO:Creating metrics dataframe
2023-05-20 23:55:53,408:INFO:Uploading results into container
2023-05-20 23:55:53,408:INFO:Uploading model into container now
2023-05-20 23:55:53,409:INFO:_master_model_container: 9
2023-05-20 23:55:53,409:INFO:_display_container: 2
2023-05-20 23:55:53,410:INFO:PassiveAggressiveRegressor(random_state=3832)
2023-05-20 23:55:53,410:INFO:create_model() successfully completed......................................
2023-05-20 23:55:53,613:INFO:SubProcess create_model() end ==================================
2023-05-20 23:55:53,613:INFO:Creating metrics dataframe
2023-05-20 23:55:53,624:INFO:Initializing Huber Regressor
2023-05-20 23:55:53,624:INFO:Total runtime is 0.8811495820681255 minutes
2023-05-20 23:55:53,628:INFO:SubProcess create_model() called ==================================
2023-05-20 23:55:53,629:INFO:Initializing create_model()
2023-05-20 23:55:53,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:55:53,629:INFO:Checking exceptions
2023-05-20 23:55:53,629:INFO:Importing libraries
2023-05-20 23:55:53,629:INFO:Copying training dataset
2023-05-20 23:55:53,640:INFO:Defining folds
2023-05-20 23:55:53,640:INFO:Declaring metric variables
2023-05-20 23:55:53,644:INFO:Importing untrained model
2023-05-20 23:55:53,648:INFO:Huber Regressor Imported successfully
2023-05-20 23:55:53,655:INFO:Starting cross validation
2023-05-20 23:55:53,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:55:53,688:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,693:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,699:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,707:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,715:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,725:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,735:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,750:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,765:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:53,777:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:55:56,209:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,701:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,740:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,781:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,793:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,847:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,855:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,866:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:56,915:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:55:57,077:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-20 23:56:00,743:INFO:Calculating mean and std
2023-05-20 23:56:00,745:INFO:Creating metrics dataframe
2023-05-20 23:56:01,218:INFO:Uploading results into container
2023-05-20 23:56:01,219:INFO:Uploading model into container now
2023-05-20 23:56:01,221:INFO:_master_model_container: 10
2023-05-20 23:56:01,221:INFO:_display_container: 2
2023-05-20 23:56:01,221:INFO:HuberRegressor()
2023-05-20 23:56:01,221:INFO:create_model() successfully completed......................................
2023-05-20 23:56:01,445:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:01,445:INFO:Creating metrics dataframe
2023-05-20 23:56:01,456:INFO:Initializing K Neighbors Regressor
2023-05-20 23:56:01,456:INFO:Total runtime is 1.0116926391919456 minutes
2023-05-20 23:56:01,460:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:01,460:INFO:Initializing create_model()
2023-05-20 23:56:01,460:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:01,460:INFO:Checking exceptions
2023-05-20 23:56:01,460:INFO:Importing libraries
2023-05-20 23:56:01,460:INFO:Copying training dataset
2023-05-20 23:56:01,473:INFO:Defining folds
2023-05-20 23:56:01,473:INFO:Declaring metric variables
2023-05-20 23:56:01,476:INFO:Importing untrained model
2023-05-20 23:56:01,480:INFO:K Neighbors Regressor Imported successfully
2023-05-20 23:56:01,486:INFO:Starting cross validation
2023-05-20 23:56:01,487:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:01,516:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,523:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,529:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,542:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,553:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,565:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,577:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,590:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,604:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:01,616:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:05,753:INFO:Calculating mean and std
2023-05-20 23:56:05,755:INFO:Creating metrics dataframe
2023-05-20 23:56:06,196:INFO:Uploading results into container
2023-05-20 23:56:06,197:INFO:Uploading model into container now
2023-05-20 23:56:06,198:INFO:_master_model_container: 11
2023-05-20 23:56:06,198:INFO:_display_container: 2
2023-05-20 23:56:06,199:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-20 23:56:06,199:INFO:create_model() successfully completed......................................
2023-05-20 23:56:06,385:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:06,385:INFO:Creating metrics dataframe
2023-05-20 23:56:06,394:INFO:Initializing Decision Tree Regressor
2023-05-20 23:56:06,394:INFO:Total runtime is 1.0939902861913047 minutes
2023-05-20 23:56:06,398:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:06,398:INFO:Initializing create_model()
2023-05-20 23:56:06,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:06,399:INFO:Checking exceptions
2023-05-20 23:56:06,399:INFO:Importing libraries
2023-05-20 23:56:06,399:INFO:Copying training dataset
2023-05-20 23:56:06,409:INFO:Defining folds
2023-05-20 23:56:06,409:INFO:Declaring metric variables
2023-05-20 23:56:06,412:INFO:Importing untrained model
2023-05-20 23:56:06,416:INFO:Decision Tree Regressor Imported successfully
2023-05-20 23:56:06,422:INFO:Starting cross validation
2023-05-20 23:56:06,423:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:06,448:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,454:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,460:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,467:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,474:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,481:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,493:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,504:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,513:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:06,529:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:10,390:INFO:Calculating mean and std
2023-05-20 23:56:10,392:INFO:Creating metrics dataframe
2023-05-20 23:56:10,837:INFO:Uploading results into container
2023-05-20 23:56:10,837:INFO:Uploading model into container now
2023-05-20 23:56:10,838:INFO:_master_model_container: 12
2023-05-20 23:56:10,838:INFO:_display_container: 2
2023-05-20 23:56:10,838:INFO:DecisionTreeRegressor(random_state=3832)
2023-05-20 23:56:10,838:INFO:create_model() successfully completed......................................
2023-05-20 23:56:11,019:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:11,021:INFO:Creating metrics dataframe
2023-05-20 23:56:11,031:INFO:Initializing Random Forest Regressor
2023-05-20 23:56:11,031:INFO:Total runtime is 1.1712694128354393 minutes
2023-05-20 23:56:11,034:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:11,035:INFO:Initializing create_model()
2023-05-20 23:56:11,035:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:11,035:INFO:Checking exceptions
2023-05-20 23:56:11,035:INFO:Importing libraries
2023-05-20 23:56:11,035:INFO:Copying training dataset
2023-05-20 23:56:11,044:INFO:Defining folds
2023-05-20 23:56:11,044:INFO:Declaring metric variables
2023-05-20 23:56:11,048:INFO:Importing untrained model
2023-05-20 23:56:11,051:INFO:Random Forest Regressor Imported successfully
2023-05-20 23:56:11,057:INFO:Starting cross validation
2023-05-20 23:56:11,058:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:11,087:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,092:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,098:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,105:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,115:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,124:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,136:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,147:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,158:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:11,171:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:13,213:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:13,253:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:13,471:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:13,471:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:13,478:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:13,549:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:17,612:INFO:Calculating mean and std
2023-05-20 23:56:17,614:INFO:Creating metrics dataframe
2023-05-20 23:56:18,068:INFO:Uploading results into container
2023-05-20 23:56:18,069:INFO:Uploading model into container now
2023-05-20 23:56:18,070:INFO:_master_model_container: 13
2023-05-20 23:56:18,070:INFO:_display_container: 2
2023-05-20 23:56:18,071:INFO:RandomForestRegressor(n_jobs=-1, random_state=3832)
2023-05-20 23:56:18,071:INFO:create_model() successfully completed......................................
2023-05-20 23:56:18,283:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:18,283:INFO:Creating metrics dataframe
2023-05-20 23:56:18,293:INFO:Initializing Extra Trees Regressor
2023-05-20 23:56:18,293:INFO:Total runtime is 1.292312832673391 minutes
2023-05-20 23:56:18,298:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:18,298:INFO:Initializing create_model()
2023-05-20 23:56:18,298:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:18,298:INFO:Checking exceptions
2023-05-20 23:56:18,298:INFO:Importing libraries
2023-05-20 23:56:18,299:INFO:Copying training dataset
2023-05-20 23:56:18,308:INFO:Defining folds
2023-05-20 23:56:18,308:INFO:Declaring metric variables
2023-05-20 23:56:18,310:INFO:Importing untrained model
2023-05-20 23:56:18,314:INFO:Extra Trees Regressor Imported successfully
2023-05-20 23:56:18,320:INFO:Starting cross validation
2023-05-20 23:56:18,323:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:18,356:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,361:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,368:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,374:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,383:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,392:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,399:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,410:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,422:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:18,437:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:21,073:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.48s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:21,085:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.38s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-20 23:56:25,139:INFO:Calculating mean and std
2023-05-20 23:56:25,140:INFO:Creating metrics dataframe
2023-05-20 23:56:25,599:INFO:Uploading results into container
2023-05-20 23:56:25,600:INFO:Uploading model into container now
2023-05-20 23:56:25,601:INFO:_master_model_container: 14
2023-05-20 23:56:25,601:INFO:_display_container: 2
2023-05-20 23:56:25,601:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3832)
2023-05-20 23:56:25,602:INFO:create_model() successfully completed......................................
2023-05-20 23:56:25,809:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:25,809:INFO:Creating metrics dataframe
2023-05-20 23:56:25,820:INFO:Initializing AdaBoost Regressor
2023-05-20 23:56:25,820:INFO:Total runtime is 1.417752269903819 minutes
2023-05-20 23:56:25,824:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:25,824:INFO:Initializing create_model()
2023-05-20 23:56:25,824:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:25,824:INFO:Checking exceptions
2023-05-20 23:56:25,825:INFO:Importing libraries
2023-05-20 23:56:25,825:INFO:Copying training dataset
2023-05-20 23:56:25,835:INFO:Defining folds
2023-05-20 23:56:25,835:INFO:Declaring metric variables
2023-05-20 23:56:25,839:INFO:Importing untrained model
2023-05-20 23:56:25,842:INFO:AdaBoost Regressor Imported successfully
2023-05-20 23:56:25,849:INFO:Starting cross validation
2023-05-20 23:56:25,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:25,882:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,889:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,896:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,903:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,912:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,920:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,930:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,940:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,952:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:25,966:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,132:INFO:Calculating mean and std
2023-05-20 23:56:31,133:INFO:Creating metrics dataframe
2023-05-20 23:56:31,593:INFO:Uploading results into container
2023-05-20 23:56:31,594:INFO:Uploading model into container now
2023-05-20 23:56:31,595:INFO:_master_model_container: 15
2023-05-20 23:56:31,595:INFO:_display_container: 2
2023-05-20 23:56:31,595:INFO:AdaBoostRegressor(random_state=3832)
2023-05-20 23:56:31,595:INFO:create_model() successfully completed......................................
2023-05-20 23:56:31,778:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:31,778:INFO:Creating metrics dataframe
2023-05-20 23:56:31,789:INFO:Initializing Gradient Boosting Regressor
2023-05-20 23:56:31,789:INFO:Total runtime is 1.5172401944796245 minutes
2023-05-20 23:56:31,793:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:31,793:INFO:Initializing create_model()
2023-05-20 23:56:31,793:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:31,793:INFO:Checking exceptions
2023-05-20 23:56:31,793:INFO:Importing libraries
2023-05-20 23:56:31,793:INFO:Copying training dataset
2023-05-20 23:56:31,802:INFO:Defining folds
2023-05-20 23:56:31,802:INFO:Declaring metric variables
2023-05-20 23:56:31,806:INFO:Importing untrained model
2023-05-20 23:56:31,809:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 23:56:31,814:INFO:Starting cross validation
2023-05-20 23:56:31,816:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:31,857:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,866:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,875:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,883:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,896:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,906:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,920:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,933:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,944:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:31,960:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:37,238:INFO:Calculating mean and std
2023-05-20 23:56:37,241:INFO:Creating metrics dataframe
2023-05-20 23:56:37,725:INFO:Uploading results into container
2023-05-20 23:56:37,725:INFO:Uploading model into container now
2023-05-20 23:56:37,726:INFO:_master_model_container: 16
2023-05-20 23:56:37,726:INFO:_display_container: 2
2023-05-20 23:56:37,727:INFO:GradientBoostingRegressor(random_state=3832)
2023-05-20 23:56:37,727:INFO:create_model() successfully completed......................................
2023-05-20 23:56:37,917:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:37,917:INFO:Creating metrics dataframe
2023-05-20 23:56:37,931:INFO:Initializing Extreme Gradient Boosting
2023-05-20 23:56:37,931:INFO:Total runtime is 1.6195981105168662 minutes
2023-05-20 23:56:37,935:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:37,935:INFO:Initializing create_model()
2023-05-20 23:56:37,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:37,935:INFO:Checking exceptions
2023-05-20 23:56:37,935:INFO:Importing libraries
2023-05-20 23:56:37,935:INFO:Copying training dataset
2023-05-20 23:56:37,945:INFO:Defining folds
2023-05-20 23:56:37,946:INFO:Declaring metric variables
2023-05-20 23:56:37,950:INFO:Importing untrained model
2023-05-20 23:56:37,954:INFO:Extreme Gradient Boosting Imported successfully
2023-05-20 23:56:37,960:INFO:Starting cross validation
2023-05-20 23:56:37,962:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:38,029:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,029:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,038:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,044:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,062:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,064:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,082:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,093:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,100:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:38,113:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,158:INFO:Calculating mean and std
2023-05-20 23:56:44,160:INFO:Creating metrics dataframe
2023-05-20 23:56:44,633:INFO:Uploading results into container
2023-05-20 23:56:44,633:INFO:Uploading model into container now
2023-05-20 23:56:44,634:INFO:_master_model_container: 17
2023-05-20 23:56:44,634:INFO:_display_container: 2
2023-05-20 23:56:44,635:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3832, ...)
2023-05-20 23:56:44,635:INFO:create_model() successfully completed......................................
2023-05-20 23:56:44,820:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:44,820:INFO:Creating metrics dataframe
2023-05-20 23:56:44,834:INFO:Initializing Light Gradient Boosting Machine
2023-05-20 23:56:44,834:INFO:Total runtime is 1.7346525112787885 minutes
2023-05-20 23:56:44,837:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:44,838:INFO:Initializing create_model()
2023-05-20 23:56:44,838:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:44,838:INFO:Checking exceptions
2023-05-20 23:56:44,838:INFO:Importing libraries
2023-05-20 23:56:44,838:INFO:Copying training dataset
2023-05-20 23:56:44,849:INFO:Defining folds
2023-05-20 23:56:44,850:INFO:Declaring metric variables
2023-05-20 23:56:44,852:INFO:Importing untrained model
2023-05-20 23:56:44,856:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 23:56:44,863:INFO:Starting cross validation
2023-05-20 23:56:44,865:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:44,962:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,962:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,963:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,973:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,980:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:44,994:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:45,005:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:45,013:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:45,026:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:45,044:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:49,866:INFO:Calculating mean and std
2023-05-20 23:56:49,868:INFO:Creating metrics dataframe
2023-05-20 23:56:50,340:INFO:Uploading results into container
2023-05-20 23:56:50,341:INFO:Uploading model into container now
2023-05-20 23:56:50,342:INFO:_master_model_container: 18
2023-05-20 23:56:50,342:INFO:_display_container: 2
2023-05-20 23:56:50,342:INFO:LGBMRegressor(random_state=3832)
2023-05-20 23:56:50,342:INFO:create_model() successfully completed......................................
2023-05-20 23:56:50,524:INFO:SubProcess create_model() end ==================================
2023-05-20 23:56:50,524:INFO:Creating metrics dataframe
2023-05-20 23:56:50,536:INFO:Initializing CatBoost Regressor
2023-05-20 23:56:50,536:INFO:Total runtime is 1.829682803153992 minutes
2023-05-20 23:56:50,540:INFO:SubProcess create_model() called ==================================
2023-05-20 23:56:50,540:INFO:Initializing create_model()
2023-05-20 23:56:50,540:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:56:50,540:INFO:Checking exceptions
2023-05-20 23:56:50,540:INFO:Importing libraries
2023-05-20 23:56:50,541:INFO:Copying training dataset
2023-05-20 23:56:50,550:INFO:Defining folds
2023-05-20 23:56:50,551:INFO:Declaring metric variables
2023-05-20 23:56:50,554:INFO:Importing untrained model
2023-05-20 23:56:50,557:INFO:CatBoost Regressor Imported successfully
2023-05-20 23:56:50,563:INFO:Starting cross validation
2023-05-20 23:56:50,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:56:51,650:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,650:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,650:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,652:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,652:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,655:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,694:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:56:51,724:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:20,243:INFO:Calculating mean and std
2023-05-20 23:57:20,246:INFO:Creating metrics dataframe
2023-05-20 23:57:20,764:INFO:Uploading results into container
2023-05-20 23:57:20,766:INFO:Uploading model into container now
2023-05-20 23:57:20,768:INFO:_master_model_container: 19
2023-05-20 23:57:20,768:INFO:_display_container: 2
2023-05-20 23:57:20,768:INFO:<catboost.core.CatBoostRegressor object at 0x00000258B388A4A0>
2023-05-20 23:57:20,769:INFO:create_model() successfully completed......................................
2023-05-20 23:57:21,048:INFO:SubProcess create_model() end ==================================
2023-05-20 23:57:21,048:INFO:Creating metrics dataframe
2023-05-20 23:57:21,061:INFO:Initializing Dummy Regressor
2023-05-20 23:57:21,061:INFO:Total runtime is 2.338431533177694 minutes
2023-05-20 23:57:21,065:INFO:SubProcess create_model() called ==================================
2023-05-20 23:57:21,065:INFO:Initializing create_model()
2023-05-20 23:57:21,065:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000258BC6EFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:21,065:INFO:Checking exceptions
2023-05-20 23:57:21,065:INFO:Importing libraries
2023-05-20 23:57:21,065:INFO:Copying training dataset
2023-05-20 23:57:21,075:INFO:Defining folds
2023-05-20 23:57:21,076:INFO:Declaring metric variables
2023-05-20 23:57:21,079:INFO:Importing untrained model
2023-05-20 23:57:21,083:INFO:Dummy Regressor Imported successfully
2023-05-20 23:57:21,089:INFO:Starting cross validation
2023-05-20 23:57:21,091:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-20 23:57:21,124:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,130:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,138:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,147:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,157:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,169:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,177:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,191:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,209:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:21,224:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:135: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.64.1', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '23.1.2', 'setuptools': '58.1.0', 'pycaret': '3.0.2', 'IPython': '8.5.0', 'ipywidgets': '8.0.6', 'tqdm': '4.65.0', 'numpy': '1.23.2', 'pandas': '1.5.2', 'jinja2': '3.1.2', 'scipy': '1.9.3', 'joblib': '1.2.0', 'sklearn': '1.1.3', 'pyod': '1.0.9', 'imblearn': '0.10.1', 'category_encoders': '2.6.1', 'lightgbm': '3.3.5', 'numba': '0.57.0', 'requests': '2.28.2', 'matplotlib': '3.5.3', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.13.1', 'kaleido': '0.2.1', 'statsmodels': '0.13.5', 'sktime': '0.17.0', 'tbats': '1.1.3', 'pmdarima': '2.0.3', 'psutil': '5.9.2'}, 'python': {'version': '3.10.3', 'machine': 'AMD64'}}
  warnings.warn(

2023-05-20 23:57:26,013:INFO:Calculating mean and std
2023-05-20 23:57:26,016:INFO:Creating metrics dataframe
2023-05-20 23:57:26,541:INFO:Uploading results into container
2023-05-20 23:57:26,542:INFO:Uploading model into container now
2023-05-20 23:57:26,543:INFO:_master_model_container: 20
2023-05-20 23:57:26,543:INFO:_display_container: 2
2023-05-20 23:57:26,543:INFO:DummyRegressor()
2023-05-20 23:57:26,543:INFO:create_model() successfully completed......................................
2023-05-20 23:57:26,813:INFO:SubProcess create_model() end ==================================
2023-05-20 23:57:26,813:INFO:Creating metrics dataframe
2023-05-20 23:57:26,837:INFO:Initializing create_model()
2023-05-20 23:57:26,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=<catboost.core.CatBoostRegressor object at 0x00000258B388A4A0>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:26,837:INFO:Checking exceptions
2023-05-20 23:57:26,839:INFO:Importing libraries
2023-05-20 23:57:26,840:INFO:Copying training dataset
2023-05-20 23:57:26,852:INFO:Defining folds
2023-05-20 23:57:26,852:INFO:Declaring metric variables
2023-05-20 23:57:26,852:INFO:Importing untrained model
2023-05-20 23:57:26,852:INFO:Declaring custom model
2023-05-20 23:57:26,853:INFO:CatBoost Regressor Imported successfully
2023-05-20 23:57:26,854:INFO:Cross validation set to False
2023-05-20 23:57:26,855:INFO:Fitting Model
2023-05-20 23:57:31,489:INFO:<catboost.core.CatBoostRegressor object at 0x00000258B38889A0>
2023-05-20 23:57:31,489:INFO:create_model() successfully completed......................................
2023-05-20 23:57:31,713:INFO:Initializing create_model()
2023-05-20 23:57:31,713:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=GradientBoostingRegressor(random_state=3832), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:31,713:INFO:Checking exceptions
2023-05-20 23:57:31,716:INFO:Importing libraries
2023-05-20 23:57:31,716:INFO:Copying training dataset
2023-05-20 23:57:31,730:INFO:Defining folds
2023-05-20 23:57:31,730:INFO:Declaring metric variables
2023-05-20 23:57:31,730:INFO:Importing untrained model
2023-05-20 23:57:31,730:INFO:Declaring custom model
2023-05-20 23:57:31,731:INFO:Gradient Boosting Regressor Imported successfully
2023-05-20 23:57:31,732:INFO:Cross validation set to False
2023-05-20 23:57:31,732:INFO:Fitting Model
2023-05-20 23:57:33,049:INFO:GradientBoostingRegressor(random_state=3832)
2023-05-20 23:57:33,049:INFO:create_model() successfully completed......................................
2023-05-20 23:57:33,308:INFO:Initializing create_model()
2023-05-20 23:57:33,309:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=LGBMRegressor(random_state=3832), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:33,309:INFO:Checking exceptions
2023-05-20 23:57:33,312:INFO:Importing libraries
2023-05-20 23:57:33,312:INFO:Copying training dataset
2023-05-20 23:57:33,326:INFO:Defining folds
2023-05-20 23:57:33,326:INFO:Declaring metric variables
2023-05-20 23:57:33,327:INFO:Importing untrained model
2023-05-20 23:57:33,327:INFO:Declaring custom model
2023-05-20 23:57:33,328:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-20 23:57:33,330:INFO:Cross validation set to False
2023-05-20 23:57:33,330:INFO:Fitting Model
2023-05-20 23:57:34,141:INFO:LGBMRegressor(random_state=3832)
2023-05-20 23:57:34,141:INFO:create_model() successfully completed......................................
2023-05-20 23:57:34,348:INFO:Initializing create_model()
2023-05-20 23:57:34,348:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:34,348:INFO:Checking exceptions
2023-05-20 23:57:34,350:INFO:Importing libraries
2023-05-20 23:57:34,350:INFO:Copying training dataset
2023-05-20 23:57:34,367:INFO:Defining folds
2023-05-20 23:57:34,367:INFO:Declaring metric variables
2023-05-20 23:57:34,368:INFO:Importing untrained model
2023-05-20 23:57:34,368:INFO:Declaring custom model
2023-05-20 23:57:34,368:INFO:Bayesian Ridge Imported successfully
2023-05-20 23:57:34,369:INFO:Cross validation set to False
2023-05-20 23:57:34,370:INFO:Fitting Model
2023-05-20 23:57:35,066:INFO:BayesianRidge()
2023-05-20 23:57:35,066:INFO:create_model() successfully completed......................................
2023-05-20 23:57:35,325:INFO:Initializing create_model()
2023-05-20 23:57:35,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000258BC27AAA0>, estimator=OrthogonalMatchingPursuit(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-20 23:57:35,325:INFO:Checking exceptions
2023-05-20 23:57:35,328:INFO:Importing libraries
2023-05-20 23:57:35,328:INFO:Copying training dataset
2023-05-20 23:57:35,341:INFO:Defining folds
2023-05-20 23:57:35,341:INFO:Declaring metric variables
2023-05-20 23:57:35,342:INFO:Importing untrained model
2023-05-20 23:57:35,342:INFO:Declaring custom model
2023-05-20 23:57:35,342:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-20 23:57:35,344:INFO:Cross validation set to False
2023-05-20 23:57:35,344:INFO:Fitting Model
2023-05-20 23:57:35,401:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning:

The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)




2023-05-20 23:57:35,851:INFO:OrthogonalMatchingPursuit()
2023-05-20 23:57:35,851:INFO:create_model() successfully completed......................................
2023-05-20 23:57:36,098:INFO:_master_model_container: 20
2023-05-20 23:57:36,098:INFO:_display_container: 2
2023-05-20 23:57:36,099:INFO:[<catboost.core.CatBoostRegressor object at 0x00000258B38889A0>, GradientBoostingRegressor(random_state=3832), LGBMRegressor(random_state=3832), BayesianRidge(), OrthogonalMatchingPursuit()]
2023-05-20 23:57:36,099:INFO:compare_models() successfully completed......................................
2023-05-21 01:01:46,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:01:46,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:01:46,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:01:46,793:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:01:47,940:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-21 01:02:16,659:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_47600\2221667683.py:4: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-21 01:02:16,791:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_47600\2221667683.py:9: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-21 01:02:18,539:INFO:PyCaret RegressionExperiment
2023-05-21 01:02:18,539:INFO:Logging name: reg-default-name
2023-05-21 01:02:18,539:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-21 01:02:18,539:INFO:version 3.0.2
2023-05-21 01:02:18,539:INFO:Initializing setup()
2023-05-21 01:02:18,539:INFO:self.USI: ed9b
2023-05-21 01:02:18,539:INFO:self._variable_keys: {'USI', 'fold_groups_param', 'target_param', 'exp_id', 'seed', 'memory', 'fold_shuffle_param', 'gpu_param', 'y_train', 'fold_generator', 'n_jobs_param', 'transform_target_param', 'idx', 'exp_name_log', 'y', 'data', 'X_test', '_ml_usecase', 'gpu_n_jobs_param', '_available_plots', 'X', 'log_plots_param', 'logging_param', 'pipeline', 'X_train', 'y_test', 'html_param'}
2023-05-21 01:02:18,540:INFO:Checking environment
2023-05-21 01:02:18,540:INFO:python_version: 3.10.3
2023-05-21 01:02:18,540:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-21 01:02:18,540:INFO:machine: AMD64
2023-05-21 01:02:18,540:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-21 01:02:18,541:INFO:Memory: svmem(total=17083187200, available=6339579904, percent=62.9, used=10743607296, free=6339579904)
2023-05-21 01:02:18,541:INFO:Physical Core: 6
2023-05-21 01:02:18,541:INFO:Logical Core: 12
2023-05-21 01:02:18,542:INFO:Checking libraries
2023-05-21 01:02:18,542:INFO:System:
2023-05-21 01:02:18,542:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-21 01:02:18,542:INFO:executable: c:\Python310\python.exe
2023-05-21 01:02:18,542:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-21 01:02:18,542:INFO:PyCaret required dependencies:
2023-05-21 01:02:18,542:INFO:                 pip: 23.1.2
2023-05-21 01:02:18,543:INFO:          setuptools: 58.1.0
2023-05-21 01:02:18,543:INFO:             pycaret: 3.0.2
2023-05-21 01:02:18,543:INFO:             IPython: 8.5.0
2023-05-21 01:02:18,543:INFO:          ipywidgets: 8.0.6
2023-05-21 01:02:18,543:INFO:                tqdm: 4.64.1
2023-05-21 01:02:18,543:INFO:               numpy: 1.23.0
2023-05-21 01:02:18,543:INFO:              pandas: 1.5.2
2023-05-21 01:02:18,543:INFO:              jinja2: 3.1.2
2023-05-21 01:02:18,543:INFO:               scipy: 1.9.3
2023-05-21 01:02:18,543:INFO:              joblib: 1.2.0
2023-05-21 01:02:18,543:INFO:             sklearn: 1.1.3
2023-05-21 01:02:18,543:INFO:                pyod: 1.0.9
2023-05-21 01:02:18,543:INFO:            imblearn: 0.10.1
2023-05-21 01:02:18,544:INFO:   category_encoders: 2.6.1
2023-05-21 01:02:18,544:INFO:            lightgbm: 3.3.5
2023-05-21 01:02:18,544:INFO:               numba: 0.57.0
2023-05-21 01:02:18,544:INFO:            requests: 2.28.2
2023-05-21 01:02:18,544:INFO:          matplotlib: 3.5.3
2023-05-21 01:02:18,544:INFO:          scikitplot: 0.3.7
2023-05-21 01:02:18,544:INFO:         yellowbrick: 1.5
2023-05-21 01:02:18,544:INFO:              plotly: 5.13.1
2023-05-21 01:02:18,544:INFO:             kaleido: 0.2.1
2023-05-21 01:02:18,544:INFO:         statsmodels: 0.13.5
2023-05-21 01:02:18,544:INFO:              sktime: 0.17.0
2023-05-21 01:02:18,544:INFO:               tbats: 1.1.3
2023-05-21 01:02:18,544:INFO:            pmdarima: 2.0.3
2023-05-21 01:02:18,545:INFO:              psutil: 5.9.2
2023-05-21 01:02:18,545:INFO:PyCaret optional dependencies:
2023-05-21 01:02:18,568:INFO:                shap: Not installed
2023-05-21 01:02:18,568:INFO:           interpret: 0.4.1
2023-05-21 01:02:18,568:INFO:                umap: Not installed
2023-05-21 01:02:18,568:INFO:    pandas_profiling: Not installed
2023-05-21 01:02:18,568:INFO:  explainerdashboard: Not installed
2023-05-21 01:02:18,568:INFO:             autoviz: Not installed
2023-05-21 01:02:18,569:INFO:           fairlearn: Not installed
2023-05-21 01:02:18,569:INFO:             xgboost: 1.7.4
2023-05-21 01:02:18,569:INFO:            catboost: 1.1.1
2023-05-21 01:02:18,569:INFO:              kmodes: Not installed
2023-05-21 01:02:18,569:INFO:             mlxtend: Not installed
2023-05-21 01:02:18,569:INFO:       statsforecast: Not installed
2023-05-21 01:02:18,569:INFO:        tune_sklearn: Not installed
2023-05-21 01:02:18,569:INFO:                 ray: Not installed
2023-05-21 01:02:18,569:INFO:            hyperopt: Not installed
2023-05-21 01:02:18,569:INFO:              optuna: Not installed
2023-05-21 01:02:18,569:INFO:               skopt: Not installed
2023-05-21 01:02:18,569:INFO:              mlflow: Not installed
2023-05-21 01:02:18,569:INFO:              gradio: Not installed
2023-05-21 01:02:18,569:INFO:             fastapi: Not installed
2023-05-21 01:02:18,569:INFO:             uvicorn: Not installed
2023-05-21 01:02:18,569:INFO:              m2cgen: Not installed
2023-05-21 01:02:18,569:INFO:           evidently: Not installed
2023-05-21 01:02:18,569:INFO:               fugue: Not installed
2023-05-21 01:02:18,569:INFO:           streamlit: Not installed
2023-05-21 01:02:18,569:INFO:             prophet: Not installed
2023-05-21 01:02:18,570:INFO:None
2023-05-21 01:02:18,570:INFO:Set up data.
2023-05-21 01:02:18,753:INFO:Set up train/test split.
2023-05-21 01:02:18,769:INFO:Set up index.
2023-05-21 01:02:18,769:INFO:Set up folding strategy.
2023-05-21 01:02:18,770:INFO:Assigning column types.
2023-05-21 01:02:18,788:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-21 01:02:18,789:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-21 01:02:18,795:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:02:18,801:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:18,877:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:18,933:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:18,935:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,032:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,033:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,039:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,044:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,112:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,160:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,161:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,164:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,165:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-21 01:02:19,170:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,175:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,242:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,289:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,290:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,293:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,299:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,303:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,419:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,419:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,422:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,422:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-21 01:02:19,433:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,503:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,552:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,553:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,556:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,566:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,635:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,682:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,683:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,686:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,687:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-21 01:02:19,765:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,818:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,819:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,822:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,899:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,946:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:02:19,947:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:19,951:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:19,952:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-21 01:02:20,030:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:20,077:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:20,081:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:20,158:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:02:20,205:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:20,208:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:20,209:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-21 01:02:20,334:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:20,337:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:20,461:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:20,464:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:20,468:INFO:Preparing preprocessing pipeline...
2023-05-21 01:02:20,468:INFO:Set up simple imputation.
2023-05-21 01:02:20,470:INFO:Set up column name cleaning.
2023-05-21 01:02:20,586:INFO:Finished creating preprocessing pipeline.
2023-05-21 01:02:20,609:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-21 01:02:20,609:INFO:Creating final display dataframe.
2023-05-21 01:02:21,075:INFO:Setup _display_container:                     Description             Value
0                    Session id              6431
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 319)
4        Transformed data shape       (1460, 319)
5   Transformed train set shape       (1021, 319)
6    Transformed test set shape        (439, 319)
7              Numeric features               318
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              ed9b
2023-05-21 01:02:21,255:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:21,259:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:21,386:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:02:21,390:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:02:21,391:INFO:setup() successfully completed in 3.61s...............
2023-05-21 01:02:21,425:INFO:Initializing compare_models()
2023-05-21 01:02:21,425:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002AE56396E90>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002AE56396E90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-21 01:02:21,425:INFO:Checking exceptions
2023-05-21 01:02:21,432:INFO:Preparing display monitor
2023-05-21 01:02:21,530:INFO:Initializing Linear Regression
2023-05-21 01:02:21,531:INFO:Total runtime is 1.6641616821289062e-05 minutes
2023-05-21 01:02:21,539:INFO:SubProcess create_model() called ==================================
2023-05-21 01:02:21,539:INFO:Initializing create_model()
2023-05-21 01:02:21,539:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002AE56396E90>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AE563FFEB0>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:02:21,539:INFO:Checking exceptions
2023-05-21 01:02:21,540:INFO:Importing libraries
2023-05-21 01:02:21,540:INFO:Copying training dataset
2023-05-21 01:02:21,557:INFO:Defining folds
2023-05-21 01:02:21,557:INFO:Declaring metric variables
2023-05-21 01:02:21,563:INFO:Importing untrained model
2023-05-21 01:02:21,570:INFO:Linear Regression Imported successfully
2023-05-21 01:02:21,584:INFO:Starting cross validation
2023-05-21 01:02:21,600:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:02:40,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:02:40,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:02:40,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:02:40,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-21 01:02:41,434:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-21 01:03:04,373:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29908\2221667683.py:4: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-21 01:03:04,456:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_29908\2221667683.py:9: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-21 01:03:05,640:INFO:PyCaret RegressionExperiment
2023-05-21 01:03:05,640:INFO:Logging name: reg-default-name
2023-05-21 01:03:05,640:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-21 01:03:05,640:INFO:version 3.0.2
2023-05-21 01:03:05,640:INFO:Initializing setup()
2023-05-21 01:03:05,640:INFO:self.USI: ca3f
2023-05-21 01:03:05,640:INFO:self._variable_keys: {'seed', 'y_train', 'log_plots_param', 'logging_param', '_available_plots', 'n_jobs_param', 'y', 'y_test', 'gpu_n_jobs_param', 'target_param', 'X', 'fold_generator', 'exp_id', 'idx', 'html_param', 'fold_groups_param', 'data', '_ml_usecase', 'fold_shuffle_param', 'memory', 'X_test', 'gpu_param', 'USI', 'pipeline', 'exp_name_log', 'transform_target_param', 'X_train'}
2023-05-21 01:03:05,641:INFO:Checking environment
2023-05-21 01:03:05,641:INFO:python_version: 3.10.3
2023-05-21 01:03:05,641:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-21 01:03:05,641:INFO:machine: AMD64
2023-05-21 01:03:05,641:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-21 01:03:05,641:INFO:Memory: svmem(total=17083187200, available=5911437312, percent=65.4, used=11171749888, free=5911437312)
2023-05-21 01:03:05,641:INFO:Physical Core: 6
2023-05-21 01:03:05,641:INFO:Logical Core: 12
2023-05-21 01:03:05,641:INFO:Checking libraries
2023-05-21 01:03:05,641:INFO:System:
2023-05-21 01:03:05,641:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-21 01:03:05,641:INFO:executable: c:\Python310\python.exe
2023-05-21 01:03:05,641:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-21 01:03:05,641:INFO:PyCaret required dependencies:
2023-05-21 01:03:05,641:INFO:                 pip: 23.1.2
2023-05-21 01:03:05,641:INFO:          setuptools: 58.1.0
2023-05-21 01:03:05,641:INFO:             pycaret: 3.0.2
2023-05-21 01:03:05,641:INFO:             IPython: 8.5.0
2023-05-21 01:03:05,641:INFO:          ipywidgets: 8.0.6
2023-05-21 01:03:05,641:INFO:                tqdm: 4.64.1
2023-05-21 01:03:05,642:INFO:               numpy: 1.23.0
2023-05-21 01:03:05,642:INFO:              pandas: 1.5.2
2023-05-21 01:03:05,642:INFO:              jinja2: 3.1.2
2023-05-21 01:03:05,642:INFO:               scipy: 1.9.3
2023-05-21 01:03:05,642:INFO:              joblib: 1.2.0
2023-05-21 01:03:05,642:INFO:             sklearn: 1.1.3
2023-05-21 01:03:05,642:INFO:                pyod: 1.0.9
2023-05-21 01:03:05,642:INFO:            imblearn: 0.10.1
2023-05-21 01:03:05,642:INFO:   category_encoders: 2.6.1
2023-05-21 01:03:05,642:INFO:            lightgbm: 3.3.5
2023-05-21 01:03:05,642:INFO:               numba: 0.57.0
2023-05-21 01:03:05,642:INFO:            requests: 2.28.2
2023-05-21 01:03:05,642:INFO:          matplotlib: 3.5.3
2023-05-21 01:03:05,642:INFO:          scikitplot: 0.3.7
2023-05-21 01:03:05,642:INFO:         yellowbrick: 1.5
2023-05-21 01:03:05,642:INFO:              plotly: 5.13.1
2023-05-21 01:03:05,642:INFO:             kaleido: 0.2.1
2023-05-21 01:03:05,642:INFO:         statsmodels: 0.13.5
2023-05-21 01:03:05,642:INFO:              sktime: 0.17.0
2023-05-21 01:03:05,642:INFO:               tbats: 1.1.3
2023-05-21 01:03:05,642:INFO:            pmdarima: 2.0.3
2023-05-21 01:03:05,642:INFO:              psutil: 5.9.2
2023-05-21 01:03:05,642:INFO:PyCaret optional dependencies:
2023-05-21 01:03:05,656:INFO:                shap: Not installed
2023-05-21 01:03:05,656:INFO:           interpret: 0.4.1
2023-05-21 01:03:05,656:INFO:                umap: Not installed
2023-05-21 01:03:05,656:INFO:    pandas_profiling: Not installed
2023-05-21 01:03:05,656:INFO:  explainerdashboard: Not installed
2023-05-21 01:03:05,656:INFO:             autoviz: Not installed
2023-05-21 01:03:05,656:INFO:           fairlearn: Not installed
2023-05-21 01:03:05,656:INFO:             xgboost: 1.7.4
2023-05-21 01:03:05,656:INFO:            catboost: 1.1.1
2023-05-21 01:03:05,657:INFO:              kmodes: Not installed
2023-05-21 01:03:05,657:INFO:             mlxtend: Not installed
2023-05-21 01:03:05,657:INFO:       statsforecast: Not installed
2023-05-21 01:03:05,657:INFO:        tune_sklearn: Not installed
2023-05-21 01:03:05,657:INFO:                 ray: Not installed
2023-05-21 01:03:05,657:INFO:            hyperopt: Not installed
2023-05-21 01:03:05,657:INFO:              optuna: Not installed
2023-05-21 01:03:05,657:INFO:               skopt: Not installed
2023-05-21 01:03:05,657:INFO:              mlflow: Not installed
2023-05-21 01:03:05,657:INFO:              gradio: Not installed
2023-05-21 01:03:05,657:INFO:             fastapi: Not installed
2023-05-21 01:03:05,657:INFO:             uvicorn: Not installed
2023-05-21 01:03:05,657:INFO:              m2cgen: Not installed
2023-05-21 01:03:05,657:INFO:           evidently: Not installed
2023-05-21 01:03:05,657:INFO:               fugue: Not installed
2023-05-21 01:03:05,657:INFO:           streamlit: Not installed
2023-05-21 01:03:05,657:INFO:             prophet: Not installed
2023-05-21 01:03:05,657:INFO:None
2023-05-21 01:03:05,657:INFO:Set up data.
2023-05-21 01:03:05,741:INFO:Set up train/test split.
2023-05-21 01:03:05,757:INFO:Set up index.
2023-05-21 01:03:05,757:INFO:Set up folding strategy.
2023-05-21 01:03:05,757:INFO:Assigning column types.
2023-05-21 01:03:05,766:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-21 01:03:05,767:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,770:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,775:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,836:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,878:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,879:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:05,921:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:05,922:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,926:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,931:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:05,988:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,030:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,031:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,034:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,034:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-21 01:03:06,038:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,043:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,103:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,144:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,144:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,147:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,153:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,157:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,223:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,267:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,268:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,269:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,270:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-21 01:03:06,279:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,337:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,379:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,389:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,399:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,411:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,480:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,523:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,635:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,637:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,639:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-21 01:03:06,709:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,756:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,757:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,760:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,831:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,875:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-21 01:03:06,876:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:06,878:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:06,879:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-21 01:03:06,973:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:07,026:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,030:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,101:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-21 01:03:07,146:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,150:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,150:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-21 01:03:07,261:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,264:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,374:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,376:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,379:INFO:Preparing preprocessing pipeline...
2023-05-21 01:03:07,379:INFO:Set up simple imputation.
2023-05-21 01:03:07,380:INFO:Set up column name cleaning.
2023-05-21 01:03:07,424:INFO:Finished creating preprocessing pipeline.
2023-05-21 01:03:07,432:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-21 01:03:07,432:INFO:Creating final display dataframe.
2023-05-21 01:03:07,612:INFO:Setup _display_container:                     Description             Value
0                    Session id              5041
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 319)
4        Transformed data shape       (1460, 319)
5   Transformed train set shape       (1021, 319)
6    Transformed test set shape        (439, 319)
7              Numeric features               318
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              ca3f
2023-05-21 01:03:07,746:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,749:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,865:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-21 01:03:07,867:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-21 01:03:07,869:INFO:setup() successfully completed in 2.72s...............
2023-05-21 01:03:07,902:INFO:Initializing compare_models()
2023-05-21 01:03:07,903:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-21 01:03:07,903:INFO:Checking exceptions
2023-05-21 01:03:07,910:INFO:Preparing display monitor
2023-05-21 01:03:07,977:INFO:Initializing Linear Regression
2023-05-21 01:03:07,977:INFO:Total runtime is 0.0 minutes
2023-05-21 01:03:07,979:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:07,979:INFO:Initializing create_model()
2023-05-21 01:03:07,979:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:07,980:INFO:Checking exceptions
2023-05-21 01:03:07,980:INFO:Importing libraries
2023-05-21 01:03:07,980:INFO:Copying training dataset
2023-05-21 01:03:07,992:INFO:Defining folds
2023-05-21 01:03:07,992:INFO:Declaring metric variables
2023-05-21 01:03:07,995:INFO:Importing untrained model
2023-05-21 01:03:07,999:INFO:Linear Regression Imported successfully
2023-05-21 01:03:08,005:INFO:Starting cross validation
2023-05-21 01:03:08,011:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:17,116:INFO:Calculating mean and std
2023-05-21 01:03:17,118:INFO:Creating metrics dataframe
2023-05-21 01:03:17,626:INFO:Uploading results into container
2023-05-21 01:03:17,627:INFO:Uploading model into container now
2023-05-21 01:03:17,627:INFO:_master_model_container: 1
2023-05-21 01:03:17,627:INFO:_display_container: 2
2023-05-21 01:03:17,628:INFO:LinearRegression(n_jobs=-1)
2023-05-21 01:03:17,628:INFO:create_model() successfully completed......................................
2023-05-21 01:03:17,709:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:17,709:INFO:Creating metrics dataframe
2023-05-21 01:03:17,719:INFO:Initializing Lasso Regression
2023-05-21 01:03:17,719:INFO:Total runtime is 0.16237558523813883 minutes
2023-05-21 01:03:17,722:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:17,723:INFO:Initializing create_model()
2023-05-21 01:03:17,723:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:17,723:INFO:Checking exceptions
2023-05-21 01:03:17,724:INFO:Importing libraries
2023-05-21 01:03:17,724:INFO:Copying training dataset
2023-05-21 01:03:17,734:INFO:Defining folds
2023-05-21 01:03:17,735:INFO:Declaring metric variables
2023-05-21 01:03:17,739:INFO:Importing untrained model
2023-05-21 01:03:17,742:INFO:Lasso Regression Imported successfully
2023-05-21 01:03:17,748:INFO:Starting cross validation
2023-05-21 01:03:17,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:22,381:INFO:Calculating mean and std
2023-05-21 01:03:22,383:INFO:Creating metrics dataframe
2023-05-21 01:03:22,872:INFO:Uploading results into container
2023-05-21 01:03:22,873:INFO:Uploading model into container now
2023-05-21 01:03:22,874:INFO:_master_model_container: 2
2023-05-21 01:03:22,874:INFO:_display_container: 2
2023-05-21 01:03:22,874:INFO:Lasso(random_state=5041)
2023-05-21 01:03:22,874:INFO:create_model() successfully completed......................................
2023-05-21 01:03:22,946:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:22,946:INFO:Creating metrics dataframe
2023-05-21 01:03:22,954:INFO:Initializing Ridge Regression
2023-05-21 01:03:22,955:INFO:Total runtime is 0.24963852961858113 minutes
2023-05-21 01:03:22,957:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:22,959:INFO:Initializing create_model()
2023-05-21 01:03:22,959:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:22,959:INFO:Checking exceptions
2023-05-21 01:03:22,959:INFO:Importing libraries
2023-05-21 01:03:22,959:INFO:Copying training dataset
2023-05-21 01:03:22,968:INFO:Defining folds
2023-05-21 01:03:22,968:INFO:Declaring metric variables
2023-05-21 01:03:22,970:INFO:Importing untrained model
2023-05-21 01:03:22,973:INFO:Ridge Regression Imported successfully
2023-05-21 01:03:22,979:INFO:Starting cross validation
2023-05-21 01:03:22,981:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:27,360:INFO:Calculating mean and std
2023-05-21 01:03:27,362:INFO:Creating metrics dataframe
2023-05-21 01:03:27,848:INFO:Uploading results into container
2023-05-21 01:03:27,849:INFO:Uploading model into container now
2023-05-21 01:03:27,850:INFO:_master_model_container: 3
2023-05-21 01:03:27,850:INFO:_display_container: 2
2023-05-21 01:03:27,850:INFO:Ridge(random_state=5041)
2023-05-21 01:03:27,850:INFO:create_model() successfully completed......................................
2023-05-21 01:03:27,922:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:27,922:INFO:Creating metrics dataframe
2023-05-21 01:03:27,931:INFO:Initializing Elastic Net
2023-05-21 01:03:27,931:INFO:Total runtime is 0.3325758457183838 minutes
2023-05-21 01:03:27,934:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:27,934:INFO:Initializing create_model()
2023-05-21 01:03:27,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:27,935:INFO:Checking exceptions
2023-05-21 01:03:27,935:INFO:Importing libraries
2023-05-21 01:03:27,935:INFO:Copying training dataset
2023-05-21 01:03:27,944:INFO:Defining folds
2023-05-21 01:03:27,944:INFO:Declaring metric variables
2023-05-21 01:03:27,947:INFO:Importing untrained model
2023-05-21 01:03:27,952:INFO:Elastic Net Imported successfully
2023-05-21 01:03:27,960:INFO:Starting cross validation
2023-05-21 01:03:27,962:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:32,281:INFO:Calculating mean and std
2023-05-21 01:03:32,282:INFO:Creating metrics dataframe
2023-05-21 01:03:32,769:INFO:Uploading results into container
2023-05-21 01:03:32,769:INFO:Uploading model into container now
2023-05-21 01:03:32,769:INFO:_master_model_container: 4
2023-05-21 01:03:32,769:INFO:_display_container: 2
2023-05-21 01:03:32,771:INFO:ElasticNet(random_state=5041)
2023-05-21 01:03:32,771:INFO:create_model() successfully completed......................................
2023-05-21 01:03:32,844:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:32,844:INFO:Creating metrics dataframe
2023-05-21 01:03:32,852:INFO:Initializing Least Angle Regression
2023-05-21 01:03:32,853:INFO:Total runtime is 0.41460434993108114 minutes
2023-05-21 01:03:32,855:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:32,856:INFO:Initializing create_model()
2023-05-21 01:03:32,856:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:32,856:INFO:Checking exceptions
2023-05-21 01:03:32,856:INFO:Importing libraries
2023-05-21 01:03:32,856:INFO:Copying training dataset
2023-05-21 01:03:32,864:INFO:Defining folds
2023-05-21 01:03:32,864:INFO:Declaring metric variables
2023-05-21 01:03:32,867:INFO:Importing untrained model
2023-05-21 01:03:32,870:INFO:Least Angle Regression Imported successfully
2023-05-21 01:03:32,877:INFO:Starting cross validation
2023-05-21 01:03:32,878:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:32,996:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,001:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,002:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,026:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,027:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=9.412e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,027:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=4.504e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,027:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,034:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=4.322e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,037:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,039:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=4.006e-04, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,044:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=3.812e-04, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,046:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.310e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,046:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=4.680e-04, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,048:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=4.597e-04, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,053:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,057:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.488e-04, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,059:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=9.762e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,059:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,064:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 148 iterations, i.e. alpha=4.709e-04, with an active set of 134 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=5.458e-04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,069:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,070:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.042e-03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,071:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=5.033e-04, with an active set of 62 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,076:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=4.577e-04, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,076:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.175e-04, with an active set of 124 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,076:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.163e-04, with an active set of 125 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,077:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:33,082:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=9.032e-05, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,082:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.454e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,083:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 200 iterations, i.e. alpha=4.647e-04, with an active set of 180 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,083:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.036e-04, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,091:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.823e-04, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,092:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=9.640e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,094:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=7.022e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,096:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 234 iterations, i.e. alpha=4.991e-04, with an active set of 207 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,098:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 246 iterations, i.e. alpha=3.478e-03, with an active set of 198 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,101:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=1.204e-04, with an active set of 126 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,107:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.892e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,111:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.736e-04, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,115:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 284 iterations, i.e. alpha=5.735e-04, with an active set of 247 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,120:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 211 iterations, i.e. alpha=3.128e+02, with an active set of 158 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,153:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 268 iterations, i.e. alpha=2.221e-03, with an active set of 217 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-21 01:03:33,211:WARNING:c:\Python310\lib\site-packages\numpy\core\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

2023-05-21 01:03:33,211:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:732: RuntimeWarning: overflow encountered in divide
  g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))

2023-05-21 01:03:33,211:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:736: RuntimeWarning: overflow encountered in divide
  g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))

2023-05-21 01:03:33,211:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:737: RuntimeWarning: divide by zero encountered in double_scalars
  gamma_ = min(g1, g2, C / AA)

2023-05-21 01:03:33,212:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:741: RuntimeWarning: overflow encountered in divide
  z = -coef[active] / (least_squares + tiny32)

2023-05-21 01:03:33,266:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-21 01:03:33,451:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,451:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,452:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,457:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,458:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-21 01:03:33,458:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,459:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-21 01:03:33,459:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,459:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,459:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,460:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,484:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,484:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,485:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,485:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,486:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,500:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,500:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,501:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,506:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-21 01:03:33,509:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-21 01:03:33,527:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,528:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,528:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:33,544:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,544:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-21 01:03:33,545:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-21 01:03:33,545:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-21 01:03:33,545:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-21 01:03:37,606:INFO:Calculating mean and std
2023-05-21 01:03:37,607:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-21 01:03:37,607:INFO:Creating metrics dataframe
2023-05-21 01:03:38,109:INFO:Uploading results into container
2023-05-21 01:03:38,109:INFO:Uploading model into container now
2023-05-21 01:03:38,109:INFO:_master_model_container: 5
2023-05-21 01:03:38,109:INFO:_display_container: 2
2023-05-21 01:03:38,109:INFO:Lars(random_state=5041)
2023-05-21 01:03:38,109:INFO:create_model() successfully completed......................................
2023-05-21 01:03:38,183:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:38,184:INFO:Creating metrics dataframe
2023-05-21 01:03:38,192:INFO:Initializing Lasso Least Angle Regression
2023-05-21 01:03:38,192:INFO:Total runtime is 0.5035923520723978 minutes
2023-05-21 01:03:38,195:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:38,195:INFO:Initializing create_model()
2023-05-21 01:03:38,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:38,196:INFO:Checking exceptions
2023-05-21 01:03:38,196:INFO:Importing libraries
2023-05-21 01:03:38,196:INFO:Copying training dataset
2023-05-21 01:03:38,204:INFO:Defining folds
2023-05-21 01:03:38,205:INFO:Declaring metric variables
2023-05-21 01:03:38,207:INFO:Importing untrained model
2023-05-21 01:03:38,211:INFO:Lasso Least Angle Regression Imported successfully
2023-05-21 01:03:38,219:INFO:Starting cross validation
2023-05-21 01:03:38,220:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:38,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,347:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,351:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,374:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,385:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,394:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,399:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,411:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:38,422:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-21 01:03:42,581:INFO:Calculating mean and std
2023-05-21 01:03:42,582:INFO:Creating metrics dataframe
2023-05-21 01:03:43,082:INFO:Uploading results into container
2023-05-21 01:03:43,082:INFO:Uploading model into container now
2023-05-21 01:03:43,083:INFO:_master_model_container: 6
2023-05-21 01:03:43,083:INFO:_display_container: 2
2023-05-21 01:03:43,083:INFO:LassoLars(random_state=5041)
2023-05-21 01:03:43,084:INFO:create_model() successfully completed......................................
2023-05-21 01:03:43,156:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:43,156:INFO:Creating metrics dataframe
2023-05-21 01:03:43,165:INFO:Initializing Orthogonal Matching Pursuit
2023-05-21 01:03:43,165:INFO:Total runtime is 0.5864661574363708 minutes
2023-05-21 01:03:43,168:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:43,168:INFO:Initializing create_model()
2023-05-21 01:03:43,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:43,168:INFO:Checking exceptions
2023-05-21 01:03:43,168:INFO:Importing libraries
2023-05-21 01:03:43,168:INFO:Copying training dataset
2023-05-21 01:03:43,176:INFO:Defining folds
2023-05-21 01:03:43,176:INFO:Declaring metric variables
2023-05-21 01:03:43,179:INFO:Importing untrained model
2023-05-21 01:03:43,181:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-21 01:03:43,188:INFO:Starting cross validation
2023-05-21 01:03:43,191:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:43,296:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,299:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,301:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,318:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,332:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,333:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,357:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,366:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,369:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:43,389:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-21 01:03:47,565:INFO:Calculating mean and std
2023-05-21 01:03:47,567:INFO:Creating metrics dataframe
2023-05-21 01:03:48,062:INFO:Uploading results into container
2023-05-21 01:03:48,064:INFO:Uploading model into container now
2023-05-21 01:03:48,064:INFO:_master_model_container: 7
2023-05-21 01:03:48,065:INFO:_display_container: 2
2023-05-21 01:03:48,065:INFO:OrthogonalMatchingPursuit()
2023-05-21 01:03:48,065:INFO:create_model() successfully completed......................................
2023-05-21 01:03:48,136:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:48,136:INFO:Creating metrics dataframe
2023-05-21 01:03:48,146:INFO:Initializing Bayesian Ridge
2023-05-21 01:03:48,146:INFO:Total runtime is 0.669480570157369 minutes
2023-05-21 01:03:48,149:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:48,150:INFO:Initializing create_model()
2023-05-21 01:03:48,150:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:48,150:INFO:Checking exceptions
2023-05-21 01:03:48,150:INFO:Importing libraries
2023-05-21 01:03:48,150:INFO:Copying training dataset
2023-05-21 01:03:48,160:INFO:Defining folds
2023-05-21 01:03:48,160:INFO:Declaring metric variables
2023-05-21 01:03:48,162:INFO:Importing untrained model
2023-05-21 01:03:48,166:INFO:Bayesian Ridge Imported successfully
2023-05-21 01:03:48,174:INFO:Starting cross validation
2023-05-21 01:03:48,176:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:52,682:INFO:Calculating mean and std
2023-05-21 01:03:52,684:INFO:Creating metrics dataframe
2023-05-21 01:03:53,233:INFO:Uploading results into container
2023-05-21 01:03:53,233:INFO:Uploading model into container now
2023-05-21 01:03:53,234:INFO:_master_model_container: 8
2023-05-21 01:03:53,234:INFO:_display_container: 2
2023-05-21 01:03:53,234:INFO:BayesianRidge()
2023-05-21 01:03:53,234:INFO:create_model() successfully completed......................................
2023-05-21 01:03:53,329:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:53,329:INFO:Creating metrics dataframe
2023-05-21 01:03:53,341:INFO:Initializing Passive Aggressive Regressor
2023-05-21 01:03:53,342:INFO:Total runtime is 0.7560840487480163 minutes
2023-05-21 01:03:53,345:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:53,345:INFO:Initializing create_model()
2023-05-21 01:03:53,345:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:53,345:INFO:Checking exceptions
2023-05-21 01:03:53,345:INFO:Importing libraries
2023-05-21 01:03:53,345:INFO:Copying training dataset
2023-05-21 01:03:53,359:INFO:Defining folds
2023-05-21 01:03:53,360:INFO:Declaring metric variables
2023-05-21 01:03:53,364:INFO:Importing untrained model
2023-05-21 01:03:53,369:INFO:Passive Aggressive Regressor Imported successfully
2023-05-21 01:03:53,376:INFO:Starting cross validation
2023-05-21 01:03:53,377:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:03:58,539:INFO:Calculating mean and std
2023-05-21 01:03:58,541:INFO:Creating metrics dataframe
2023-05-21 01:03:59,132:INFO:Uploading results into container
2023-05-21 01:03:59,134:INFO:Uploading model into container now
2023-05-21 01:03:59,134:INFO:_master_model_container: 9
2023-05-21 01:03:59,134:INFO:_display_container: 2
2023-05-21 01:03:59,134:INFO:PassiveAggressiveRegressor(random_state=5041)
2023-05-21 01:03:59,134:INFO:create_model() successfully completed......................................
2023-05-21 01:03:59,243:INFO:SubProcess create_model() end ==================================
2023-05-21 01:03:59,243:INFO:Creating metrics dataframe
2023-05-21 01:03:59,255:INFO:Initializing Huber Regressor
2023-05-21 01:03:59,255:INFO:Total runtime is 0.85464182694753 minutes
2023-05-21 01:03:59,260:INFO:SubProcess create_model() called ==================================
2023-05-21 01:03:59,260:INFO:Initializing create_model()
2023-05-21 01:03:59,260:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:03:59,261:INFO:Checking exceptions
2023-05-21 01:03:59,261:INFO:Importing libraries
2023-05-21 01:03:59,261:INFO:Copying training dataset
2023-05-21 01:03:59,274:INFO:Defining folds
2023-05-21 01:03:59,275:INFO:Declaring metric variables
2023-05-21 01:03:59,278:INFO:Importing untrained model
2023-05-21 01:03:59,285:INFO:Huber Regressor Imported successfully
2023-05-21 01:03:59,294:INFO:Starting cross validation
2023-05-21 01:03:59,296:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:01,581:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,095:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,126:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,158:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,262:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,303:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,404:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,502:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,635:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:02,889:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-21 01:04:07,308:INFO:Calculating mean and std
2023-05-21 01:04:07,311:INFO:Creating metrics dataframe
2023-05-21 01:04:07,840:INFO:Uploading results into container
2023-05-21 01:04:07,840:INFO:Uploading model into container now
2023-05-21 01:04:07,840:INFO:_master_model_container: 10
2023-05-21 01:04:07,840:INFO:_display_container: 2
2023-05-21 01:04:07,841:INFO:HuberRegressor()
2023-05-21 01:04:07,841:INFO:create_model() successfully completed......................................
2023-05-21 01:04:07,942:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:07,942:INFO:Creating metrics dataframe
2023-05-21 01:04:07,954:INFO:Initializing K Neighbors Regressor
2023-05-21 01:04:07,954:INFO:Total runtime is 0.9996191938718159 minutes
2023-05-21 01:04:07,958:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:07,959:INFO:Initializing create_model()
2023-05-21 01:04:07,959:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:07,959:INFO:Checking exceptions
2023-05-21 01:04:07,959:INFO:Importing libraries
2023-05-21 01:04:07,959:INFO:Copying training dataset
2023-05-21 01:04:07,976:INFO:Defining folds
2023-05-21 01:04:07,976:INFO:Declaring metric variables
2023-05-21 01:04:07,981:INFO:Importing untrained model
2023-05-21 01:04:07,986:INFO:K Neighbors Regressor Imported successfully
2023-05-21 01:04:07,993:INFO:Starting cross validation
2023-05-21 01:04:07,995:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:12,662:INFO:Calculating mean and std
2023-05-21 01:04:12,664:INFO:Creating metrics dataframe
2023-05-21 01:04:13,211:INFO:Uploading results into container
2023-05-21 01:04:13,212:INFO:Uploading model into container now
2023-05-21 01:04:13,212:INFO:_master_model_container: 11
2023-05-21 01:04:13,213:INFO:_display_container: 2
2023-05-21 01:04:13,213:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-21 01:04:13,213:INFO:create_model() successfully completed......................................
2023-05-21 01:04:13,299:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:13,299:INFO:Creating metrics dataframe
2023-05-21 01:04:13,309:INFO:Initializing Decision Tree Regressor
2023-05-21 01:04:13,309:INFO:Total runtime is 1.08887190024058 minutes
2023-05-21 01:04:13,312:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:13,312:INFO:Initializing create_model()
2023-05-21 01:04:13,312:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:13,312:INFO:Checking exceptions
2023-05-21 01:04:13,313:INFO:Importing libraries
2023-05-21 01:04:13,313:INFO:Copying training dataset
2023-05-21 01:04:13,328:INFO:Defining folds
2023-05-21 01:04:13,328:INFO:Declaring metric variables
2023-05-21 01:04:13,331:INFO:Importing untrained model
2023-05-21 01:04:13,335:INFO:Decision Tree Regressor Imported successfully
2023-05-21 01:04:13,342:INFO:Starting cross validation
2023-05-21 01:04:13,344:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:17,966:INFO:Calculating mean and std
2023-05-21 01:04:17,967:INFO:Creating metrics dataframe
2023-05-21 01:04:18,494:INFO:Uploading results into container
2023-05-21 01:04:18,495:INFO:Uploading model into container now
2023-05-21 01:04:18,495:INFO:_master_model_container: 12
2023-05-21 01:04:18,495:INFO:_display_container: 2
2023-05-21 01:04:18,496:INFO:DecisionTreeRegressor(random_state=5041)
2023-05-21 01:04:18,496:INFO:create_model() successfully completed......................................
2023-05-21 01:04:18,591:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:18,591:INFO:Creating metrics dataframe
2023-05-21 01:04:18,601:INFO:Initializing Random Forest Regressor
2023-05-21 01:04:18,601:INFO:Total runtime is 1.1770747621854143 minutes
2023-05-21 01:04:18,606:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:18,606:INFO:Initializing create_model()
2023-05-21 01:04:18,606:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:18,607:INFO:Checking exceptions
2023-05-21 01:04:18,607:INFO:Importing libraries
2023-05-21 01:04:18,607:INFO:Copying training dataset
2023-05-21 01:04:18,618:INFO:Defining folds
2023-05-21 01:04:18,619:INFO:Declaring metric variables
2023-05-21 01:04:18,623:INFO:Importing untrained model
2023-05-21 01:04:18,627:INFO:Random Forest Regressor Imported successfully
2023-05-21 01:04:18,633:INFO:Starting cross validation
2023-05-21 01:04:18,635:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:20,186:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:20,187:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:20,976:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.84s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:21,102:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:21,435:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:21,451:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:25,912:INFO:Calculating mean and std
2023-05-21 01:04:25,914:INFO:Creating metrics dataframe
2023-05-21 01:04:26,428:INFO:Uploading results into container
2023-05-21 01:04:26,429:INFO:Uploading model into container now
2023-05-21 01:04:26,429:INFO:_master_model_container: 13
2023-05-21 01:04:26,429:INFO:_display_container: 2
2023-05-21 01:04:26,430:INFO:RandomForestRegressor(n_jobs=-1, random_state=5041)
2023-05-21 01:04:26,430:INFO:create_model() successfully completed......................................
2023-05-21 01:04:26,516:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:26,517:INFO:Creating metrics dataframe
2023-05-21 01:04:26,528:INFO:Initializing Extra Trees Regressor
2023-05-21 01:04:26,528:INFO:Total runtime is 1.3091861089070636 minutes
2023-05-21 01:04:26,533:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:26,533:INFO:Initializing create_model()
2023-05-21 01:04:26,533:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:26,533:INFO:Checking exceptions
2023-05-21 01:04:26,534:INFO:Importing libraries
2023-05-21 01:04:26,534:INFO:Copying training dataset
2023-05-21 01:04:26,544:INFO:Defining folds
2023-05-21 01:04:26,544:INFO:Declaring metric variables
2023-05-21 01:04:26,547:INFO:Importing untrained model
2023-05-21 01:04:26,551:INFO:Extra Trees Regressor Imported successfully
2023-05-21 01:04:26,557:INFO:Starting cross validation
2023-05-21 01:04:26,559:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:29,028:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:29,436:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.45s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-21 01:04:34,269:INFO:Calculating mean and std
2023-05-21 01:04:34,270:INFO:Creating metrics dataframe
2023-05-21 01:04:34,795:INFO:Uploading results into container
2023-05-21 01:04:34,797:INFO:Uploading model into container now
2023-05-21 01:04:34,797:INFO:_master_model_container: 14
2023-05-21 01:04:34,797:INFO:_display_container: 2
2023-05-21 01:04:34,799:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=5041)
2023-05-21 01:04:34,799:INFO:create_model() successfully completed......................................
2023-05-21 01:04:34,894:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:34,894:INFO:Creating metrics dataframe
2023-05-21 01:04:34,906:INFO:Initializing AdaBoost Regressor
2023-05-21 01:04:34,906:INFO:Total runtime is 1.4488239288330076 minutes
2023-05-21 01:04:34,910:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:34,910:INFO:Initializing create_model()
2023-05-21 01:04:34,910:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:34,910:INFO:Checking exceptions
2023-05-21 01:04:34,911:INFO:Importing libraries
2023-05-21 01:04:34,911:INFO:Copying training dataset
2023-05-21 01:04:34,925:INFO:Defining folds
2023-05-21 01:04:34,925:INFO:Declaring metric variables
2023-05-21 01:04:34,928:INFO:Importing untrained model
2023-05-21 01:04:34,932:INFO:AdaBoost Regressor Imported successfully
2023-05-21 01:04:34,940:INFO:Starting cross validation
2023-05-21 01:04:34,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:41,415:INFO:Calculating mean and std
2023-05-21 01:04:41,417:INFO:Creating metrics dataframe
2023-05-21 01:04:41,943:INFO:Uploading results into container
2023-05-21 01:04:41,944:INFO:Uploading model into container now
2023-05-21 01:04:41,944:INFO:_master_model_container: 15
2023-05-21 01:04:41,944:INFO:_display_container: 2
2023-05-21 01:04:41,945:INFO:AdaBoostRegressor(random_state=5041)
2023-05-21 01:04:41,945:INFO:create_model() successfully completed......................................
2023-05-21 01:04:42,026:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:42,026:INFO:Creating metrics dataframe
2023-05-21 01:04:42,039:INFO:Initializing Gradient Boosting Regressor
2023-05-21 01:04:42,039:INFO:Total runtime is 1.5677116195360816 minutes
2023-05-21 01:04:42,042:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:42,042:INFO:Initializing create_model()
2023-05-21 01:04:42,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:42,042:INFO:Checking exceptions
2023-05-21 01:04:42,043:INFO:Importing libraries
2023-05-21 01:04:42,043:INFO:Copying training dataset
2023-05-21 01:04:42,053:INFO:Defining folds
2023-05-21 01:04:42,054:INFO:Declaring metric variables
2023-05-21 01:04:42,057:INFO:Importing untrained model
2023-05-21 01:04:42,060:INFO:Gradient Boosting Regressor Imported successfully
2023-05-21 01:04:42,066:INFO:Starting cross validation
2023-05-21 01:04:42,067:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:48,100:INFO:Calculating mean and std
2023-05-21 01:04:48,102:INFO:Creating metrics dataframe
2023-05-21 01:04:48,654:INFO:Uploading results into container
2023-05-21 01:04:48,655:INFO:Uploading model into container now
2023-05-21 01:04:48,656:INFO:_master_model_container: 16
2023-05-21 01:04:48,656:INFO:_display_container: 2
2023-05-21 01:04:48,657:INFO:GradientBoostingRegressor(random_state=5041)
2023-05-21 01:04:48,657:INFO:create_model() successfully completed......................................
2023-05-21 01:04:48,750:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:48,750:INFO:Creating metrics dataframe
2023-05-21 01:04:48,760:INFO:Initializing Extreme Gradient Boosting
2023-05-21 01:04:48,760:INFO:Total runtime is 1.679726159572601 minutes
2023-05-21 01:04:48,765:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:48,766:INFO:Initializing create_model()
2023-05-21 01:04:48,766:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:48,766:INFO:Checking exceptions
2023-05-21 01:04:48,766:INFO:Importing libraries
2023-05-21 01:04:48,766:INFO:Copying training dataset
2023-05-21 01:04:48,779:INFO:Defining folds
2023-05-21 01:04:48,779:INFO:Declaring metric variables
2023-05-21 01:04:48,785:INFO:Importing untrained model
2023-05-21 01:04:48,789:INFO:Extreme Gradient Boosting Imported successfully
2023-05-21 01:04:48,795:INFO:Starting cross validation
2023-05-21 01:04:48,797:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:04:55,591:INFO:Calculating mean and std
2023-05-21 01:04:55,593:INFO:Creating metrics dataframe
2023-05-21 01:04:56,140:INFO:Uploading results into container
2023-05-21 01:04:56,142:INFO:Uploading model into container now
2023-05-21 01:04:56,142:INFO:_master_model_container: 17
2023-05-21 01:04:56,142:INFO:_display_container: 2
2023-05-21 01:04:56,143:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=5041, ...)
2023-05-21 01:04:56,143:INFO:create_model() successfully completed......................................
2023-05-21 01:04:56,221:INFO:SubProcess create_model() end ==================================
2023-05-21 01:04:56,222:INFO:Creating metrics dataframe
2023-05-21 01:04:56,232:INFO:Initializing Light Gradient Boosting Machine
2023-05-21 01:04:56,233:INFO:Total runtime is 1.804274729887644 minutes
2023-05-21 01:04:56,235:INFO:SubProcess create_model() called ==================================
2023-05-21 01:04:56,236:INFO:Initializing create_model()
2023-05-21 01:04:56,236:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:04:56,236:INFO:Checking exceptions
2023-05-21 01:04:56,236:INFO:Importing libraries
2023-05-21 01:04:56,236:INFO:Copying training dataset
2023-05-21 01:04:56,247:INFO:Defining folds
2023-05-21 01:04:56,247:INFO:Declaring metric variables
2023-05-21 01:04:56,252:INFO:Importing untrained model
2023-05-21 01:04:56,255:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-21 01:04:56,261:INFO:Starting cross validation
2023-05-21 01:04:56,262:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:05:01,651:INFO:Calculating mean and std
2023-05-21 01:05:01,653:INFO:Creating metrics dataframe
2023-05-21 01:05:02,249:INFO:Uploading results into container
2023-05-21 01:05:02,250:INFO:Uploading model into container now
2023-05-21 01:05:02,250:INFO:_master_model_container: 18
2023-05-21 01:05:02,250:INFO:_display_container: 2
2023-05-21 01:05:02,251:INFO:LGBMRegressor(random_state=5041)
2023-05-21 01:05:02,251:INFO:create_model() successfully completed......................................
2023-05-21 01:05:02,340:INFO:SubProcess create_model() end ==================================
2023-05-21 01:05:02,340:INFO:Creating metrics dataframe
2023-05-21 01:05:02,353:INFO:Initializing CatBoost Regressor
2023-05-21 01:05:02,353:INFO:Total runtime is 1.906265830993652 minutes
2023-05-21 01:05:02,358:INFO:SubProcess create_model() called ==================================
2023-05-21 01:05:02,358:INFO:Initializing create_model()
2023-05-21 01:05:02,358:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:02,359:INFO:Checking exceptions
2023-05-21 01:05:02,359:INFO:Importing libraries
2023-05-21 01:05:02,359:INFO:Copying training dataset
2023-05-21 01:05:02,369:INFO:Defining folds
2023-05-21 01:05:02,370:INFO:Declaring metric variables
2023-05-21 01:05:02,374:INFO:Importing untrained model
2023-05-21 01:05:02,378:INFO:CatBoost Regressor Imported successfully
2023-05-21 01:05:02,384:INFO:Starting cross validation
2023-05-21 01:05:02,386:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:05:32,166:INFO:Calculating mean and std
2023-05-21 01:05:32,168:INFO:Creating metrics dataframe
2023-05-21 01:05:32,782:INFO:Uploading results into container
2023-05-21 01:05:32,783:INFO:Uploading model into container now
2023-05-21 01:05:32,784:INFO:_master_model_container: 19
2023-05-21 01:05:32,784:INFO:_display_container: 2
2023-05-21 01:05:32,784:INFO:<catboost.core.CatBoostRegressor object at 0x00000215DD82C310>
2023-05-21 01:05:32,784:INFO:create_model() successfully completed......................................
2023-05-21 01:05:32,924:INFO:SubProcess create_model() end ==================================
2023-05-21 01:05:32,924:INFO:Creating metrics dataframe
2023-05-21 01:05:32,938:INFO:Initializing Dummy Regressor
2023-05-21 01:05:32,938:INFO:Total runtime is 2.4160214424133297 minutes
2023-05-21 01:05:32,942:INFO:SubProcess create_model() called ==================================
2023-05-21 01:05:32,943:INFO:Initializing create_model()
2023-05-21 01:05:32,943:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000215F36B8280>, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:32,943:INFO:Checking exceptions
2023-05-21 01:05:32,943:INFO:Importing libraries
2023-05-21 01:05:32,943:INFO:Copying training dataset
2023-05-21 01:05:32,957:INFO:Defining folds
2023-05-21 01:05:32,957:INFO:Declaring metric variables
2023-05-21 01:05:32,960:INFO:Importing untrained model
2023-05-21 01:05:32,964:INFO:Dummy Regressor Imported successfully
2023-05-21 01:05:32,971:INFO:Starting cross validation
2023-05-21 01:05:32,973:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-21 01:05:38,080:INFO:Calculating mean and std
2023-05-21 01:05:38,082:INFO:Creating metrics dataframe
2023-05-21 01:05:38,632:INFO:Uploading results into container
2023-05-21 01:05:38,632:INFO:Uploading model into container now
2023-05-21 01:05:38,633:INFO:_master_model_container: 20
2023-05-21 01:05:38,633:INFO:_display_container: 2
2023-05-21 01:05:38,633:INFO:DummyRegressor()
2023-05-21 01:05:38,633:INFO:create_model() successfully completed......................................
2023-05-21 01:05:38,714:INFO:SubProcess create_model() end ==================================
2023-05-21 01:05:38,714:INFO:Creating metrics dataframe
2023-05-21 01:05:38,737:INFO:Initializing create_model()
2023-05-21 01:05:38,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=<catboost.core.CatBoostRegressor object at 0x00000215DD82C310>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:38,737:INFO:Checking exceptions
2023-05-21 01:05:38,741:INFO:Importing libraries
2023-05-21 01:05:38,741:INFO:Copying training dataset
2023-05-21 01:05:38,751:INFO:Defining folds
2023-05-21 01:05:38,751:INFO:Declaring metric variables
2023-05-21 01:05:38,751:INFO:Importing untrained model
2023-05-21 01:05:38,751:INFO:Declaring custom model
2023-05-21 01:05:38,752:INFO:CatBoost Regressor Imported successfully
2023-05-21 01:05:38,753:INFO:Cross validation set to False
2023-05-21 01:05:38,753:INFO:Fitting Model
2023-05-21 01:05:42,839:INFO:<catboost.core.CatBoostRegressor object at 0x00000215DD231D80>
2023-05-21 01:05:42,839:INFO:create_model() successfully completed......................................
2023-05-21 01:05:42,925:INFO:Initializing create_model()
2023-05-21 01:05:42,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=GradientBoostingRegressor(random_state=5041), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:42,925:INFO:Checking exceptions
2023-05-21 01:05:42,927:INFO:Importing libraries
2023-05-21 01:05:42,927:INFO:Copying training dataset
2023-05-21 01:05:42,936:INFO:Defining folds
2023-05-21 01:05:42,936:INFO:Declaring metric variables
2023-05-21 01:05:42,936:INFO:Importing untrained model
2023-05-21 01:05:42,936:INFO:Declaring custom model
2023-05-21 01:05:42,937:INFO:Gradient Boosting Regressor Imported successfully
2023-05-21 01:05:42,939:INFO:Cross validation set to False
2023-05-21 01:05:42,939:INFO:Fitting Model
2023-05-21 01:05:44,194:INFO:GradientBoostingRegressor(random_state=5041)
2023-05-21 01:05:44,194:INFO:create_model() successfully completed......................................
2023-05-21 01:05:44,281:INFO:Initializing create_model()
2023-05-21 01:05:44,281:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=OrthogonalMatchingPursuit(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:44,281:INFO:Checking exceptions
2023-05-21 01:05:44,283:INFO:Importing libraries
2023-05-21 01:05:44,283:INFO:Copying training dataset
2023-05-21 01:05:44,292:INFO:Defining folds
2023-05-21 01:05:44,293:INFO:Declaring metric variables
2023-05-21 01:05:44,293:INFO:Importing untrained model
2023-05-21 01:05:44,293:INFO:Declaring custom model
2023-05-21 01:05:44,293:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-21 01:05:44,295:INFO:Cross validation set to False
2023-05-21 01:05:44,295:INFO:Fitting Model
2023-05-21 01:05:44,339:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning:

The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)




2023-05-21 01:05:44,813:INFO:OrthogonalMatchingPursuit()
2023-05-21 01:05:44,814:INFO:create_model() successfully completed......................................
2023-05-21 01:05:44,902:INFO:Initializing create_model()
2023-05-21 01:05:44,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:44,903:INFO:Checking exceptions
2023-05-21 01:05:44,906:INFO:Importing libraries
2023-05-21 01:05:44,906:INFO:Copying training dataset
2023-05-21 01:05:44,914:INFO:Defining folds
2023-05-21 01:05:44,914:INFO:Declaring metric variables
2023-05-21 01:05:44,914:INFO:Importing untrained model
2023-05-21 01:05:44,914:INFO:Declaring custom model
2023-05-21 01:05:44,915:INFO:Bayesian Ridge Imported successfully
2023-05-21 01:05:44,916:INFO:Cross validation set to False
2023-05-21 01:05:44,916:INFO:Fitting Model
2023-05-21 01:05:45,607:INFO:BayesianRidge()
2023-05-21 01:05:45,607:INFO:create_model() successfully completed......................................
2023-05-21 01:05:45,692:INFO:Initializing create_model()
2023-05-21 01:05:45,692:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000215DD46E410>, estimator=LGBMRegressor(random_state=5041), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-21 01:05:45,692:INFO:Checking exceptions
2023-05-21 01:05:45,694:INFO:Importing libraries
2023-05-21 01:05:45,694:INFO:Copying training dataset
2023-05-21 01:05:45,703:INFO:Defining folds
2023-05-21 01:05:45,703:INFO:Declaring metric variables
2023-05-21 01:05:45,703:INFO:Importing untrained model
2023-05-21 01:05:45,704:INFO:Declaring custom model
2023-05-21 01:05:45,704:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-21 01:05:45,706:INFO:Cross validation set to False
2023-05-21 01:05:45,706:INFO:Fitting Model
2023-05-21 01:05:46,492:INFO:LGBMRegressor(random_state=5041)
2023-05-21 01:05:46,492:INFO:create_model() successfully completed......................................
2023-05-21 01:05:46,601:INFO:_master_model_container: 20
2023-05-21 01:05:46,601:INFO:_display_container: 2
2023-05-21 01:05:46,603:INFO:[<catboost.core.CatBoostRegressor object at 0x00000215DD231D80>, GradientBoostingRegressor(random_state=5041), OrthogonalMatchingPursuit(), BayesianRidge(), LGBMRegressor(random_state=5041)]
2023-05-21 01:05:46,603:INFO:compare_models() successfully completed......................................
2023-05-24 12:08:07,935:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 12:08:08,027:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 12:08:08,028:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 12:08:08,028:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 12:08:09,912:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-24 16:00:22,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 16:00:22,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 16:00:22,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 16:00:22,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-05-24 16:00:23,408:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-05-24 16:00:23,746:WARNING:<frozen importlib._bootstrap>:241: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 232 from PyObject

2023-05-24 16:00:56,235:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32996\2221667683.py:4: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-24 16:00:56,328:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32996\2221667683.py:9: UserWarning:



`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751



2023-05-24 16:00:58,586:INFO:PyCaret RegressionExperiment
2023-05-24 16:00:58,586:INFO:Logging name: reg-default-name
2023-05-24 16:00:58,586:INFO:ML Usecase: MLUsecase.REGRESSION
2023-05-24 16:00:58,586:INFO:version 3.0.2
2023-05-24 16:00:58,586:INFO:Initializing setup()
2023-05-24 16:00:58,586:INFO:self.USI: 3c74
2023-05-24 16:00:58,586:INFO:self._variable_keys: {'fold_shuffle_param', '_ml_usecase', 'seed', 'n_jobs_param', 'pipeline', 'memory', 'X', 'idx', 'exp_name_log', 'exp_id', 'target_param', 'html_param', 'X_train', 'transform_target_param', 'USI', 'gpu_param', 'log_plots_param', 'X_test', 'data', 'y_test', 'fold_groups_param', 'y_train', 'gpu_n_jobs_param', 'y', 'fold_generator', '_available_plots', 'logging_param'}
2023-05-24 16:00:58,587:INFO:Checking environment
2023-05-24 16:00:58,587:INFO:python_version: 3.10.3
2023-05-24 16:00:58,587:INFO:python_build: ('tags/v3.10.3:a342a49', 'Mar 16 2022 13:07:40')
2023-05-24 16:00:58,587:INFO:machine: AMD64
2023-05-24 16:00:58,587:INFO:platform: Windows-10-10.0.22621-SP0
2023-05-24 16:00:58,588:INFO:Memory: svmem(total=17083187200, available=6436765696, percent=62.3, used=10646421504, free=6436765696)
2023-05-24 16:00:58,588:INFO:Physical Core: 6
2023-05-24 16:00:58,588:INFO:Logical Core: 12
2023-05-24 16:00:58,588:INFO:Checking libraries
2023-05-24 16:00:58,588:INFO:System:
2023-05-24 16:00:58,588:INFO:    python: 3.10.3 (tags/v3.10.3:a342a49, Mar 16 2022, 13:07:40) [MSC v.1929 64 bit (AMD64)]
2023-05-24 16:00:58,588:INFO:executable: c:\Python310\python.exe
2023-05-24 16:00:58,588:INFO:   machine: Windows-10-10.0.22621-SP0
2023-05-24 16:00:58,588:INFO:PyCaret required dependencies:
2023-05-24 16:00:58,588:INFO:                 pip: 23.1.2
2023-05-24 16:00:58,588:INFO:          setuptools: 58.1.0
2023-05-24 16:00:58,589:INFO:             pycaret: 3.0.2
2023-05-24 16:00:58,589:INFO:             IPython: 8.5.0
2023-05-24 16:00:58,589:INFO:          ipywidgets: 8.0.6
2023-05-24 16:00:58,589:INFO:                tqdm: 4.64.1
2023-05-24 16:00:58,589:INFO:               numpy: 1.23.0
2023-05-24 16:00:58,589:INFO:              pandas: 1.5.2
2023-05-24 16:00:58,589:INFO:              jinja2: 3.1.2
2023-05-24 16:00:58,589:INFO:               scipy: 1.9.3
2023-05-24 16:00:58,589:INFO:              joblib: 1.2.0
2023-05-24 16:00:58,589:INFO:             sklearn: 1.1.3
2023-05-24 16:00:58,589:INFO:                pyod: 1.0.9
2023-05-24 16:00:58,589:INFO:            imblearn: 0.10.1
2023-05-24 16:00:58,589:INFO:   category_encoders: 2.6.1
2023-05-24 16:00:58,589:INFO:            lightgbm: 3.3.5
2023-05-24 16:00:58,589:INFO:               numba: 0.57.0
2023-05-24 16:00:58,589:INFO:            requests: 2.28.2
2023-05-24 16:00:58,589:INFO:          matplotlib: 3.5.3
2023-05-24 16:00:58,590:INFO:          scikitplot: 0.3.7
2023-05-24 16:00:58,590:INFO:         yellowbrick: 1.5
2023-05-24 16:00:58,590:INFO:              plotly: 5.13.1
2023-05-24 16:00:58,590:INFO:             kaleido: 0.2.1
2023-05-24 16:00:58,590:INFO:         statsmodels: 0.13.5
2023-05-24 16:00:58,590:INFO:              sktime: 0.17.0
2023-05-24 16:00:58,590:INFO:               tbats: 1.1.3
2023-05-24 16:00:58,590:INFO:            pmdarima: 2.0.3
2023-05-24 16:00:58,590:INFO:              psutil: 5.9.2
2023-05-24 16:00:58,590:INFO:PyCaret optional dependencies:
2023-05-24 16:00:58,734:INFO:                shap: Not installed
2023-05-24 16:00:58,734:INFO:           interpret: 0.4.1
2023-05-24 16:00:58,734:INFO:                umap: Not installed
2023-05-24 16:00:58,735:INFO:    pandas_profiling: Not installed
2023-05-24 16:00:58,735:INFO:  explainerdashboard: Not installed
2023-05-24 16:00:58,735:INFO:             autoviz: Not installed
2023-05-24 16:00:58,735:INFO:           fairlearn: Not installed
2023-05-24 16:00:58,735:INFO:             xgboost: 1.7.4
2023-05-24 16:00:58,735:INFO:            catboost: 1.1.1
2023-05-24 16:00:58,735:INFO:              kmodes: Not installed
2023-05-24 16:00:58,735:INFO:             mlxtend: Not installed
2023-05-24 16:00:58,735:INFO:       statsforecast: Not installed
2023-05-24 16:00:58,736:INFO:        tune_sklearn: Not installed
2023-05-24 16:00:58,736:INFO:                 ray: Not installed
2023-05-24 16:00:58,736:INFO:            hyperopt: Not installed
2023-05-24 16:00:58,736:INFO:              optuna: Not installed
2023-05-24 16:00:58,736:INFO:               skopt: Not installed
2023-05-24 16:00:58,736:INFO:              mlflow: Not installed
2023-05-24 16:00:58,736:INFO:              gradio: Not installed
2023-05-24 16:00:58,736:INFO:             fastapi: Not installed
2023-05-24 16:00:58,736:INFO:             uvicorn: Not installed
2023-05-24 16:00:58,737:INFO:              m2cgen: Not installed
2023-05-24 16:00:58,737:INFO:           evidently: Not installed
2023-05-24 16:00:58,737:INFO:               fugue: Not installed
2023-05-24 16:00:58,737:INFO:           streamlit: Not installed
2023-05-24 16:00:58,737:INFO:             prophet: Not installed
2023-05-24 16:00:58,737:INFO:None
2023-05-24 16:00:58,737:INFO:Set up data.
2023-05-24 16:00:58,834:INFO:Set up train/test split.
2023-05-24 16:00:58,847:INFO:Set up index.
2023-05-24 16:00:58,847:INFO:Set up folding strategy.
2023-05-24 16:00:58,847:INFO:Assigning column types.
2023-05-24 16:00:58,857:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-05-24 16:00:58,857:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-24 16:00:58,862:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-24 16:00:58,866:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:58,928:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:58,970:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:58,971:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,072:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,073:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,078:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,083:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,146:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,188:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,188:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,191:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,191:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-05-24 16:00:59,196:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,201:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,264:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,307:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,307:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,310:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,315:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,320:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,379:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,424:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,424:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,427:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,428:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-05-24 16:00:59,437:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,496:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,540:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,542:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,545:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,557:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,614:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,658:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,752:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,755:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,756:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-05-24 16:00:59,853:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,913:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:00:59,914:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:00:59,916:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:00:59,987:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:01:00,033:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-05-24 16:01:00,034:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:00,037:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:00,037:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-05-24 16:01:00,107:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:01:00,152:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:00,155:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:00,226:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-05-24 16:01:00,271:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:00,274:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:00,274:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-05-24 16:01:00,388:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:00,390:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:00,504:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:00,508:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:00,516:INFO:Preparing preprocessing pipeline...
2023-05-24 16:01:00,516:INFO:Set up simple imputation.
2023-05-24 16:01:00,518:INFO:Set up column name cleaning.
2023-05-24 16:01:00,597:INFO:Finished creating preprocessing pipeline.
2023-05-24 16:01:00,611:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\AYMANM~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'OverallQual', 'OverallCond',
                                             'YearBuilt', 'YearRemodAdd',
                                             'MasVnrArea', 'BsmtFinSF1',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', '1stFlrSF',
                                             '2ndFlrSF', 'LowQualFinSF',
                                             'GrLivArea', 'BsmtFullBath',
                                             'Bsmt...
                                             'KitchenAbvGr', 'TotRmsAbvGrd',
                                             'Fireplaces', 'GarageYrBlt',
                                             'GarageCars', 'GarageArea',
                                             'WoodDeckSF', 'OpenPorchSF',
                                             'EnclosedPorch', '3SsnPorch', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-05-24 16:01:00,612:INFO:Creating final display dataframe.
2023-05-24 16:01:00,861:INFO:Setup _display_container:                     Description             Value
0                    Session id              3361
1                        Target         SalePrice
2                   Target type        Regression
3           Original data shape       (1460, 319)
4        Transformed data shape       (1460, 319)
5   Transformed train set shape       (1021, 319)
6    Transformed test set shape        (439, 319)
7              Numeric features               318
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              3c74
2023-05-24 16:01:01,002:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:01,006:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:01,145:INFO:Soft dependency imported: xgboost: 1.7.4
2023-05-24 16:01:01,148:INFO:Soft dependency imported: catboost: 1.1.1
2023-05-24 16:01:01,149:INFO:setup() successfully completed in 3.94s...............
2023-05-24 16:01:01,233:INFO:Initializing compare_models()
2023-05-24 16:01:01,233:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-05-24 16:01:01,233:INFO:Checking exceptions
2023-05-24 16:01:01,239:INFO:Preparing display monitor
2023-05-24 16:01:01,307:INFO:Initializing Linear Regression
2023-05-24 16:01:01,308:INFO:Total runtime is 1.6621748606363933e-05 minutes
2023-05-24 16:01:01,313:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:01,314:INFO:Initializing create_model()
2023-05-24 16:01:01,314:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:01,314:INFO:Checking exceptions
2023-05-24 16:01:01,314:INFO:Importing libraries
2023-05-24 16:01:01,315:INFO:Copying training dataset
2023-05-24 16:01:01,329:INFO:Defining folds
2023-05-24 16:01:01,329:INFO:Declaring metric variables
2023-05-24 16:01:01,334:INFO:Importing untrained model
2023-05-24 16:01:01,339:INFO:Linear Regression Imported successfully
2023-05-24 16:01:01,348:INFO:Starting cross validation
2023-05-24 16:01:01,361:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:14,728:INFO:Calculating mean and std
2023-05-24 16:01:14,730:INFO:Creating metrics dataframe
2023-05-24 16:01:15,517:INFO:Uploading results into container
2023-05-24 16:01:15,517:INFO:Uploading model into container now
2023-05-24 16:01:15,518:INFO:_master_model_container: 1
2023-05-24 16:01:15,518:INFO:_display_container: 2
2023-05-24 16:01:15,518:INFO:LinearRegression(n_jobs=-1)
2023-05-24 16:01:15,519:INFO:create_model() successfully completed......................................
2023-05-24 16:01:15,622:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:15,622:INFO:Creating metrics dataframe
2023-05-24 16:01:15,631:INFO:Initializing Lasso Regression
2023-05-24 16:01:15,631:INFO:Total runtime is 0.23873398701349893 minutes
2023-05-24 16:01:15,634:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:15,634:INFO:Initializing create_model()
2023-05-24 16:01:15,635:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:15,635:INFO:Checking exceptions
2023-05-24 16:01:15,635:INFO:Importing libraries
2023-05-24 16:01:15,636:INFO:Copying training dataset
2023-05-24 16:01:15,650:INFO:Defining folds
2023-05-24 16:01:15,651:INFO:Declaring metric variables
2023-05-24 16:01:15,656:INFO:Importing untrained model
2023-05-24 16:01:15,660:INFO:Lasso Regression Imported successfully
2023-05-24 16:01:15,677:INFO:Starting cross validation
2023-05-24 16:01:15,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:22,510:INFO:Calculating mean and std
2023-05-24 16:01:22,512:INFO:Creating metrics dataframe
2023-05-24 16:01:23,267:INFO:Uploading results into container
2023-05-24 16:01:23,269:INFO:Uploading model into container now
2023-05-24 16:01:23,270:INFO:_master_model_container: 2
2023-05-24 16:01:23,270:INFO:_display_container: 2
2023-05-24 16:01:23,270:INFO:Lasso(random_state=3361)
2023-05-24 16:01:23,270:INFO:create_model() successfully completed......................................
2023-05-24 16:01:23,372:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:23,372:INFO:Creating metrics dataframe
2023-05-24 16:01:23,381:INFO:Initializing Ridge Regression
2023-05-24 16:01:23,381:INFO:Total runtime is 0.3678989291191101 minutes
2023-05-24 16:01:23,384:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:23,384:INFO:Initializing create_model()
2023-05-24 16:01:23,385:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:23,385:INFO:Checking exceptions
2023-05-24 16:01:23,385:INFO:Importing libraries
2023-05-24 16:01:23,385:INFO:Copying training dataset
2023-05-24 16:01:23,398:INFO:Defining folds
2023-05-24 16:01:23,398:INFO:Declaring metric variables
2023-05-24 16:01:23,401:INFO:Importing untrained model
2023-05-24 16:01:23,407:INFO:Ridge Regression Imported successfully
2023-05-24 16:01:23,415:INFO:Starting cross validation
2023-05-24 16:01:23,419:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:28,835:INFO:Calculating mean and std
2023-05-24 16:01:28,836:INFO:Creating metrics dataframe
2023-05-24 16:01:29,450:INFO:Uploading results into container
2023-05-24 16:01:29,451:INFO:Uploading model into container now
2023-05-24 16:01:29,451:INFO:_master_model_container: 3
2023-05-24 16:01:29,451:INFO:_display_container: 2
2023-05-24 16:01:29,452:INFO:Ridge(random_state=3361)
2023-05-24 16:01:29,452:INFO:create_model() successfully completed......................................
2023-05-24 16:01:29,528:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:29,528:INFO:Creating metrics dataframe
2023-05-24 16:01:29,538:INFO:Initializing Elastic Net
2023-05-24 16:01:29,538:INFO:Total runtime is 0.47050527334213255 minutes
2023-05-24 16:01:29,541:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:29,541:INFO:Initializing create_model()
2023-05-24 16:01:29,541:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:29,541:INFO:Checking exceptions
2023-05-24 16:01:29,542:INFO:Importing libraries
2023-05-24 16:01:29,542:INFO:Copying training dataset
2023-05-24 16:01:29,552:INFO:Defining folds
2023-05-24 16:01:29,552:INFO:Declaring metric variables
2023-05-24 16:01:29,556:INFO:Importing untrained model
2023-05-24 16:01:29,561:INFO:Elastic Net Imported successfully
2023-05-24 16:01:29,568:INFO:Starting cross validation
2023-05-24 16:01:29,571:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:34,785:INFO:Calculating mean and std
2023-05-24 16:01:34,787:INFO:Creating metrics dataframe
2023-05-24 16:01:35,404:INFO:Uploading results into container
2023-05-24 16:01:35,404:INFO:Uploading model into container now
2023-05-24 16:01:35,405:INFO:_master_model_container: 4
2023-05-24 16:01:35,405:INFO:_display_container: 2
2023-05-24 16:01:35,405:INFO:ElasticNet(random_state=3361)
2023-05-24 16:01:35,405:INFO:create_model() successfully completed......................................
2023-05-24 16:01:35,479:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:35,479:INFO:Creating metrics dataframe
2023-05-24 16:01:35,489:INFO:Initializing Least Angle Regression
2023-05-24 16:01:35,489:INFO:Total runtime is 0.5696903944015502 minutes
2023-05-24 16:01:35,494:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:35,494:INFO:Initializing create_model()
2023-05-24 16:01:35,494:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:35,494:INFO:Checking exceptions
2023-05-24 16:01:35,495:INFO:Importing libraries
2023-05-24 16:01:35,495:INFO:Copying training dataset
2023-05-24 16:01:35,504:INFO:Defining folds
2023-05-24 16:01:35,504:INFO:Declaring metric variables
2023-05-24 16:01:35,508:INFO:Importing untrained model
2023-05-24 16:01:35,514:INFO:Least Angle Regression Imported successfully
2023-05-24 16:01:35,525:INFO:Starting cross validation
2023-05-24 16:01:35,528:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:35,665:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,681:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,687:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,697:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,708:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,715:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.164e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,715:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=5.130e-04, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,715:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=9.586e-04, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,717:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,719:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.043e-03, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,723:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=6.899e-04, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,727:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=4.866e-04, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.830e-04, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,733:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,733:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.584e-04, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,735:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=6.367e-04, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,735:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.349e-03, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,736:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=7.517e-04, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,740:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,741:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.912e-04, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,745:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=3.483e-04, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,747:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=6.444e-03, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,747:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:35,751:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.565e-03, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,762:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 121 iterations, i.e. alpha=2.734e-04, with an active set of 112 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,763:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.168e-03, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,764:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.724e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,765:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=2.577e-04, with an active set of 118 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,769:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.721e-03, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,777:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 198 iterations, i.e. alpha=9.476e-04, with an active set of 176 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,778:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=6.000e-04, with an active set of 64 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,779:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 166 iterations, i.e. alpha=2.153e-04, with an active set of 148 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,781:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=1.665e-04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,783:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.747e-03, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,785:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=1.362e-04, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,788:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 171 iterations, i.e. alpha=8.750e-04, with an active set of 150 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,794:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=1.757e-03, with an active set of 132 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,801:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=6.353e-04, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,804:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 260 iterations, i.e. alpha=8.337e-04, with an active set of 230 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-05-24 16:01:35,960:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:773: RuntimeWarning: overflow encountered in multiply
  coef[active] = prev_coef[active] + gamma_ * least_squares

2023-05-24 16:01:36,178:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,178:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,179:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,218:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,218:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,219:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,222:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,222:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,222:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,223:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,233:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-24 16:01:36,234:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,234:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:180: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)

2023-05-24 16:01:36,234:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,239:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,239:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,240:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,241:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,241:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,242:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,256:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,256:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,257:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,260:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,261:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,261:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,264:WARNING:c:\Python310\lib\site-packages\sklearn\utils\extmath.py:152: RuntimeWarning: invalid value encountered in matmul
  ret = a @ b

2023-05-24 16:01:36,281:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,282:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:446: RuntimeWarning: overflow encountered in square
  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)

2023-05-24 16:01:36,282:WARNING:c:\Python310\lib\site-packages\sklearn\metrics\_regression.py:927: RuntimeWarning: overflow encountered in square
  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)

2023-05-24 16:01:36,293:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_scorer.py", line 267, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 196, in mean_absolute_error
    y_type, y_true, y_pred, multioutput = _check_reg_targets(
  File "c:\Python310\lib\site-packages\sklearn\metrics\_regression.py", line 102, in _check_reg_targets
    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 899, in check_array
    _assert_all_finite(
  File "c:\Python310\lib\site-packages\sklearn\utils\validation.py", line 146, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-05-24 16:01:42,068:INFO:Calculating mean and std
2023-05-24 16:01:42,069:WARNING:c:\Python310\lib\site-packages\numpy\core\_methods.py:233: RuntimeWarning:

invalid value encountered in subtract


2023-05-24 16:01:42,071:INFO:Creating metrics dataframe
2023-05-24 16:01:42,942:INFO:Uploading results into container
2023-05-24 16:01:42,942:INFO:Uploading model into container now
2023-05-24 16:01:42,943:INFO:_master_model_container: 5
2023-05-24 16:01:42,943:INFO:_display_container: 2
2023-05-24 16:01:42,943:INFO:Lars(random_state=3361)
2023-05-24 16:01:42,943:INFO:create_model() successfully completed......................................
2023-05-24 16:01:43,032:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:43,032:INFO:Creating metrics dataframe
2023-05-24 16:01:43,045:INFO:Initializing Lasso Least Angle Regression
2023-05-24 16:01:43,045:INFO:Total runtime is 0.6956264813741048 minutes
2023-05-24 16:01:43,049:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:43,049:INFO:Initializing create_model()
2023-05-24 16:01:43,049:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:43,049:INFO:Checking exceptions
2023-05-24 16:01:43,049:INFO:Importing libraries
2023-05-24 16:01:43,049:INFO:Copying training dataset
2023-05-24 16:01:43,061:INFO:Defining folds
2023-05-24 16:01:43,061:INFO:Declaring metric variables
2023-05-24 16:01:43,066:INFO:Importing untrained model
2023-05-24 16:01:43,071:INFO:Lasso Least Angle Regression Imported successfully
2023-05-24 16:01:43,078:INFO:Starting cross validation
2023-05-24 16:01:43,081:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:43,233:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,248:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,268:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,283:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,306:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,309:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,323:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,348:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:43,367:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-05-24 16:01:49,858:INFO:Calculating mean and std
2023-05-24 16:01:49,860:INFO:Creating metrics dataframe
2023-05-24 16:01:50,475:INFO:Uploading results into container
2023-05-24 16:01:50,476:INFO:Uploading model into container now
2023-05-24 16:01:50,476:INFO:_master_model_container: 6
2023-05-24 16:01:50,476:INFO:_display_container: 2
2023-05-24 16:01:50,477:INFO:LassoLars(random_state=3361)
2023-05-24 16:01:50,477:INFO:create_model() successfully completed......................................
2023-05-24 16:01:50,557:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:50,557:INFO:Creating metrics dataframe
2023-05-24 16:01:50,567:INFO:Initializing Orthogonal Matching Pursuit
2023-05-24 16:01:50,568:INFO:Total runtime is 0.8210161209106445 minutes
2023-05-24 16:01:50,572:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:50,573:INFO:Initializing create_model()
2023-05-24 16:01:50,573:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:50,573:INFO:Checking exceptions
2023-05-24 16:01:50,573:INFO:Importing libraries
2023-05-24 16:01:50,573:INFO:Copying training dataset
2023-05-24 16:01:50,585:INFO:Defining folds
2023-05-24 16:01:50,586:INFO:Declaring metric variables
2023-05-24 16:01:50,590:INFO:Importing untrained model
2023-05-24 16:01:50,594:INFO:Orthogonal Matching Pursuit Imported successfully
2023-05-24 16:01:50,602:INFO:Starting cross validation
2023-05-24 16:01:50,604:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:01:50,705:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,724:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,725:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,747:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,759:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,775:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,801:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,817:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,824:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:50,839:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-05-24 16:01:56,134:INFO:Calculating mean and std
2023-05-24 16:01:56,138:INFO:Creating metrics dataframe
2023-05-24 16:01:56,787:INFO:Uploading results into container
2023-05-24 16:01:56,788:INFO:Uploading model into container now
2023-05-24 16:01:56,789:INFO:_master_model_container: 7
2023-05-24 16:01:56,789:INFO:_display_container: 2
2023-05-24 16:01:56,789:INFO:OrthogonalMatchingPursuit()
2023-05-24 16:01:56,789:INFO:create_model() successfully completed......................................
2023-05-24 16:01:56,869:INFO:SubProcess create_model() end ==================================
2023-05-24 16:01:56,870:INFO:Creating metrics dataframe
2023-05-24 16:01:56,881:INFO:Initializing Bayesian Ridge
2023-05-24 16:01:56,881:INFO:Total runtime is 0.9262241562207539 minutes
2023-05-24 16:01:56,886:INFO:SubProcess create_model() called ==================================
2023-05-24 16:01:56,886:INFO:Initializing create_model()
2023-05-24 16:01:56,886:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:01:56,886:INFO:Checking exceptions
2023-05-24 16:01:56,886:INFO:Importing libraries
2023-05-24 16:01:56,886:INFO:Copying training dataset
2023-05-24 16:01:56,898:INFO:Defining folds
2023-05-24 16:01:56,898:INFO:Declaring metric variables
2023-05-24 16:01:56,902:INFO:Importing untrained model
2023-05-24 16:01:56,907:INFO:Bayesian Ridge Imported successfully
2023-05-24 16:01:56,914:INFO:Starting cross validation
2023-05-24 16:01:56,917:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:02,407:INFO:Calculating mean and std
2023-05-24 16:02:02,408:INFO:Creating metrics dataframe
2023-05-24 16:02:03,000:INFO:Uploading results into container
2023-05-24 16:02:03,001:INFO:Uploading model into container now
2023-05-24 16:02:03,001:INFO:_master_model_container: 8
2023-05-24 16:02:03,002:INFO:_display_container: 2
2023-05-24 16:02:03,002:INFO:BayesianRidge()
2023-05-24 16:02:03,002:INFO:create_model() successfully completed......................................
2023-05-24 16:02:03,081:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:03,081:INFO:Creating metrics dataframe
2023-05-24 16:02:03,091:INFO:Initializing Passive Aggressive Regressor
2023-05-24 16:02:03,091:INFO:Total runtime is 1.0297348300615945 minutes
2023-05-24 16:02:03,094:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:03,094:INFO:Initializing create_model()
2023-05-24 16:02:03,094:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:03,094:INFO:Checking exceptions
2023-05-24 16:02:03,094:INFO:Importing libraries
2023-05-24 16:02:03,094:INFO:Copying training dataset
2023-05-24 16:02:03,106:INFO:Defining folds
2023-05-24 16:02:03,107:INFO:Declaring metric variables
2023-05-24 16:02:03,111:INFO:Importing untrained model
2023-05-24 16:02:03,116:INFO:Passive Aggressive Regressor Imported successfully
2023-05-24 16:02:03,122:INFO:Starting cross validation
2023-05-24 16:02:03,125:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:08,682:INFO:Calculating mean and std
2023-05-24 16:02:08,685:INFO:Creating metrics dataframe
2023-05-24 16:02:09,338:INFO:Uploading results into container
2023-05-24 16:02:09,339:INFO:Uploading model into container now
2023-05-24 16:02:09,340:INFO:_master_model_container: 9
2023-05-24 16:02:09,341:INFO:_display_container: 2
2023-05-24 16:02:09,341:INFO:PassiveAggressiveRegressor(random_state=3361)
2023-05-24 16:02:09,342:INFO:create_model() successfully completed......................................
2023-05-24 16:02:09,423:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:09,423:INFO:Creating metrics dataframe
2023-05-24 16:02:09,433:INFO:Initializing Huber Regressor
2023-05-24 16:02:09,433:INFO:Total runtime is 1.1354195435841876 minutes
2023-05-24 16:02:09,437:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:09,437:INFO:Initializing create_model()
2023-05-24 16:02:09,438:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:09,438:INFO:Checking exceptions
2023-05-24 16:02:09,438:INFO:Importing libraries
2023-05-24 16:02:09,438:INFO:Copying training dataset
2023-05-24 16:02:09,449:INFO:Defining folds
2023-05-24 16:02:09,449:INFO:Declaring metric variables
2023-05-24 16:02:09,451:INFO:Importing untrained model
2023-05-24 16:02:09,456:INFO:Huber Regressor Imported successfully
2023-05-24 16:02:09,464:INFO:Starting cross validation
2023-05-24 16:02:09,466:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:11,914:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,331:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,371:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,394:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,435:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,458:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,464:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,466:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,492:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:12,498:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-05-24 16:02:17,633:INFO:Calculating mean and std
2023-05-24 16:02:17,634:INFO:Creating metrics dataframe
2023-05-24 16:02:18,298:INFO:Uploading results into container
2023-05-24 16:02:18,299:INFO:Uploading model into container now
2023-05-24 16:02:18,299:INFO:_master_model_container: 10
2023-05-24 16:02:18,299:INFO:_display_container: 2
2023-05-24 16:02:18,299:INFO:HuberRegressor()
2023-05-24 16:02:18,299:INFO:create_model() successfully completed......................................
2023-05-24 16:02:18,376:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:18,377:INFO:Creating metrics dataframe
2023-05-24 16:02:18,389:INFO:Initializing K Neighbors Regressor
2023-05-24 16:02:18,389:INFO:Total runtime is 1.284694218635559 minutes
2023-05-24 16:02:18,392:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:18,392:INFO:Initializing create_model()
2023-05-24 16:02:18,393:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:18,393:INFO:Checking exceptions
2023-05-24 16:02:18,393:INFO:Importing libraries
2023-05-24 16:02:18,393:INFO:Copying training dataset
2023-05-24 16:02:18,402:INFO:Defining folds
2023-05-24 16:02:18,403:INFO:Declaring metric variables
2023-05-24 16:02:18,406:INFO:Importing untrained model
2023-05-24 16:02:18,412:INFO:K Neighbors Regressor Imported successfully
2023-05-24 16:02:18,418:INFO:Starting cross validation
2023-05-24 16:02:18,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:23,885:INFO:Calculating mean and std
2023-05-24 16:02:23,887:INFO:Creating metrics dataframe
2023-05-24 16:02:24,524:INFO:Uploading results into container
2023-05-24 16:02:24,524:INFO:Uploading model into container now
2023-05-24 16:02:24,524:INFO:_master_model_container: 11
2023-05-24 16:02:24,524:INFO:_display_container: 2
2023-05-24 16:02:24,525:INFO:KNeighborsRegressor(n_jobs=-1)
2023-05-24 16:02:24,525:INFO:create_model() successfully completed......................................
2023-05-24 16:02:24,602:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:24,602:INFO:Creating metrics dataframe
2023-05-24 16:02:24,614:INFO:Initializing Decision Tree Regressor
2023-05-24 16:02:24,615:INFO:Total runtime is 1.388464868068695 minutes
2023-05-24 16:02:24,618:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:24,618:INFO:Initializing create_model()
2023-05-24 16:02:24,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:24,618:INFO:Checking exceptions
2023-05-24 16:02:24,618:INFO:Importing libraries
2023-05-24 16:02:24,618:INFO:Copying training dataset
2023-05-24 16:02:24,631:INFO:Defining folds
2023-05-24 16:02:24,631:INFO:Declaring metric variables
2023-05-24 16:02:24,634:INFO:Importing untrained model
2023-05-24 16:02:24,639:INFO:Decision Tree Regressor Imported successfully
2023-05-24 16:02:24,648:INFO:Starting cross validation
2023-05-24 16:02:24,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:30,084:INFO:Calculating mean and std
2023-05-24 16:02:30,086:INFO:Creating metrics dataframe
2023-05-24 16:02:30,730:INFO:Uploading results into container
2023-05-24 16:02:30,730:INFO:Uploading model into container now
2023-05-24 16:02:30,731:INFO:_master_model_container: 12
2023-05-24 16:02:30,731:INFO:_display_container: 2
2023-05-24 16:02:30,731:INFO:DecisionTreeRegressor(random_state=3361)
2023-05-24 16:02:30,731:INFO:create_model() successfully completed......................................
2023-05-24 16:02:30,806:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:30,806:INFO:Creating metrics dataframe
2023-05-24 16:02:30,816:INFO:Initializing Random Forest Regressor
2023-05-24 16:02:30,816:INFO:Total runtime is 1.4918149828910827 minutes
2023-05-24 16:02:30,821:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:30,821:INFO:Initializing create_model()
2023-05-24 16:02:30,822:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:30,822:INFO:Checking exceptions
2023-05-24 16:02:30,822:INFO:Importing libraries
2023-05-24 16:02:30,822:INFO:Copying training dataset
2023-05-24 16:02:30,833:INFO:Defining folds
2023-05-24 16:02:30,833:INFO:Declaring metric variables
2023-05-24 16:02:30,836:INFO:Importing untrained model
2023-05-24 16:02:30,841:INFO:Random Forest Regressor Imported successfully
2023-05-24 16:02:30,848:INFO:Starting cross validation
2023-05-24 16:02:30,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:33,848:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:33,908:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:39,726:INFO:Calculating mean and std
2023-05-24 16:02:39,728:INFO:Creating metrics dataframe
2023-05-24 16:02:40,363:INFO:Uploading results into container
2023-05-24 16:02:40,364:INFO:Uploading model into container now
2023-05-24 16:02:40,364:INFO:_master_model_container: 13
2023-05-24 16:02:40,364:INFO:_display_container: 2
2023-05-24 16:02:40,364:INFO:RandomForestRegressor(n_jobs=-1, random_state=3361)
2023-05-24 16:02:40,365:INFO:create_model() successfully completed......................................
2023-05-24 16:02:40,445:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:40,445:INFO:Creating metrics dataframe
2023-05-24 16:02:40,456:INFO:Initializing Extra Trees Regressor
2023-05-24 16:02:40,456:INFO:Total runtime is 1.6524798115094503 minutes
2023-05-24 16:02:40,458:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:40,459:INFO:Initializing create_model()
2023-05-24 16:02:40,459:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:40,459:INFO:Checking exceptions
2023-05-24 16:02:40,459:INFO:Importing libraries
2023-05-24 16:02:40,459:INFO:Copying training dataset
2023-05-24 16:02:40,472:INFO:Defining folds
2023-05-24 16:02:40,472:INFO:Declaring metric variables
2023-05-24 16:02:40,476:INFO:Importing untrained model
2023-05-24 16:02:40,480:INFO:Extra Trees Regressor Imported successfully
2023-05-24 16:02:40,488:INFO:Starting cross validation
2023-05-24 16:02:40,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:42,196:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:42,794:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.86s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:43,852:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.31s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:44,127:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:44,177:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.41s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:44,317:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.45s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-05-24 16:02:44,535:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:44,549:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-05-24 16:02:44,563:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.79s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-05-24 16:02:50,136:INFO:Calculating mean and std
2023-05-24 16:02:50,137:INFO:Creating metrics dataframe
2023-05-24 16:02:50,768:INFO:Uploading results into container
2023-05-24 16:02:50,769:INFO:Uploading model into container now
2023-05-24 16:02:50,769:INFO:_master_model_container: 14
2023-05-24 16:02:50,769:INFO:_display_container: 2
2023-05-24 16:02:50,770:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3361)
2023-05-24 16:02:50,770:INFO:create_model() successfully completed......................................
2023-05-24 16:02:50,846:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:50,846:INFO:Creating metrics dataframe
2023-05-24 16:02:50,857:INFO:Initializing AdaBoost Regressor
2023-05-24 16:02:50,857:INFO:Total runtime is 1.8258284211158753 minutes
2023-05-24 16:02:50,861:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:50,861:INFO:Initializing create_model()
2023-05-24 16:02:50,862:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:50,862:INFO:Checking exceptions
2023-05-24 16:02:50,862:INFO:Importing libraries
2023-05-24 16:02:50,862:INFO:Copying training dataset
2023-05-24 16:02:50,872:INFO:Defining folds
2023-05-24 16:02:50,872:INFO:Declaring metric variables
2023-05-24 16:02:50,876:INFO:Importing untrained model
2023-05-24 16:02:50,880:INFO:AdaBoost Regressor Imported successfully
2023-05-24 16:02:50,889:INFO:Starting cross validation
2023-05-24 16:02:50,891:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:02:57,820:INFO:Calculating mean and std
2023-05-24 16:02:57,821:INFO:Creating metrics dataframe
2023-05-24 16:02:58,499:INFO:Uploading results into container
2023-05-24 16:02:58,499:INFO:Uploading model into container now
2023-05-24 16:02:58,501:INFO:_master_model_container: 15
2023-05-24 16:02:58,501:INFO:_display_container: 2
2023-05-24 16:02:58,502:INFO:AdaBoostRegressor(random_state=3361)
2023-05-24 16:02:58,502:INFO:create_model() successfully completed......................................
2023-05-24 16:02:58,582:INFO:SubProcess create_model() end ==================================
2023-05-24 16:02:58,582:INFO:Creating metrics dataframe
2023-05-24 16:02:58,594:INFO:Initializing Gradient Boosting Regressor
2023-05-24 16:02:58,594:INFO:Total runtime is 1.954768963654836 minutes
2023-05-24 16:02:58,598:INFO:SubProcess create_model() called ==================================
2023-05-24 16:02:58,598:INFO:Initializing create_model()
2023-05-24 16:02:58,598:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:02:58,598:INFO:Checking exceptions
2023-05-24 16:02:58,598:INFO:Importing libraries
2023-05-24 16:02:58,598:INFO:Copying training dataset
2023-05-24 16:02:58,610:INFO:Defining folds
2023-05-24 16:02:58,610:INFO:Declaring metric variables
2023-05-24 16:02:58,613:INFO:Importing untrained model
2023-05-24 16:02:58,618:INFO:Gradient Boosting Regressor Imported successfully
2023-05-24 16:02:58,626:INFO:Starting cross validation
2023-05-24 16:02:58,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:03:05,907:INFO:Calculating mean and std
2023-05-24 16:03:05,908:INFO:Creating metrics dataframe
2023-05-24 16:03:06,606:INFO:Uploading results into container
2023-05-24 16:03:06,606:INFO:Uploading model into container now
2023-05-24 16:03:06,607:INFO:_master_model_container: 16
2023-05-24 16:03:06,607:INFO:_display_container: 2
2023-05-24 16:03:06,607:INFO:GradientBoostingRegressor(random_state=3361)
2023-05-24 16:03:06,607:INFO:create_model() successfully completed......................................
2023-05-24 16:03:06,682:INFO:SubProcess create_model() end ==================================
2023-05-24 16:03:06,682:INFO:Creating metrics dataframe
2023-05-24 16:03:06,695:INFO:Initializing Extreme Gradient Boosting
2023-05-24 16:03:06,695:INFO:Total runtime is 2.0898019909858703 minutes
2023-05-24 16:03:06,698:INFO:SubProcess create_model() called ==================================
2023-05-24 16:03:06,698:INFO:Initializing create_model()
2023-05-24 16:03:06,698:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:03:06,698:INFO:Checking exceptions
2023-05-24 16:03:06,698:INFO:Importing libraries
2023-05-24 16:03:06,698:INFO:Copying training dataset
2023-05-24 16:03:06,709:INFO:Defining folds
2023-05-24 16:03:06,709:INFO:Declaring metric variables
2023-05-24 16:03:06,713:INFO:Importing untrained model
2023-05-24 16:03:06,716:INFO:Extreme Gradient Boosting Imported successfully
2023-05-24 16:03:06,726:INFO:Starting cross validation
2023-05-24 16:03:06,729:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:03:14,917:INFO:Calculating mean and std
2023-05-24 16:03:14,919:INFO:Creating metrics dataframe
2023-05-24 16:03:15,716:INFO:Uploading results into container
2023-05-24 16:03:15,717:INFO:Uploading model into container now
2023-05-24 16:03:15,718:INFO:_master_model_container: 17
2023-05-24 16:03:15,718:INFO:_display_container: 2
2023-05-24 16:03:15,719:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3361, ...)
2023-05-24 16:03:15,719:INFO:create_model() successfully completed......................................
2023-05-24 16:03:15,837:INFO:SubProcess create_model() end ==================================
2023-05-24 16:03:15,837:INFO:Creating metrics dataframe
2023-05-24 16:03:15,854:INFO:Initializing Light Gradient Boosting Machine
2023-05-24 16:03:15,854:INFO:Total runtime is 2.242446231842041 minutes
2023-05-24 16:03:15,857:INFO:SubProcess create_model() called ==================================
2023-05-24 16:03:15,858:INFO:Initializing create_model()
2023-05-24 16:03:15,858:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:03:15,858:INFO:Checking exceptions
2023-05-24 16:03:15,859:INFO:Importing libraries
2023-05-24 16:03:15,859:INFO:Copying training dataset
2023-05-24 16:03:15,874:INFO:Defining folds
2023-05-24 16:03:15,874:INFO:Declaring metric variables
2023-05-24 16:03:15,878:INFO:Importing untrained model
2023-05-24 16:03:15,887:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-24 16:03:15,896:INFO:Starting cross validation
2023-05-24 16:03:15,899:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:03:22,864:INFO:Calculating mean and std
2023-05-24 16:03:22,865:INFO:Creating metrics dataframe
2023-05-24 16:03:23,561:INFO:Uploading results into container
2023-05-24 16:03:23,562:INFO:Uploading model into container now
2023-05-24 16:03:23,562:INFO:_master_model_container: 18
2023-05-24 16:03:23,562:INFO:_display_container: 2
2023-05-24 16:03:23,563:INFO:LGBMRegressor(random_state=3361)
2023-05-24 16:03:23,563:INFO:create_model() successfully completed......................................
2023-05-24 16:03:23,645:INFO:SubProcess create_model() end ==================================
2023-05-24 16:03:23,645:INFO:Creating metrics dataframe
2023-05-24 16:03:23,657:INFO:Initializing CatBoost Regressor
2023-05-24 16:03:23,657:INFO:Total runtime is 2.3724986672401425 minutes
2023-05-24 16:03:23,661:INFO:SubProcess create_model() called ==================================
2023-05-24 16:03:23,661:INFO:Initializing create_model()
2023-05-24 16:03:23,661:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=catboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:03:23,661:INFO:Checking exceptions
2023-05-24 16:03:23,661:INFO:Importing libraries
2023-05-24 16:03:23,661:INFO:Copying training dataset
2023-05-24 16:03:23,673:INFO:Defining folds
2023-05-24 16:03:23,673:INFO:Declaring metric variables
2023-05-24 16:03:23,678:INFO:Importing untrained model
2023-05-24 16:03:23,692:INFO:CatBoost Regressor Imported successfully
2023-05-24 16:03:23,700:INFO:Starting cross validation
2023-05-24 16:03:23,702:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:03:54,652:INFO:Calculating mean and std
2023-05-24 16:03:54,653:INFO:Creating metrics dataframe
2023-05-24 16:03:55,325:INFO:Uploading results into container
2023-05-24 16:03:55,326:INFO:Uploading model into container now
2023-05-24 16:03:55,326:INFO:_master_model_container: 19
2023-05-24 16:03:55,326:INFO:_display_container: 2
2023-05-24 16:03:55,326:INFO:<catboost.core.CatBoostRegressor object at 0x000001934F1C1180>
2023-05-24 16:03:55,326:INFO:create_model() successfully completed......................................
2023-05-24 16:03:55,403:INFO:SubProcess create_model() end ==================================
2023-05-24 16:03:55,403:INFO:Creating metrics dataframe
2023-05-24 16:03:55,422:INFO:Initializing Dummy Regressor
2023-05-24 16:03:55,423:INFO:Total runtime is 2.901931718985239 minutes
2023-05-24 16:03:55,426:INFO:SubProcess create_model() called ==================================
2023-05-24 16:03:55,427:INFO:Initializing create_model()
2023-05-24 16:03:55,427:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001935CE3A710>, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:03:55,427:INFO:Checking exceptions
2023-05-24 16:03:55,427:INFO:Importing libraries
2023-05-24 16:03:55,427:INFO:Copying training dataset
2023-05-24 16:03:55,437:INFO:Defining folds
2023-05-24 16:03:55,438:INFO:Declaring metric variables
2023-05-24 16:03:55,441:INFO:Importing untrained model
2023-05-24 16:03:55,445:INFO:Dummy Regressor Imported successfully
2023-05-24 16:03:55,453:INFO:Starting cross validation
2023-05-24 16:03:55,455:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-05-24 16:04:01,187:INFO:Calculating mean and std
2023-05-24 16:04:01,188:INFO:Creating metrics dataframe
2023-05-24 16:04:01,876:INFO:Uploading results into container
2023-05-24 16:04:01,877:INFO:Uploading model into container now
2023-05-24 16:04:01,878:INFO:_master_model_container: 20
2023-05-24 16:04:01,878:INFO:_display_container: 2
2023-05-24 16:04:01,878:INFO:DummyRegressor()
2023-05-24 16:04:01,878:INFO:create_model() successfully completed......................................
2023-05-24 16:04:01,956:INFO:SubProcess create_model() end ==================================
2023-05-24 16:04:01,956:INFO:Creating metrics dataframe
2023-05-24 16:04:01,984:INFO:Initializing create_model()
2023-05-24 16:04:01,985:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=<catboost.core.CatBoostRegressor object at 0x000001934F1C1180>, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:04:01,985:INFO:Checking exceptions
2023-05-24 16:04:01,988:INFO:Importing libraries
2023-05-24 16:04:01,988:INFO:Copying training dataset
2023-05-24 16:04:01,996:INFO:Defining folds
2023-05-24 16:04:01,996:INFO:Declaring metric variables
2023-05-24 16:04:01,996:INFO:Importing untrained model
2023-05-24 16:04:01,996:INFO:Declaring custom model
2023-05-24 16:04:01,997:INFO:CatBoost Regressor Imported successfully
2023-05-24 16:04:01,998:INFO:Cross validation set to False
2023-05-24 16:04:01,998:INFO:Fitting Model
2023-05-24 16:04:07,313:INFO:<catboost.core.CatBoostRegressor object at 0x000001934F77DFC0>
2023-05-24 16:04:07,313:INFO:create_model() successfully completed......................................
2023-05-24 16:04:07,399:INFO:Initializing create_model()
2023-05-24 16:04:07,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=GradientBoostingRegressor(random_state=3361), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:04:07,399:INFO:Checking exceptions
2023-05-24 16:04:07,402:INFO:Importing libraries
2023-05-24 16:04:07,402:INFO:Copying training dataset
2023-05-24 16:04:07,410:INFO:Defining folds
2023-05-24 16:04:07,410:INFO:Declaring metric variables
2023-05-24 16:04:07,411:INFO:Importing untrained model
2023-05-24 16:04:07,411:INFO:Declaring custom model
2023-05-24 16:04:07,411:INFO:Gradient Boosting Regressor Imported successfully
2023-05-24 16:04:07,413:INFO:Cross validation set to False
2023-05-24 16:04:07,413:INFO:Fitting Model
2023-05-24 16:04:08,788:INFO:GradientBoostingRegressor(random_state=3361)
2023-05-24 16:04:08,788:INFO:create_model() successfully completed......................................
2023-05-24 16:04:08,870:INFO:Initializing create_model()
2023-05-24 16:04:08,871:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=LGBMRegressor(random_state=3361), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:04:08,871:INFO:Checking exceptions
2023-05-24 16:04:08,873:INFO:Importing libraries
2023-05-24 16:04:08,873:INFO:Copying training dataset
2023-05-24 16:04:08,881:INFO:Defining folds
2023-05-24 16:04:08,881:INFO:Declaring metric variables
2023-05-24 16:04:08,881:INFO:Importing untrained model
2023-05-24 16:04:08,881:INFO:Declaring custom model
2023-05-24 16:04:08,883:INFO:Light Gradient Boosting Machine Imported successfully
2023-05-24 16:04:08,884:INFO:Cross validation set to False
2023-05-24 16:04:08,884:INFO:Fitting Model
2023-05-24 16:04:09,810:INFO:LGBMRegressor(random_state=3361)
2023-05-24 16:04:09,810:INFO:create_model() successfully completed......................................
2023-05-24 16:04:09,890:INFO:Initializing create_model()
2023-05-24 16:04:09,891:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=BayesianRidge(), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:04:09,891:INFO:Checking exceptions
2023-05-24 16:04:09,894:INFO:Importing libraries
2023-05-24 16:04:09,894:INFO:Copying training dataset
2023-05-24 16:04:09,901:INFO:Defining folds
2023-05-24 16:04:09,901:INFO:Declaring metric variables
2023-05-24 16:04:09,901:INFO:Importing untrained model
2023-05-24 16:04:09,901:INFO:Declaring custom model
2023-05-24 16:04:09,902:INFO:Bayesian Ridge Imported successfully
2023-05-24 16:04:09,903:INFO:Cross validation set to False
2023-05-24 16:04:09,904:INFO:Fitting Model
2023-05-24 16:04:10,757:INFO:BayesianRidge()
2023-05-24 16:04:10,757:INFO:create_model() successfully completed......................................
2023-05-24 16:04:10,837:INFO:Initializing create_model()
2023-05-24 16:04:10,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001934EE825C0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3361, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-05-24 16:04:10,837:INFO:Checking exceptions
2023-05-24 16:04:10,839:INFO:Importing libraries
2023-05-24 16:04:10,839:INFO:Copying training dataset
2023-05-24 16:04:10,849:INFO:Defining folds
2023-05-24 16:04:10,849:INFO:Declaring metric variables
2023-05-24 16:04:10,849:INFO:Importing untrained model
2023-05-24 16:04:10,849:INFO:Declaring custom model
2023-05-24 16:04:10,850:INFO:Extreme Gradient Boosting Imported successfully
2023-05-24 16:04:10,852:INFO:Cross validation set to False
2023-05-24 16:04:10,852:INFO:Fitting Model
2023-05-24 16:04:12,060:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3361, ...)
2023-05-24 16:04:12,060:INFO:create_model() successfully completed......................................
2023-05-24 16:04:12,176:INFO:_master_model_container: 20
2023-05-24 16:04:12,177:INFO:_display_container: 2
2023-05-24 16:04:12,180:INFO:[<catboost.core.CatBoostRegressor object at 0x000001934F77DFC0>, GradientBoostingRegressor(random_state=3361), LGBMRegressor(random_state=3361), BayesianRidge(), XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             n_estimators=100, n_jobs=-1, num_parallel_tree=None,
             predictor=None, random_state=3361, ...)]
2023-05-24 16:04:12,180:INFO:compare_models() successfully completed......................................
2023-05-24 16:39:11,046:WARNING:c:\Python310\lib\site-packages\pandas\core\arraylike.py:402: RuntimeWarning:

overflow encountered in exp


2023-05-24 16:40:05,704:WARNING:C:\Users\aymanmorshdy\AppData\Local\Temp\ipykernel_32996\1069392184.py:7: RuntimeWarning:

overflow encountered in exp


